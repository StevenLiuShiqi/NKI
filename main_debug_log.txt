(Pdb) p neuron_config.__dict__
{'capacity_factor': None, 'glu_mlp': True, 'glu_type': 'glu', 'hidden_act_scaling_factor': 1.0, 'hidden_act_bias': 0.0, 'early_expert_affinity_modulation': False, 'normalize_top_k_affinities': True, 'fused_shared_experts': False, 'shared_experts_sequence_parallel_enabled': False, 'return_expert_index': False, 'hybrid_sharding_config': None, 'moe_tp_degree': 1, 'moe_ep_degree': 1, 'transpose_shared_experts_weights': False, 'blockwise_matmul_config': <neuronx_distributed.modules.moe.moe_configs.BlockwiseMatmulConfig object at 0x7d9d06eef6d0>, 'router_config': <neuronx_distributed.modules.moe.moe_configs.RouterConfig object at 0x7d9d06eef8e0>, 'batch_size': 1, 'padding_side': 'right', 'allow_input_truncation': False, 'seq_len': 128, 'n_active_tokens': 128, 'n_positions': 128, 'on_cpu': False, 'output_logits': False, 'is_prefill_stage': None, 'torch_dtype': torch.bfloat16, 'overrides_torch_dtype': False, 'cast_type': 'config', 'rpl_reduce_dtype': None, 'attention_dtype': None, 'max_context_length': 128, 'max_new_tokens': None, 'max_length': 128, 'vocab_parallel': False, 'enable_cte_modular_flow': False, 'layer_boundary_markers': False, 'fused_qkv': False, 'sequence_parallel_enabled': False, 'weight_gather_seq_len_threshold': 32768, 'attn_cls': 'NeuronLlamaAttention', 'ctx_batch_size': 1, 'tkg_batch_size': 1, 'max_batch_size': 1, 'is_continuous_batching': False, 'kv_cache_batch_size': 1, 'kv_cache_padding_size': 0, 'on_device_sampling_config': None, 'async_mode': False, 'enable_bucketing': False, 'buckets': [128], 'bucket_n_active_tokens': False, 'context_encoding_buckets': None, 'prefix_buckets': None, 'token_generation_buckets': None, 'quantized': False, 'quantized_checkpoints_path': None, 'quantization_type': 'per_tensor_symmetric', 'quantization_dtype': 'int8', 'modules_to_not_convert': None, 'draft_model_modules_to_not_convert': None, 'kv_cache_quant': False, 'speculation_length': 0, 'spec_batch_size': 1, 'enable_fused_speculation': False, 'enable_eagle_speculation': False, 'is_eagle_draft': False, 'enable_eagle_draft_input_norm': False, 'token_tree_config': None, 'enable_token_tree': False, 'is_medusa': False, 'medusa_speculation_length': 0, 'num_medusa_heads': 0, 'medusa_tree': None, 'is_block_kv_layout': False, 'pa_num_blocks': 1, 'pa_block_size': 128, 'is_prefix_caching': False, 'chunked_prefill_config': None, 'is_chunked_prefill': False, 'tensor_capture_config': None, 'lora_config': None, 'tp_degree': 8, 'cp_degree': 1, 'attention_dp_degree': 1, 'pp_degree': 1, 'ep_degree': 1, 'save_sharded_checkpoint': False, 'skip_sharding': False, 'qk_layernorm': False, 'world_size': 8, 'start_rank_id': 0, 'local_ranks_size': 8, 'flash_decoding_enabled': False, 'disable_kv_cache_tiling': False, 'kv_cache_tiling': False, 'k_cache_transposed': False, 'attn_kernel_enabled': None, 'strided_context_parallel_kernel_enabled': False, 'qkv_kernel_enabled': False, 'qkv_kernel_nbsd_layout': False, 'mlp_kernel_enabled': False, 'fused_rmsnorm_skip_gamma': False, 'mlp_kernel_fuse_residual_add': False, 'qkv_kernel_fuse_residual_add': False, 'quantized_mlp_kernel_enabled': False, 'activation_quantization_type': None, 'attn_tkg_nki_kernel_enabled': False, 'attn_tkg_builtin_kernel_enabled': False, 'attn_block_tkg_nki_kernel_enabled': False, 'attn_block_cte_nki_kernel_enabled': False, 'attn_block_tkg_nki_kernel_cache_update': False, 'attn_block_tkg_nki_kernel_cascaded_attention': False, 'rmsnorm_quantize_kernel_enabled': False, 'quantize_clamp_bound': inf, 'moe_fused_nki_kernel_enabled': None, 'router_topk_nki_kernel_enabled': None, 'expert_mlp_nki_kernel_enabled': None, 'shared_mlp_nki_kernel_enabled': None, 'logical_nc_config': 1, 'lm_head_pad': False, 'lm_head_pad_alignment_size': 1, 'cc_pipeline_tiling_factor': 2, 'seq_len_threshold_for_cc_tiling': 16384, 'tile_cc': False, 'target': None, 'enable_spill_reload_dge': False, 'weights_to_skip_layout_optimization': [], 'apply_seq_ids_mask': False, 'skip_warmup': False, 'scratchpad_page_size': None, 'enable_long_context_mode': False, 'enable_output_completion_notifications': False}

(Pdb) p config.__dict__
{'neuron_config': <__main__.NeuronGPTOSSConfig object at 0x756a92c0b6d0>, 'fused_spec_config': None, 'vocab_size': 201088, 'hidden_size': 2880, 'intermediate_size': 2880, 'num_hidden_layers': 24, 'num_attention_heads': 64, 'num_local_experts': 32, 'sliding_window': 128, 'num_experts_per_tok': 4, 'num_key_value_heads': 8, 'hidden_act': 'silu', 'initializer_range': 0.02, 'rms_norm_eps': 1e-05, 'rope_theta': 150000, 'rope_scaling': {'beta_fast': 32.0, 'beta_slow': 1.0, 'factor': 32.0, 'original_max_position_embeddings': 4096, 'rope_type': 'yarn', 'truncate': False}, 'attention_dropout': 0.0, 'head_dim': 64, 'layer_types': ['sliding_attention', 'full_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'full_attention', 'sliding_attention', 'full_attention'], 'attention_bias': True, 'max_position_embeddings': 131072, 'router_aux_loss_coef': 0.9, 'output_router_logits': False, 'use_cache': True, 'return_dict': True, 'output_hidden_states': False, 'torchscript': False, 'dtype': 'bfloat16', 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'architectures': ['GptOssForCausalLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'task_specific_params': None, 'problem_type': None, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': None, 'pad_token_id': 199999, 'eos_token_id': 200002, 'sep_token_id': None, 'decoder_start_token_id': None, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'num_beam_groups': 1, 'diversity_penalty': 0.0, '_name_or_path': '/home/ubuntu/models/gpt-oss-20b/', 'transformers_version': '4.56.0.dev0', 'experts_per_token': 4, 'initial_context_length': 4096, 'model_type': 'gpt_oss', 'swiglu_limit': 7.0, 'tf_legacy_loss': False, 'use_bfloat16': False, 'output_attentions': False, 'attribute_map': {}, 'metadata': None, 'num_cores_per_group': 1}

(Pdb) generation_config = GenerationConfig.from_pretrained(model_path)
(Pdb) generation_config
GenerationConfig {
  "bos_token_id": 199998,
  "do_sample": true,
  "eos_token_id": [
    200002,
    199999
  ],
  "pad_token_id": 199999
}