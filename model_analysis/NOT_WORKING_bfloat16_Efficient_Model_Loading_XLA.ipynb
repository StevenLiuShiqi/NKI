{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "608c72e8-3066-47c5-96c2-96e569f4732c",
   "metadata": {},
   "source": [
    "Requires a trn1.32xlarge instance.\n",
    "\n",
    "1. Activate the Neuron Distributed Inference environment\n",
    "```Shell\n",
    "source /opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/bin/activate\n",
    "```\n",
    "\n",
    "2. Download Unsloth's bf16 version of gpt-oss-20b from HuggingFace\n",
    "```Shell\n",
    "hf download unsloth/gpt-oss-20b-BF16 --local-dir ~/models/gpt-oss-20b/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d5e08-a619-469a-b725-d08f23b7d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_xla.core.xla_model as xm\n",
    "device = xm.xla_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bb6786-02e4-4b62-8015-417efa798123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "MODEL_DIRECTORY_PATH = os.path.expanduser('~/models/gpt-oss-20b/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbc6276-beba-4b0a-89e8-94239209c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "safetensors_file_names = glob(os.path.join(MODEL_DIRECTORY_PATH, '*.safetensors'))\n",
    "safetensors_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d55e4c-ae73-42b0-9b58-287274c156e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "\n",
    "def yield_keys_and_tensors(safetensors_file_names):\n",
    "    for safetensors_file_name in safetensors_file_names:\n",
    "        with safe_open(safetensors_file_name, framework='pt') as f:\n",
    "            for k in f.keys():\n",
    "                yield k, f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37994716-318f-4e25-9341-670a65d6855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "MAX_POSITION_EMBEDDINGS = 131072\n",
    "MAX_LENGTH = 20\n",
    "TOP_K = 50\n",
    "EOS_TOKEN_ID = [200002, 199999]\n",
    "PAD_TOKEN_ID = 199999\n",
    "\n",
    "\n",
    "class GptOssTopKRouter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(32, 2880, dtype=torch.bfloat16))\n",
    "        self.bias = nn.Parameter(torch.empty(32, dtype=torch.bfloat16))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = hidden_states.reshape(-1, 2880)\n",
    "        router_logits = F.linear(hidden_states, self.weight, self.bias)  # (seq_len, num_experts)\n",
    "        router_top_value, router_indices = torch.topk(router_logits, 4, dim=-1)  # (seq_len, top_k)\n",
    "        router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n",
    "        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n",
    "        return router_scores, router_indices\n",
    "\n",
    "\n",
    "class GptOssRMSNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(2880, dtype=torch.bfloat16))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + 1e-05)\n",
    "        return (self.weight * hidden_states).to(input_dtype)\n",
    "\n",
    "def get_mscale(scale, mscale=1):\n",
    "    return 0.1 * mscale * math.log(scale) + 1.0\n",
    "\n",
    "\n",
    "def find_correction_dim(num_rotations, dim, base, max_position_embeddings):\n",
    "    return dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi)) / (2 * math.log(base))\n",
    "\n",
    "\n",
    "def find_correction_range(low_rot, high_rot, dim, base, max_position_embeddings):\n",
    "    low = find_correction_dim(low_rot, dim, base, max_position_embeddings)\n",
    "    high = find_correction_dim(high_rot, dim, base, max_position_embeddings)\n",
    "    return (max(low, 0), min(high, dim - 1))\n",
    "\n",
    "\n",
    "def linear_ramp_factor(min, max, dim):\n",
    "    linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n",
    "    ramp_func = torch.clamp(linear_func, 0, 1)\n",
    "    return ramp_func\n",
    "\n",
    "\n",
    "class GptOssRotaryEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attention_scaling = get_mscale(32.0)\n",
    "        \n",
    "        low, high = find_correction_range(32.0, 1.0, 64, 150000, 4096)\n",
    "        inv_freq_extrapolation_factor = 1 - linear_ramp_factor(low, high, 64 // 2)\n",
    "        pos_freqs = 150000 ** (torch.arange(0, 64, 2).to(dtype=torch.float) / 64)\n",
    "        inv_freq_extrapolation = 1.0 / pos_freqs\n",
    "        inv_freq_interpolation = 1.0 / (32.0 * pos_freqs)\n",
    "        inv_freq = inv_freq_interpolation * (1 - inv_freq_extrapolation_factor) + inv_freq_extrapolation * inv_freq_extrapolation_factor\n",
    "        \n",
    "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != 'mps' else 'cpu'\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = freqs\n",
    "            cos = emb.cos() * self.attention_scaling\n",
    "            sin = emb.sin() * self.attention_scaling\n",
    "        return (cos.to(x.dtype), sin.to(x.dtype))\n",
    "\n",
    "\n",
    "class GptOssExperts(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gate_up_proj = nn.Parameter(torch.empty(32, 2880, 2 * 2880, dtype=torch.bfloat16))\n",
    "        self.gate_up_proj_bias = nn.Parameter(torch.empty(32, 2 * 2880, dtype=torch.bfloat16))\n",
    "        self.down_proj = nn.Parameter(torch.empty((32, 2880, 2880), dtype=torch.bfloat16))\n",
    "        self.down_proj_bias = nn.Parameter(torch.empty(32, 2880, dtype=torch.bfloat16))\n",
    "        self.alpha = 1.702\n",
    "        self.limit = 7.0\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weights=None) -> torch.Tensor:\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        hidden_states = hidden_states.reshape(-1, 2880)\n",
    "        num_experts = routing_weights.shape[1]\n",
    "\n",
    "        next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n",
    "        with torch.no_grad():\n",
    "            expert_mask = torch.nn.functional.one_hot(router_indices, num_classes=num_experts + 1)\n",
    "            expert_mask = expert_mask.permute(2, 1, 0)\n",
    "            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n",
    "        for expert_idx in expert_hit[:]:\n",
    "            expert_idx = expert_idx[0]\n",
    "            with torch.no_grad():\n",
    "                _, token_idx = torch.where(expert_mask[expert_idx])\n",
    "            current_state = hidden_states[token_idx]\n",
    "            gate_up = current_state @ self.gate_up_proj[expert_idx] + self.gate_up_proj_bias[expert_idx]\n",
    "            gate, up = (gate_up[..., ::2], gate_up[..., 1::2])\n",
    "            gate = gate.clamp(min=None, max=self.limit)\n",
    "            up = up.clamp(min=-self.limit, max=self.limit)\n",
    "            glu = gate * torch.sigmoid(gate * self.alpha)\n",
    "            gated_output = (up + 1) * glu\n",
    "            out = gated_output @ self.down_proj[expert_idx] + self.down_proj_bias[expert_idx]\n",
    "            weighted_output = out * routing_weights[token_idx, expert_idx, None]\n",
    "            next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n",
    "        next_states = next_states.view(batch_size, -1, 2880)\n",
    "\n",
    "        return next_states\n",
    "\n",
    "\n",
    "CONFIG_LAYER_TYPES = (\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention'\n",
    ")\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    scaling: float,\n",
    "    dropout: float=0.0,\n",
    "    # **kwargs\n",
    "):\n",
    "    key_states = repeat_kv(key, 8)\n",
    "    value_states = repeat_kv(value, 8)\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = attention_mask[:, :, :, :key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "    sinks = module.sinks.reshape(1, -1, 1, 1).expand(query.shape[0], -1, query.shape[-2], -1)\n",
    "    combined_logits = torch.cat([attn_weights, sinks], dim=-1)\n",
    "    combined_logits = combined_logits - combined_logits.max(dim=-1, keepdim=True).values\n",
    "    probs = F.softmax(combined_logits, dim=-1, dtype=combined_logits.dtype)\n",
    "    scores = probs[..., :-1]\n",
    "    attn_weights = nn.functional.dropout(scores, p=dropout, training=False)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    return (attn_output, attn_weights)\n",
    "\n",
    "\n",
    "def _apply_rotary_emb(\n",
    "    x: torch.Tensor,\n",
    "    cos: torch.Tensor,\n",
    "    sin: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    first_half, second_half = torch.chunk(x, 2, dim=-1)\n",
    "    first_ = first_half * cos - second_half * sin\n",
    "    second_ = second_half * cos + first_half * sin\n",
    "    return torch.cat((first_, second_), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = _apply_rotary_emb(q, cos, sin)\n",
    "    k_embed = _apply_rotary_emb(k, cos, sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class GptOssAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(2880, 64 * 64, bias=True, dtype=torch.bfloat16)\n",
    "        self.k_proj = nn.Linear(2880, 8 * 64, bias=True, dtype=torch.bfloat16)\n",
    "        self.v_proj = nn.Linear(2880, 8 * 64, bias=True, dtype=torch.bfloat16)\n",
    "        self.o_proj = nn.Linear(64 * 64, 2880, bias=True, dtype=torch.bfloat16)\n",
    "        self.sinks = nn.Parameter(torch.empty(64, dtype=torch.bfloat16))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, 64)\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "        attn_output, attn_weights = eager_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0,\n",
    "            scaling=0.125\n",
    "        )\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return (attn_output, attn_weights)\n",
    "\n",
    "\n",
    "class GptOssMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.router = GptOssTopKRouter()\n",
    "        self.experts = GptOssExperts()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        router_scores, router_indices = self.router(hidden_states)\n",
    "        routed_out = self.experts(hidden_states, router_indices=router_indices, routing_weights=router_scores)\n",
    "        return (routed_out, router_scores)\n",
    "\n",
    "\n",
    "class GptOssDecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.self_attn = GptOssAttention()\n",
    "        self.mlp = GptOssMLP()\n",
    "        self.input_layernorm = GptOssRMSNorm()\n",
    "        self.post_attention_layernorm = GptOssRMSNorm()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor]=None,\n",
    "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]]=None,\n",
    "    ) -> torch.Tensor:\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        hidden_states, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_embeddings=position_embeddings,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states, _ = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "def _vmap_for_bhqkv(mask_function):\n",
    "    dimensions = [(None, None, None, 0), (None, None, 0, None), (None, 0, None, None), (0, None, None, None)]\n",
    "    for dims in dimensions:\n",
    "        mask_function = torch.vmap(mask_function, in_dims=dims, out_dims=0)\n",
    "    return mask_function\n",
    "\n",
    "\n",
    "def create_causal_mask(\n",
    "    attention_mask,\n",
    "    dtype,\n",
    "    device,\n",
    "):\n",
    "    batch_size = attention_mask.shape[0]\n",
    "    cur_len = attention_mask.shape[1]\n",
    "    \n",
    "    batch_arange = torch.arange(batch_size, device=device)\n",
    "    head_arange = torch.arange(1, device=device)\n",
    "    q_arange = torch.arange(cur_len, device=device)\n",
    "    kv_arange = torch.arange(cur_len, device=device)\n",
    "\n",
    "    def mask_function(batch_idx, head_idx, q_idx, kv_idx):\n",
    "        return q_idx.new_ones((), dtype=torch.bool) & (kv_idx <= q_idx).to(device) & (attention_mask[batch_idx, kv_idx]).to(device)\n",
    "    \n",
    "    with TransformGetItemToIndex():\n",
    "        mask = _vmap_for_bhqkv(mask_function)(batch_arange, head_arange, q_arange, kv_arange)\n",
    "    \n",
    "    mask = torch.where(\n",
    "        mask,\n",
    "        torch.tensor(0.0, device=device, dtype=dtype),\n",
    "        torch.finfo(dtype).min\n",
    "    )\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_sliding_window_causal_mask(\n",
    "    attention_mask,\n",
    "    dtype,\n",
    "    device,\n",
    "):\n",
    "    batch_size = attention_mask.shape[0]\n",
    "    cur_len = attention_mask.shape[1]\n",
    "    \n",
    "    batch_arange = torch.arange(batch_size, device=device)\n",
    "    head_arange = torch.arange(1, device=device)\n",
    "    q_arange = torch.arange(cur_len, device=device)\n",
    "    kv_arange = torch.arange(cur_len, device=device)\n",
    "\n",
    "    def mask_function(batch_idx, head_idx, q_idx, kv_idx):\n",
    "        return q_idx.new_ones((), dtype=torch.bool) & (kv_idx > q_idx - 128).to(device) & (kv_idx <= q_idx).to(device) & (attention_mask[batch_idx, kv_idx]).to(device)\n",
    "    \n",
    "    with TransformGetItemToIndex():\n",
    "        mask = _vmap_for_bhqkv(mask_function)(batch_arange, head_arange, q_arange, kv_arange)\n",
    "    \n",
    "    mask = torch.where(\n",
    "        mask,\n",
    "        torch.tensor(0.0, device=device, dtype=dtype),\n",
    "        torch.finfo(dtype).min\n",
    "    )\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "class GptOssModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(201088, 2880, 199999, dtype=torch.bfloat16)\n",
    "        self.layers = nn.ModuleList([GptOssDecoderLayer() for _ in range(24)])\n",
    "        self.norm = GptOssRMSNorm()\n",
    "        self.rotary_emb = GptOssRotaryEmbedding()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_ids: torch.LongTensor,\n",
    "    ):\n",
    "        input_embeddings = self.embed_tokens(input_ids)\n",
    "        causal_mask_mapping = {\n",
    "            'full_attention': create_causal_mask(\n",
    "                attention_mask=attention_mask,\n",
    "                dtype=input_embeddings.dtype,\n",
    "                device=input_embeddings.device,\n",
    "            ),\n",
    "            'sliding_attention': create_sliding_window_causal_mask(\n",
    "                attention_mask=attention_mask,\n",
    "                dtype=input_embeddings.dtype,\n",
    "                device=input_embeddings.device,\n",
    "            )\n",
    "        }\n",
    "        hidden_states = input_embeddings\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "        for decoder_layer, layer_type in zip(self.layers, CONFIG_LAYER_TYPES):\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask_mapping[layer_type],\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class GptOssForCausalLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = GptOssModel()\n",
    "        self.lm_head = nn.Linear(2880, 201088, bias=False, dtype=torch.bfloat16)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_ids: torch.LongTensor,\n",
    "    ):\n",
    "        hidden_states = self.model(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f258fa-d823-4bef-b559-5285625945d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GptOssForCausalLM()       # Uninitialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afea395b-943f-4356-a60a-23951931a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)                  # Ensure model is already on correct device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b5268-9115-4b2a-9965-bf04a9b0efc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()   # map of key->parameter/buffer (references, not clones)\n",
    "for key, tensor in yield_keys_and_tensors(safetensors_file_names):\n",
    "    if key not in state_dict:\n",
    "        print(f\"Warning: {key} not in model's state dict\")\n",
    "        continue\n",
    "    state_tensor = state_dict[key]\n",
    "    # Copy tensor data to the parameter/buffer (move to proper device if necessary)\n",
    "    state_tensor.copy_(tensor.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939bc664-ce3e-4937-884a-6136a9563bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "):  \n",
    "    batch_size = input_ids.shape[0]\n",
    "    cur_len = input_ids.shape[1]\n",
    "\n",
    "    max_length = min(MAX_LENGTH, MAX_POSITION_EMBEDDINGS)\n",
    "    pad_token_tensor = torch.tensor(PAD_TOKEN_ID, device=input_ids.device, dtype=torch.long)\n",
    "    eos_token_tensor = torch.tensor(EOS_TOKEN_ID, device=input_ids.device, dtype=torch.long)\n",
    "\n",
    "    all_sequences_finished = False\n",
    "    unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "    while not all_sequences_finished:\n",
    "        # Fully recompute position_ids for new length\n",
    "        position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "        # Stateless: only pass input_ids, attention_mask, position_ids\n",
    "        logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        # Get probs for next token in sequence\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        top_k = min(max(TOP_K, 1), next_token_logits.size(-1))\n",
    "        indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "        next_token_scores = next_token_logits.masked_fill(indices_to_remove, -float('Inf'))\n",
    "        probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
    "        \n",
    "        next_tokens = (\n",
    "            torch.multinomial(probs, num_samples=1).squeeze(1) * unfinished_sequences\n",
    "            + pad_token_tensor * (1 - unfinished_sequences)\n",
    "        )\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)\n",
    "\n",
    "        is_max_length = torch.full((input_ids.shape[0],), input_ids.shape[1] >= max_length, device=input_ids.device, dtype=torch.bool)\n",
    "        is_eos_token_generated = torch.isin(input_ids[:, -1], eos_token_tensor)\n",
    "        is_stopping = is_max_length | is_eos_token_generated\n",
    "        \n",
    "        unfinished_sequences = unfinished_sequences & ~is_stopping\n",
    "        all_sequences_finished = unfinished_sequences.max() == 0\n",
    "        cur_len += 1\n",
    "\n",
    "        xm.mark_step()  # XLA: collect ops and run them in XLA runtime\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4efe8a-1e52-49d4-bca7-f1e8be477549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa3e33-8959-485f-ab4c-124331b6c059",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.LongTensor([[   40,  6423,   290, 10915,   328,  2615,   382]]).to(device)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59777236-23af-4f73-a155-4ba6d1cd01be",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.BoolTensor([[True, True, True, True, True, True, True]]).to(device)\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca4cf5-9db5-4393-a0ac-8af641df42ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_token_sequences = generate(model, input_ids, attention_mask).to(torch.device('cpu'))\n",
    "output_token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a08587-59f2-499d-b15b-20868f7d91df",
   "metadata": {},
   "outputs": [],
   "source": [
    "[tokenizer.decode(output_token_sequence) for output_token_sequence in output_token_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87da94bc-bbaa-4ef4-a83a-c2f7a385316a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
