{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4ff2592-473d-43cc-8769-664c736e9356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5eb0905f-0f1b-45c3-947d-00b17e3daf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssTopKRouter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(32, 2880))\n",
    "        self.bias = nn.Parameter(torch.empty(32))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = hidden_states.reshape(-1, 2880)\n",
    "        router_logits = F.linear(hidden_states, self.weight, self.bias)  # (seq_len, num_experts)\n",
    "        router_top_value, router_indices = torch.topk(router_logits, 4, dim=-1)  # (seq_len, top_k)\n",
    "        router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n",
    "        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n",
    "        return router_scores, router_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74709f98-1c0a-492b-9232-7b9084f0f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "router = GptOssTopKRouter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3d0ff5d-8902-4e9c-82db-d08e8596b94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router.load_state_dict(torch.load('model.layers.7.mlp.router.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4ab569c-1011-4924-86e3-95a909c26a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.layers.7.mlp.router.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42a56ac5-2b37-4ef1-b7e9-8a21433df242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hidden_states', 'return'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3fdd2d9-14c3-4651-a039-501523377cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = torch.Tensor(parameters['hidden_states']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a03a2de-2c23-4ece-8f1d-e8a228eaa190",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_scores, router_indices = torch.Tensor(parameters['return']['items'][0]['data']), torch.LongTensor(parameters['return']['items'][1]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abb5b64b-89de-4031-b0ab-267d5ba86b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_router_scores, actual_router_indices = router(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd5c86f3-0b52-4c9b-88e6-dea125381e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_scores.allclose(actual_router_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65f0184a-d73b-4026-a8ef-260d90051a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_indices.allclose(actual_router_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40fac0-cea4-4fad-8d53-4c38b7d52be1",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "169f474a-6629-4449-8f1f-ca73ca05537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssRMSNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(2880))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + 1e-05)\n",
    "        return (self.weight * hidden_states).to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98c7bd05-834f-44a9-bd7f-6c071aaa8874",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_attention_layernorm = GptOssRMSNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f2c0a35-b3e7-4fe5-adc8-5766bad5d539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_attention_layernorm.load_state_dict(torch.load('model.layers.23.post_attention_layernorm.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ccdaa5be-1f5c-4e16-ace2-dd21f693f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.layers.23.post_attention_layernorm.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "128c49ef-b139-4365-9da5-afbea68fc94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = torch.Tensor(parameters['hidden_states']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45564f68-3666-481f-80f6-f9ad79408956",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = torch.Tensor(parameters['return']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbf8f2ef-69f3-4026-a161-5ef8ce5c86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ret = post_attention_layernorm(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fe1756d0-5181-40de-97f8-265f39e459e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.allclose(actual_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3efb67-3784-496b-8fef-02c3818f751e",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a11a7a93-9201-421b-940d-17bd98edd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_yarn_parameters():\n",
    "    base = 150000\n",
    "    partial_rotary_factor = 1.0\n",
    "    head_dim = 64\n",
    "    dim = int(head_dim * partial_rotary_factor)\n",
    "    factor = 32.0\n",
    "    attention_factor = None\n",
    "    mscale = None\n",
    "    mscale_all_dim = None\n",
    "    original_max_position_embeddings = 4096\n",
    "\n",
    "    def get_mscale(scale, mscale=1):\n",
    "        return 0.1 * mscale * math.log(scale) + 1.0\n",
    "\n",
    "    if attention_factor is None:\n",
    "        attention_factor = get_mscale(factor)\n",
    "            \n",
    "    beta_fast = 32.0\n",
    "    beta_slow = 1.0\n",
    "\n",
    "    def find_correction_dim(num_rotations, dim, base, max_position_embeddings):\n",
    "        return dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi)) / (2 * math.log(base))\n",
    "\n",
    "    def find_correction_range(low_rot, high_rot, dim, base, max_position_embeddings, truncate):\n",
    "        low = find_correction_dim(low_rot, dim, base, max_position_embeddings)\n",
    "        high = find_correction_dim(high_rot, dim, base, max_position_embeddings)\n",
    "        return (max(low, 0), min(high, dim - 1))\n",
    "\n",
    "    def linear_ramp_factor(min, max, dim):\n",
    "        linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n",
    "        ramp_func = torch.clamp(linear_func, 0, 1)\n",
    "        return ramp_func\n",
    "    \n",
    "    pos_freqs = base ** (torch.arange(0, dim, 2).to(dtype=torch.float) / dim)\n",
    "    inv_freq_extrapolation = 1.0 / pos_freqs\n",
    "    inv_freq_interpolation = 1.0 / (factor * pos_freqs)\n",
    "    truncate = False\n",
    "    low, high = find_correction_range(beta_fast, beta_slow, dim, base, original_max_position_embeddings, truncate)\n",
    "    inv_freq_extrapolation_factor = 1 - linear_ramp_factor(low, high, dim // 2).to(dtype=torch.float)\n",
    "    inv_freq = inv_freq_interpolation * (1 - inv_freq_extrapolation_factor) + inv_freq_extrapolation * inv_freq_extrapolation_factor\n",
    "    return (inv_freq, attention_factor)\n",
    "\n",
    "\n",
    "class GptOssRotaryEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        inv_freq, self.attention_scaling = _compute_yarn_parameters()\n",
    "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != 'mps' else 'cpu'\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = freqs\n",
    "            cos = emb.cos() * self.attention_scaling\n",
    "            sin = emb.sin() * self.attention_scaling\n",
    "        return (cos.to(x.dtype), sin.to(x.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c1ad588-20a0-42ef-8e6b-adf3ba0d1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotary_embedding = GptOssRotaryEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fef52fe-9a0a-4cb2-9e5b-9ea22219a0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotary_embedding.load_state_dict(torch.load('model.rotary_emb.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "229ca646-de69-4257-ac92-e5250c56f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.rotary_emb.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c0fd6c4e-ccd7-4abe-8923-af2be0be9787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x', 'position_ids', 'return'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6f917ad-cda9-4ce8-ac68-fefc38504ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(parameters['x']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2bcdcdd9-60c8-431b-b868-c9ca4870200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.Tensor(parameters['position_ids']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f00c0895-d26f-4e59-86fc-2481a444e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_0, return_1 = torch.Tensor(parameters['return']['items'][0]['data']), torch.Tensor(parameters['return']['items'][1]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2919e1bf-7c5b-4a66-a103-920b00d5acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_return_0, actual_return_1 = rotary_embedding(x, position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67d9e218-299d-463e-b2b3-006fd718eaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_0.allclose(actual_return_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6d2bfe61-2375-4be5-b1bc-e0ae24b78588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_1.allclose(actual_return_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80318b3c-7751-48c4-9ed3-2c67f80a604a",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc843071-a595-4264-9d9b-d23614963ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.layers.4.mlp.experts.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75a1d1d8-856a-468f-97eb-e8e747cc65d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hidden_states', 'router_indices', 'routing_weights', 'return'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34bfa9a3-3831-40a6-8c83-9fed481d68cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = torch.Tensor(parameters['hidden_states']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b3f2f95-f068-4137-9f9c-89350d04b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_indices = torch.LongTensor(parameters['router_indices']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "485912b8-6c57-4177-872d-1458f118e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_weights = torch.Tensor(parameters['routing_weights']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b109fea-5d14-4c67-b982-faa66b468e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = torch.Tensor(parameters['return']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7478f090-3e13-4104-9a80-5fc21deed4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssExperts(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gate_up_proj = nn.Parameter(torch.empty(32, 2880, 2 * 2880))\n",
    "        self.gate_up_proj_bias = nn.Parameter(torch.empty(32, 2 * 2880))\n",
    "        self.down_proj = nn.Parameter(torch.empty((32, 2880, 2880)))\n",
    "        self.down_proj_bias = nn.Parameter(torch.empty(32, 2880))\n",
    "        self.alpha = 1.702\n",
    "        self.limit = 7.0\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weights=None) -> torch.Tensor:\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        hidden_states = hidden_states.reshape(-1, 2880)\n",
    "        num_experts = routing_weights.shape[1]\n",
    "\n",
    "        next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n",
    "        with torch.no_grad():\n",
    "            expert_mask = torch.nn.functional.one_hot(router_indices, num_classes=num_experts + 1)\n",
    "            expert_mask = expert_mask.permute(2, 1, 0)\n",
    "            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n",
    "        for expert_idx in expert_hit[:]:\n",
    "            expert_idx = expert_idx[0]\n",
    "            with torch.no_grad():\n",
    "                _, token_idx = torch.where(expert_mask[expert_idx])\n",
    "            current_state = hidden_states[token_idx]\n",
    "            gate_up = current_state @ self.gate_up_proj[expert_idx] + self.gate_up_proj_bias[expert_idx]\n",
    "            gate, up = (gate_up[..., ::2], gate_up[..., 1::2])\n",
    "            gate = gate.clamp(min=None, max=self.limit)\n",
    "            up = up.clamp(min=-self.limit, max=self.limit)\n",
    "            glu = gate * torch.sigmoid(gate * self.alpha)\n",
    "            gated_output = (up + 1) * glu\n",
    "            out = gated_output @ self.down_proj[expert_idx] + self.down_proj_bias[expert_idx]\n",
    "            weighted_output = out * routing_weights[token_idx, expert_idx, None]\n",
    "            next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n",
    "        next_states = next_states.view(batch_size, -1, 2880)\n",
    "\n",
    "        return next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "25fd5713-817a-4b16-960e-691774efed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "experts = GptOssExperts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5e522b4f-19d7-42aa-8e06-c991bee36a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experts.load_state_dict(torch.load('model.layers.4.mlp.experts.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "07e55a77-5e65-4a38-a405-86faae9a6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ret = experts(hidden_states, router_indices, routing_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44cd14e5-7c7e-42ad-b074-0d0d53033dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.allclose(actual_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a65764-81ff-4800-b16e-84d9e5fed614",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56039e4d-3e8e-49a6-b77e-dcc1d577cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.layers.8.self_attn.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35174625-b337-444c-b989-5ba464669f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hidden_states', 'attention_mask', 'position_ids', 'use_cache', 'cache_position', 'position_embeddings', 'output_router_logits', 'return'])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0cac0471-84b7-4b27-8fc4-245786f4ab57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.2242e-03,  1.3980e-02, -3.5311e-02,  ..., -1.2268e-02,\n",
       "           6.2347e-03, -5.4394e-03],\n",
       "         [ 2.3627e-01,  3.6567e-01,  1.3929e+00,  ..., -6.9081e-01,\n",
       "           3.9588e-01,  3.1281e-01],\n",
       "         [ 9.4030e-01, -1.1515e-01,  5.0602e-01,  ..., -9.3824e-02,\n",
       "           3.3725e-01, -1.1308e-01],\n",
       "         ...,\n",
       "         [ 3.1247e-01, -1.3037e-01,  2.4110e-01,  ...,  5.6335e-02,\n",
       "          -3.3835e-01,  1.2023e-01],\n",
       "         [ 1.0252e-01, -8.6217e-01,  8.6264e-01,  ..., -6.1494e-01,\n",
       "           1.2398e-01,  1.3620e-01],\n",
       "         [-9.1242e-02, -4.9699e-01,  1.3439e-01,  ..., -7.0277e-03,\n",
       "           3.6442e-02,  4.3396e-01]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = torch.Tensor(parameters['hidden_states']['data'])\n",
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9dd8f180-edce-4fbf-a34e-3742b374f67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.Tensor(parameters['attention_mask']['data'])\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b0c65c0f-c217-4130-8b86-1904b43ff7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = torch.LongTensor(parameters['position_ids']['data'])\n",
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5af3032f-5579-4066-be8a-b0987f518338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['use_cache']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eda38b02-941e-4033-9f91-1a0a679112f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Tensor', 'data': [0, 1, 2, 3, 4, 5, 6]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['cache_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7dfb7a7f-1afc-4901-8648-fb8fb61a2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embeddings_0 = torch.Tensor(parameters['position_embeddings']['items'][0]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "43896d66-a5da-486f-9e07-45b32d9dbacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embeddings_1 = torch.Tensor(parameters['position_embeddings']['items'][1]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4161d88-9655-41f7-975f-58e862cd6ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['output_router_logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f49ca6c3-54f0-4135-8793-adf5b112f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_0 = torch.Tensor(parameters['return']['items'][0]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ea0a406-f6ed-4cbc-b75d-2fd6ccb4cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_1 = torch.Tensor(parameters['return']['items'][1]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a290721-8a04-421d-aca2-f6b86a631812",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_LAYER_TYPES = (\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention'\n",
    ")\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    scaling: float,\n",
    "    dropout: float=0.0,\n",
    "    # **kwargs\n",
    "):\n",
    "    key_states = repeat_kv(key, 8)\n",
    "    value_states = repeat_kv(value, 8)\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = attention_mask[:, :, :, :key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "    sinks = module.sinks.reshape(1, -1, 1, 1).expand(query.shape[0], -1, query.shape[-2], -1)\n",
    "    combined_logits = torch.cat([attn_weights, sinks], dim=-1)\n",
    "    combined_logits = combined_logits - combined_logits.max(dim=-1, keepdim=True).values\n",
    "    probs = F.softmax(combined_logits, dim=-1, dtype=combined_logits.dtype)\n",
    "    scores = probs[..., :-1]\n",
    "    attn_weights = nn.functional.dropout(scores, p=dropout, training=False)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    return (attn_output, attn_weights)\n",
    "\n",
    "\n",
    "def _apply_rotary_emb(\n",
    "    x: torch.Tensor,\n",
    "    cos: torch.Tensor,\n",
    "    sin: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    first_half, second_half = torch.chunk(x, 2, dim=-1)\n",
    "    first_ = first_half * cos - second_half * sin\n",
    "    second_ = second_half * cos + first_half * sin\n",
    "    return torch.cat((first_, second_), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = _apply_rotary_emb(q, cos, sin)\n",
    "    k_embed = _apply_rotary_emb(k, cos, sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class GptOssAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(2880, 64 * 64, bias=True)\n",
    "        self.k_proj = nn.Linear(2880, 8 * 64, bias=True)\n",
    "        self.v_proj = nn.Linear(2880, 8 * 64, bias=True)\n",
    "        self.o_proj = nn.Linear(64 * 64, 2880, bias=True)\n",
    "        self.sinks = nn.Parameter(torch.empty(64))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, 64)\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "        attn_output, attn_weights = eager_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0,\n",
    "            scaling=0.125\n",
    "        )\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return (attn_output, attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cd635915-058b-4ca7-9b7f-cc404a3c2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn = GptOssAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c97a51e-e6f8-4b3e-9c1b-1d8b97576a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attn.load_state_dict(torch.load('model.layers.8.self_attn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d045d49d-c332-4077-84eb-718729f53910",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ret_0, actual_ret_1 = self_attn(hidden_states, attention_mask, (position_embeddings_0, position_embeddings_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "51fa3e31-6c3c-428a-b212-e7c1a009e096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_0.allclose(actual_ret_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "460d1aff-11aa-4f42-8408-6282c126543c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_1.allclose(actual_ret_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672a3da-3a43-4659-bced-5827d5c3724d",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a464971-c5f8-420a-9d45-8130617d508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.router = GptOssTopKRouter()\n",
    "        self.experts = GptOssExperts()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        router_scores, router_indices = self.router(hidden_states)\n",
    "        routed_out = self.experts(hidden_states, router_indices=router_indices, routing_weights=router_scores)\n",
    "        return (routed_out, router_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "18e323cf-d058-496f-9c4d-b2e1ac925918",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = GptOssMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "263f91bd-99cf-4f12-9b28-85bf9241fe15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.load_state_dict(torch.load('model.layers.7.mlp.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06a9c265-574f-4053-b98c-debbdb90580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.layers.7.mlp.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "68671537-085f-46ad-b338-90b9d91cc805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hidden_states', 'return'])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5679bdba-d2da-4ff5-b7cc-90f03ff32140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0026,  0.0128, -0.0469,  ..., -0.0138,  0.0093, -0.0074],\n",
       "         [ 0.5175,  0.2834,  1.4318,  ..., -0.9154,  0.6919,  0.3307],\n",
       "         [ 1.2628, -0.2193,  0.2334,  ..., -0.1061,  0.2105, -0.0426],\n",
       "         ...,\n",
       "         [ 0.3651,  0.1151,  0.0359,  ..., -0.1679, -0.6479,  0.4814],\n",
       "         [ 0.3451, -0.5466,  1.1070,  ..., -0.2724, -0.0070,  0.4061],\n",
       "         [-0.0204, -0.2789,  0.3336,  ...,  0.1099, -0.4804,  0.3660]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = torch.Tensor(parameters['hidden_states']['data'])\n",
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6dd93ee6-8ba1-49a0-b4f7-fbc059b2d4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2821,  1.0698, -0.0336,  ..., -0.2790, -0.1671,  0.0159],\n",
       "         [-0.8354,  1.0031,  2.6871,  ...,  0.2195, -0.4317,  0.5068],\n",
       "         [ 0.3887,  0.6649,  2.4014,  ..., -0.0706,  1.1407, -0.5056],\n",
       "         ...,\n",
       "         [ 0.5996, -1.9755,  1.6059,  ...,  1.2806,  0.3268, -1.1146],\n",
       "         [-0.8949, -4.6857,  1.5567,  ..., -3.7020,  0.9273, -0.8176],\n",
       "         [-0.5841, -2.5539, -0.6914,  ..., -0.6995,  2.2621,  1.3178]]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_0 = torch.Tensor(parameters['return']['items'][0]['data'])\n",
    "return_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d22bce64-0fdd-4465-99e1-4ad71cb7279b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4082,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0473, 0.1839, 0.0000, 0.0000, 0.3606,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2380, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.1453, 0.3785, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2382, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.1256, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.2111, 0.3871, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.2762, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2579, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3414, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.2112, 0.0000, 0.1895, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2907, 0.0000, 0.0000, 0.2391,\n",
       "         0.0000, 0.0000, 0.0000, 0.1452, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.3250, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2939, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2582, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3259, 0.0000, 0.0000, 0.1219,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2286, 0.2702, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2164, 0.2848, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_1 = torch.Tensor(parameters['return']['items'][1]['data'])\n",
    "return_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2c523ef-d910-4c4f-9cf3-bc2449906fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ret_0, actual_ret_1 = mlp(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8635d92e-2299-407a-8814-df2ad6ff56d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_0.allclose(actual_ret_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dac08724-9df6-4689-92a3-ec16d71fb0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_1.allclose(actual_ret_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23112683-c811-46c9-bd70-9e45f925cd65",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "71636c0d-c0b9-4cee-9017-c03d3a1701ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssDecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.self_attn = GptOssAttention()\n",
    "        self.mlp = GptOssMLP()\n",
    "        self.input_layernorm = GptOssRMSNorm()\n",
    "        self.post_attention_layernorm = GptOssRMSNorm()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor]=None,\n",
    "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]]=None,\n",
    "    ) -> torch.Tensor:\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        hidden_states, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_embeddings=position_embeddings,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states, _ = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dc1546c9-084b-4fa9-968f-114a571b9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_20 = GptOssDecoderLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "be934386-576c-48aa-bc18-6ae8f63952e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_20.load_state_dict(torch.load('model.layers.23.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "35d2a5aa-02e1-4338-90f1-8991b948cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.layers.23.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "60709e9a-40da-404e-8a14-1fd8e7ef16e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hidden_states', 'attention_mask', 'position_ids', 'use_cache', 'cache_position', 'position_embeddings', 'output_router_logits', 'return'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "315b8a8a-a564-41b6-aca7-aa31a7dd3ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-310.5319, -112.5024,   -8.2616,  ...,  -17.4091,  -40.4604,\n",
       "          -207.4654],\n",
       "         [-188.8453,  -66.0881,  263.9162,  ..., -236.2886, -201.9604,\n",
       "          -366.8062],\n",
       "         [ 270.5173,   47.0313,  273.6834,  ...,  -70.0861,   -8.4492,\n",
       "          -306.6377],\n",
       "         ...,\n",
       "         [ 273.5627,   99.6852,   58.8853,  ...,  -12.0948,  -77.8574,\n",
       "           -48.6941],\n",
       "         [-193.7547, -217.7804,  160.3706,  ...,   50.5104,   97.3323,\n",
       "          -292.2645],\n",
       "         [-629.9043,   32.1421,  155.6927,  ...,  -42.8831,   68.8168,\n",
       "           347.0712]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = torch.Tensor(parameters['hidden_states']['data'])\n",
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "aa2153ff-288e-4465-bcd1-82e209ed1a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.Tensor(parameters['attention_mask']['data'])\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6cd480d-d45c-4c19-946d-745545f0999a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4., 5., 6.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(parameters['position_ids']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "63fb6ec8-c54b-471d-8cb5-2bcbbe9603de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['use_cache']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "195aca30-3995-402b-b105-c823ac89f1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Tensor', 'data': [0, 1, 2, 3, 4, 5, 6]}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['cache_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a2c3bc5d-5d90-450a-bcd8-8da8ee097a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466],\n",
       "         [ 0.7276,  1.0394,  1.1976,  1.2752,  1.3125,  1.3304,  1.3389,\n",
       "           1.3429,  1.3448,  1.3459,  1.3463,  1.3465,  1.3465,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466],\n",
       "         [-0.5604,  0.2579,  0.7838,  1.0685,  1.2120,  1.2821,  1.3158,\n",
       "           1.3320,  1.3396,  1.3439,  1.3456,  1.3462,  1.3464,  1.3465,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466],\n",
       "         [-1.3331, -0.6412,  0.1965,  0.7485,  1.0502,  1.2030,  1.2778,\n",
       "           1.3138,  1.3310,  1.3405,  1.3443,  1.3458,  1.3463,  1.3465,\n",
       "           1.3465,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466],\n",
       "         [-0.8802, -1.2478, -0.4342,  0.3491,  0.8353,  1.0949,  1.2251,\n",
       "           1.2884,  1.3189,  1.3358,  1.3425,  1.3451,  1.3461,  1.3464,\n",
       "           1.3465,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466],\n",
       "         [ 0.3820, -1.2850, -0.9689, -0.0874,  0.5781,  0.9605,  1.1583,\n",
       "           1.2561,  1.3033,  1.3297,  1.3403,  1.3443,  1.3458,  1.3463,\n",
       "           1.3465,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466],\n",
       "         [ 1.2929, -0.7358, -1.2892, -0.5145,  0.2916,  0.8029,  1.0784,\n",
       "           1.2169,  1.2845,  1.3223,  1.3375,  1.3433,  1.3455,  1.3462,\n",
       "           1.3465,  1.3465,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings_0 = torch.Tensor(parameters['position_embeddings']['items'][0]['data'])\n",
    "position_embeddings_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "de284898-b181-4a54-8cc7-d7733345f258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 1.1331e+00,  8.5615e-01,  6.1558e-01,  4.3271e-01,  3.0098e-01,\n",
       "           2.0831e-01,  1.4384e-01,  9.9212e-02,  6.8394e-02,  4.2687e-02,\n",
       "           2.6034e-02,  1.5609e-02,  9.1498e-03,  5.1982e-03,  2.8194e-03,\n",
       "           1.4174e-03,  6.1469e-04,  1.7414e-04,  5.1586e-05,  3.5545e-05,\n",
       "           2.4492e-05,  1.6876e-05,  1.1628e-05,  8.0124e-06,  5.5209e-06,\n",
       "           3.8042e-06,  2.6212e-06,  1.8061e-06,  1.2445e-06,  8.5753e-07,\n",
       "           5.9087e-07,  4.0714e-07],\n",
       "         [ 1.2244e+00,  1.3216e+00,  1.0950e+00,  8.1952e-01,  5.8673e-01,\n",
       "           4.1161e-01,  2.8604e-01,  1.9789e-01,  1.3661e-01,  8.5331e-02,\n",
       "           5.2059e-02,  3.1216e-02,  1.8299e-02,  1.0396e-02,  5.6389e-03,\n",
       "           2.8348e-03,  1.2294e-03,  3.4827e-04,  1.0317e-04,  7.1090e-05,\n",
       "           4.8984e-05,  3.3752e-05,  2.3257e-05,  1.6025e-05,  1.1042e-05,\n",
       "           7.6083e-06,  5.2425e-06,  3.6123e-06,  2.4890e-06,  1.7151e-06,\n",
       "           1.1817e-06,  8.1428e-07],\n",
       "         [ 1.9003e-01,  1.1841e+00,  1.3322e+00,  1.1194e+00,  8.4279e-01,\n",
       "           6.0500e-01,  4.2496e-01,  2.9548e-01,  2.0448e-01,  1.2789e-01,\n",
       "           7.8064e-02,  4.6819e-02,  2.7448e-02,  1.5594e-02,  8.4583e-03,\n",
       "           4.2522e-03,  1.8441e-03,  5.2241e-04,  1.5476e-04,  1.0663e-04,\n",
       "           7.3476e-05,  5.0628e-05,  3.4885e-05,  2.4037e-05,  1.6563e-05,\n",
       "           1.1412e-05,  7.8637e-06,  5.4184e-06,  3.7335e-06,  2.5726e-06,\n",
       "           1.7726e-06,  1.2214e-06],\n",
       "         [-1.0191e+00,  5.0624e-01,  1.2746e+00,  1.3005e+00,  1.0562e+00,\n",
       "           7.8382e-01,  5.5902e-01,  3.9147e-01,  2.7181e-01,  1.7032e-01,\n",
       "           1.0404e-01,  6.2416e-02,  3.6595e-02,  2.0792e-02,  1.1278e-02,\n",
       "           5.6696e-03,  2.4588e-03,  6.9655e-04,  2.0634e-04,  1.4218e-04,\n",
       "           9.7968e-05,  6.7504e-05,  4.6513e-05,  3.2050e-05,  2.2084e-05,\n",
       "           1.5217e-05,  1.0485e-05,  7.2246e-06,  4.9781e-06,  3.4301e-06,\n",
       "           2.3635e-06,  1.6286e-06],\n",
       "         [-1.2913e+00, -4.0261e-01,  9.3515e-01,  1.3437e+00,  1.2162e+00,\n",
       "           9.4377e-01,  6.8668e-01,  4.8534e-01,  3.3845e-01,  2.1258e-01,\n",
       "           1.2998e-01,  7.8004e-02,  4.5741e-02,  2.5990e-02,  1.4097e-02,\n",
       "           7.0870e-03,  3.0734e-03,  8.7069e-04,  2.5793e-04,  1.7772e-04,\n",
       "           1.2246e-04,  8.4380e-05,  5.8142e-05,  4.0062e-05,  2.7605e-05,\n",
       "           1.9021e-05,  1.3106e-05,  9.0307e-06,  6.2226e-06,  4.2876e-06,\n",
       "           2.9544e-06,  2.0357e-06],\n",
       "         [-3.7625e-01, -1.1277e+00,  3.8880e-01,  1.2444e+00,  1.3146e+00,\n",
       "           1.0810e+00,  8.0648e-01,  5.7656e-01,  4.0421e-01,  2.5462e-01,\n",
       "           1.5587e-01,  9.3582e-02,  5.4884e-02,  3.1187e-02,  1.6916e-02,\n",
       "           8.5044e-03,  3.6881e-03,  1.0448e-03,  3.0951e-04,  2.1327e-04,\n",
       "           1.4695e-04,  1.0126e-04,  6.9770e-05,  4.8075e-05,  3.3126e-05,\n",
       "           2.2825e-05,  1.5727e-05,  1.0837e-05,  7.4671e-06,  5.1452e-06,\n",
       "           3.5452e-06,  2.4428e-06]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings_1 = torch.Tensor(parameters['position_embeddings']['items'][1]['data'])\n",
    "position_embeddings_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "028e0a0b-71be-4ed4-bba8-1797306488ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-344.7182, -150.9824,  -52.5078,  ...,   57.1455, -178.1548,\n",
       "          -157.4110],\n",
       "         [-216.1115,  -68.6140,  281.1167,  ..., -178.8455, -215.4072,\n",
       "          -409.9992],\n",
       "         [ 291.9169,   99.1574,  307.8014,  ...,  -51.2403,  -44.2255,\n",
       "          -315.0646],\n",
       "         ...,\n",
       "         [ 288.8483,  131.1585,   46.5113,  ...,   35.3362,  -84.8533,\n",
       "           -61.4071],\n",
       "         [-253.9920, -223.1962,  185.6435,  ...,  139.4449,  113.5052,\n",
       "          -320.9462],\n",
       "         [-640.1213,  -17.8357,  138.9137,  ...,  100.0607,   93.0733,\n",
       "           382.8972]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = torch.Tensor(parameters['return']['data'])\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "462f4cfd-6736-4d91-83de-5319b0271daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ret = layers_20(hidden_states, attention_mask, (position_embeddings_0, position_embeddings_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3f0c8690-5bda-4fc7-8686-5c8f5b9b1d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.allclose(actual_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6f6b1-bdcc-4b32-8a3c-ffc8e63ae71d",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9861a398-94e0-4b6c-b4eb-8dd9dfaf12a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex\n",
    "\n",
    "\n",
    "def _vmap_for_bhqkv(mask_function):\n",
    "    dimensions = [(None, None, None, 0), (None, None, 0, None), (None, 0, None, None), (0, None, None, None)]\n",
    "    for dims in dimensions:\n",
    "        mask_function = torch.vmap(mask_function, in_dims=dims, out_dims=0)\n",
    "    return mask_function\n",
    "\n",
    "\n",
    "def create_causal_mask(\n",
    "    attention_mask,\n",
    "    dtype,\n",
    "    device,\n",
    "):\n",
    "    batch_size = attention_mask.shape[0]\n",
    "    cur_len = attention_mask.shape[1]\n",
    "    \n",
    "    batch_arange = torch.arange(batch_size, device=device)\n",
    "    head_arange = torch.arange(1, device=device)\n",
    "    q_arange = torch.arange(cur_len, device=device)\n",
    "    kv_arange = torch.arange(cur_len, device=device)\n",
    "\n",
    "    def mask_function(batch_idx, head_idx, q_idx, kv_idx):\n",
    "        return q_idx.new_ones((), dtype=torch.bool) & (kv_idx <= q_idx).to(device) & (attention_mask[batch_idx, kv_idx]).to(device)\n",
    "    \n",
    "    with TransformGetItemToIndex():\n",
    "        mask = _vmap_for_bhqkv(mask_function)(batch_arange, head_arange, q_arange, kv_arange)\n",
    "    \n",
    "    mask = torch.where(\n",
    "        mask,\n",
    "        torch.tensor(0.0, device=device, dtype=dtype),\n",
    "        torch.finfo(dtype).min\n",
    "    )\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_sliding_window_causal_mask(\n",
    "    attention_mask,\n",
    "    dtype,\n",
    "    device,\n",
    "):\n",
    "    batch_size = attention_mask.shape[0]\n",
    "    cur_len = attention_mask.shape[1]\n",
    "    \n",
    "    batch_arange = torch.arange(batch_size, device=device)\n",
    "    head_arange = torch.arange(1, device=device)\n",
    "    q_arange = torch.arange(cur_len, device=device)\n",
    "    kv_arange = torch.arange(cur_len, device=device)\n",
    "\n",
    "    def mask_function(batch_idx, head_idx, q_idx, kv_idx):\n",
    "        return q_idx.new_ones((), dtype=torch.bool) & (kv_idx > q_idx - 128).to(device) & (kv_idx <= q_idx).to(device) & (attention_mask[batch_idx, kv_idx]).to(device)\n",
    "    \n",
    "    with TransformGetItemToIndex():\n",
    "        mask = _vmap_for_bhqkv(mask_function)(batch_arange, head_arange, q_arange, kv_arange)\n",
    "    \n",
    "    mask = torch.where(\n",
    "        mask,\n",
    "        torch.tensor(0.0, device=device, dtype=dtype),\n",
    "        torch.finfo(dtype).min\n",
    "    )\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "44e12333-16fa-483d-8a5c-79d688485205",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(201088, 2880, 199999)\n",
    "        self.layers = nn.ModuleList([GptOssDecoderLayer() for _ in range(24)])\n",
    "        self.norm = GptOssRMSNorm()\n",
    "        self.rotary_emb = GptOssRotaryEmbedding()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_ids: torch.LongTensor,\n",
    "    ):\n",
    "        input_embeddings = self.embed_tokens(input_ids)\n",
    "        causal_mask_mapping = {\n",
    "            'full_attention': create_causal_mask(\n",
    "                attention_mask=attention_mask,\n",
    "                dtype=input_embeddings.dtype,\n",
    "                device=input_embeddings.device,\n",
    "            ),\n",
    "            'sliding_attention': create_sliding_window_causal_mask(\n",
    "                attention_mask=attention_mask,\n",
    "                dtype=input_embeddings.dtype,\n",
    "                device=input_embeddings.device,\n",
    "            )\n",
    "        }\n",
    "        hidden_states = input_embeddings\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "        for decoder_layer, layer_type in zip(self.layers, CONFIG_LAYER_TYPES):\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask_mapping[layer_type],\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ca201fc9-cc85-4c5e-9c25-3d7f630bfebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssForCausalLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = GptOssModel()\n",
    "        self.lm_head = nn.Linear(2880, 201088, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_ids: torch.LongTensor,\n",
    "    ):\n",
    "        hidden_states = self.model(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b65c56ba-f18b-4e74-9341-64871aca1c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "default = GptOssForCausalLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ff605065-220c-463e-899e-14d8f4cb1c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default.load_state_dict(torch.load('default.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ae0ccf4-c45c-4199-8b23-61c4c83939e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('default.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2e95748e-595e-4e29-96a6-3fb7a90b993d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40,  6423,   290, 10915,   328,  2615,   382]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.LongTensor(parameters['input_ids']['data'])\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e7813184-0f56-4946-a572-96e4b975cca4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.BoolTensor(parameters['attention_mask']['data'])\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cc9ce303-9158-47ef-b381-8fc44a2be52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = torch.arange(input_ids.shape[1]).unsqueeze(0)\n",
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "4d4a3fc7-7d8a-4f79-b1d1-c62fddcc0a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.3101e+00,  8.3138e+00,  5.0847e+00,  ...,  5.1554e-02,\n",
       "           3.0999e-02,  4.7686e-02],\n",
       "         [ 5.7981e+00,  8.1965e+00,  1.6099e+00,  ..., -5.6433e-02,\n",
       "          -5.3738e-03,  1.2444e-01],\n",
       "         [-3.1752e-01,  1.7453e+00, -2.5907e+00,  ..., -3.8693e-02,\n",
       "          -1.3778e-01,  8.7426e-02],\n",
       "         ...,\n",
       "         [ 1.6380e+00,  5.2167e+00, -1.5925e+00,  ..., -1.3175e-01,\n",
       "          -2.1799e-02,  8.8453e-02],\n",
       "         [ 7.1273e+00,  1.0012e+01,  3.8354e+00,  ..., -5.9899e-02,\n",
       "           1.7766e-02, -9.3871e-03],\n",
       "         [ 5.4633e+00,  8.1599e+00,  1.5274e+00,  ..., -8.8212e-02,\n",
       "           6.2491e-02,  1.8130e-03]]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.Tensor(parameters['return']['items'][0][1]['data'])\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5a617df2-573b-4097-8a01-6b7d81e58b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4.3101e+00,  8.3138e+00,  5.0847e+00,  ...,  5.1554e-02,\n",
       "           3.0999e-02,  4.7686e-02],\n",
       "         [ 5.7981e+00,  8.1965e+00,  1.6099e+00,  ..., -5.6433e-02,\n",
       "          -5.3738e-03,  1.2444e-01],\n",
       "         [-3.1752e-01,  1.7453e+00, -2.5907e+00,  ..., -3.8693e-02,\n",
       "          -1.3778e-01,  8.7426e-02],\n",
       "         ...,\n",
       "         [ 1.6380e+00,  5.2167e+00, -1.5925e+00,  ..., -1.3175e-01,\n",
       "          -2.1799e-02,  8.8453e-02],\n",
       "         [ 7.1273e+00,  1.0012e+01,  3.8354e+00,  ..., -5.9899e-02,\n",
       "           1.7766e-02, -9.3871e-03],\n",
       "         [ 5.4633e+00,  8.1599e+00,  1.5274e+00,  ..., -8.8212e-02,\n",
       "           6.2491e-02,  1.8130e-03]]], grad_fn=<UnsafeViewBackward0>)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_logits = default(input_ids, attention_mask, position_ids)\n",
    "actual_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "560933fd-7532-4b46-9538-3c449244f465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual_logits.allclose(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fb86ba-e7bb-413e-8827-60daf3adcbf7",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "127babb7-76f3-419f-a849-0cec714b655b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_POSITION_EMBEDDINGS = 131072 # config.max_position_embeddings\n",
    "MAX_LENGTH = 20 # generation_config.max_length\n",
    "TOP_K = 50 # generation_config.top_k\n",
    "EOS_TOKEN_ID = [200002, 199999] # generation_config.eos_token_id\n",
    "PAD_TOKEN_ID = 199999 # generation_config.pad_token_id\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "):  \n",
    "    batch_size = input_ids.shape[0]\n",
    "    cur_len = input_ids.shape[1]\n",
    "\n",
    "    max_length = min(MAX_LENGTH, MAX_POSITION_EMBEDDINGS)\n",
    "    pad_token_tensor = torch.tensor(PAD_TOKEN_ID, device=input_ids.device, dtype=torch.long)\n",
    "    eos_token_tensor = torch.tensor(EOS_TOKEN_ID, device=input_ids.device, dtype=torch.long)\n",
    "\n",
    "    all_sequences_finished = False\n",
    "    unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "    while not all_sequences_finished:\n",
    "        # Fully recompute position_ids for new length\n",
    "        position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "        # Stateless: only pass input_ids, attention_mask, position_ids\n",
    "        logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        # Get probs for next token in sequence\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        top_k = min(max(TOP_K, 1), next_token_logits.size(-1))\n",
    "        indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "        next_token_scores = next_token_logits.masked_fill(indices_to_remove, -float('Inf'))\n",
    "        probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
    "        \n",
    "        next_tokens = (\n",
    "            torch.multinomial(probs, num_samples=1).squeeze(1) * unfinished_sequences\n",
    "            + pad_token_tensor * (1 - unfinished_sequences)\n",
    "        )\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)\n",
    "\n",
    "        is_max_length = torch.full((input_ids.shape[0],), input_ids.shape[1] >= max_length, device=input_ids.device, dtype=torch.bool)\n",
    "        is_eos_token_generated = torch.isin(input_ids[:, -1], eos_token_tensor)\n",
    "        is_stopping = is_max_length | is_eos_token_generated\n",
    "        \n",
    "        unfinished_sequences = unfinished_sequences & ~is_stopping\n",
    "        all_sequences_finished = unfinished_sequences.max() == 0\n",
    "        cur_len += 1\n",
    "\n",
    "        del logits\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "744bee6a-14b7-4f50-bab3-0b9e8fb90852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40,  6423,   290, 10915,   328,  2615,   382,   316,  1652,  5036,\n",
       "           326,   413,  3675,    13,   279,  7924,   382,   261,  6107,   326]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_token_sequences = generate(default, input_ids, attention_mask)\n",
    "output_token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "83a3f156-5ec5-4277-8037-0b17f20fc7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "MODEL_DIRECTORY_PATH = os.path.expanduser('~/models/gpt-oss-20b/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "86bd390b-c0c1-44e6-81cc-a15241a55ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0963c636-8fe7-4d08-9fd9-87d7a5df819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "519b5ab7-8ce8-4874-80b4-3510b0bdd4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I believe the meaning of life is to help others and be kind.\\n\\nThat is a beautiful and']"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(output_token_sequence) for output_token_sequence in output_token_sequences]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
