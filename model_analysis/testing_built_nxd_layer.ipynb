{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9518284-5d27-417e-989d-56795dc1494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:70: UserWarning: Warning: Failed to import blockwise_mm_baseline_shard_n_k1_while_2loops: No module named 'neuronxcc.nki._private_kernels.blockwise_matmul_while'\n",
      "  warnings.warn(f\"Warning: {error}\")\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n"
     ]
    }
   ],
   "source": [
    "from neuronx_distributed_inference.utils.testing import build_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b850c9-94fe-486e-814c-2a06054ed0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def piecewise_safe_division(a, b):\n",
    "    \"\"\"\n",
    "    Piecewise safe division of tensors a/b with special rules for division by zero.\n",
    "    Unravels/flattens `a` and `b` if not already 1-D (handled by summary_stats).\n",
    "    \"\"\"\n",
    "    is_zero = b == 0\n",
    "\n",
    "    # Compute x/y where y != 0\n",
    "    div = a / b\n",
    "\n",
    "    # For x==0 & y==0 --> 0\n",
    "    case1 = (a == 0) & is_zero\n",
    "\n",
    "    # For x<0 & y==0 --> -inf\n",
    "    case2 = (a < 0) & is_zero\n",
    "\n",
    "    # For x>0 & y==0 --> inf\n",
    "    case3 = (a > 0) & is_zero\n",
    "\n",
    "    # Start with normal division\n",
    "    c = div\n",
    "\n",
    "    # Set 0 where x==0 & y==0\n",
    "    c = torch.where(case1, torch.zeros_like(c), c)\n",
    "\n",
    "    # Set -inf where x<0 & y==0\n",
    "    c = torch.where(case2, torch.full_like(c, -float('inf')), c)\n",
    "\n",
    "    # Set inf where x>0 & y==0\n",
    "    c = torch.where(case3, torch.full_like(c, float('inf')), c)\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "def summary_stats(A, B):\n",
    "    if isinstance(A, torch.Tensor) and isinstance(B, torch.Tensor):\n",
    "        with torch.no_grad():\n",
    "            # 1. Check matching shape\n",
    "            if A.shape != B.shape:\n",
    "                raise ValueError(f\"Tensor shape mismatch: {A.shape} vs {B.shape}\")\n",
    "\n",
    "            # 2. Flatten inputs\n",
    "            A = A.contiguous().view(-1)\n",
    "            B = B.contiguous().view(-1)\n",
    "\n",
    "            diff = A - B\n",
    "\n",
    "            abs_error = torch.abs(diff)\n",
    "            min_abs_error = torch.min(abs_error)\n",
    "            median_abs_error = torch.median(abs_error)\n",
    "            max_abs_error = torch.max(abs_error)\n",
    "\n",
    "            denominator = torch.abs(A) + torch.abs(B)\n",
    "            rel_error = piecewise_safe_division(abs_error, denominator)\n",
    "            min_rel_error = torch.min(rel_error)\n",
    "            median_rel_error = torch.median(rel_error)\n",
    "            max_rel_error = torch.max(rel_error)\n",
    "            \n",
    "            print(\"Tensor Comparison Results:\\n\")\n",
    "            print(f\"Absolute Error (min/median/max):  {min_abs_error:.4g}/{median_abs_error:.4g}/{max_abs_error:.4g}\")\n",
    "            print(f\"Relative Error (min/median/max):  {min_rel_error:.4g}/{median_rel_error:.4g}/{max_rel_error:.4g}\")\n",
    "    elif isinstance(A, tuple) and isinstance(B, tuple):\n",
    "        if len(A) != len(B):\n",
    "            raise ValueError(f\"Tuple length mismatch: {len(A)} vs {len(B)}\")\n",
    "\n",
    "        for i, (a, b) in enumerate(zip(A, B)):\n",
    "            print(f\"Element {i}:\")\n",
    "            print()\n",
    "\n",
    "            summary_stats(a, b)\n",
    "            print()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown and/or mismatching types: {type(A)} and {type(B)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225cefaa-8eae-461e-bdb8-d2e4f0ed84fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "MAX_POSITION_EMBEDDINGS = 131072\n",
    "MAX_LENGTH = 20\n",
    "TOP_K = 50\n",
    "EOS_TOKEN_ID = [200002, 199999]\n",
    "PAD_TOKEN_ID = 199999\n",
    "\n",
    "\n",
    "class GptOssTopKRouter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(32, 2880, dtype=torch.bfloat16))\n",
    "        self.bias = nn.Parameter(torch.empty(32, dtype=torch.bfloat16))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = hidden_states.reshape(-1, 2880)\n",
    "        router_logits = F.linear(hidden_states, self.weight, self.bias)  # (seq_len, num_experts)\n",
    "        router_top_value, router_indices = torch.topk(router_logits, 4, dim=-1)  # (seq_len, top_k)\n",
    "        router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n",
    "        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n",
    "        return router_scores, router_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33541ae-6f6b-410d-9d0d-a42b79f6e8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['hidden_states', 'return'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_and_outputs = torch.load('model.layers.7.mlp.router.pt')\n",
    "inputs_and_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e129ed00-0711-4195-899a-cc3d6895fac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bias', 'weight'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = torch.load('model.layers.7.mlp.router.ckpt')\n",
    "state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "436ff784-c6f7-490a-96f0-43c208f58a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Saving to compiler workdir: /tmp/nxdi_test_f35b7a63-2afb-4c81-b35c-b768e754ca5a/compiler_workdir\n",
      "Neuron: Using checkpoint path: model.layers.7.mlp.router.ckpt\n",
      "Neuron: Generating HLOs for the following models: ['GptOssTopKRouter']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-06 19:42:50.314: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-12-06 19:42:50.315: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-12-06 19:42:50.315: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-12-06 19:42:50.316: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-12-06 19:42:50.316: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-12-06 19:42:50.317: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7ff4e5f33910>, 'Ascending Ring PG Group')>\n",
      "[2025-12-06 19:42:50.317: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-12-06 19:42:50.317: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-12-06 19:42:50.318: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-12-06 19:42:50.318: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-12-06 19:42:50.319: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-12-06 19:42:50.319: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Generating 1 hlos for key: GptOssTopKRouter\n",
      "Neuron: Started loading module GptOssTopKRouter\n",
      "Neuron: Finished loading module GptOssTopKRouter in 0.00015592575073242188 seconds\n",
      "Neuron: generating HLO: GptOssTopKRouter, input example shape = torch.Size([1, 7, 2880])\n",
      "Neuron: Finished generating HLO for GptOssTopKRouter in 0.008608341217041016 seconds, input example shape = torch.Size([1, 7, 2880])\n",
      "Neuron: Generated all HLOs in 0.02144002914428711 seconds\n",
      "Neuron: Starting compilation for the priority HLO\n",
      "Neuron: 'GptOssTopKRouter' is the priority model with bucket rank 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-06 19:42:50.000345:  2079  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/nxdi_test_f35b7a63-2afb-4c81-b35c-b768e754ca5a/compiler_workdir/GptOssTopKRouter/_tp0_bk0/model.MODULE_1a742a9f70c4e493a69a+529b46ce.hlo_module.pb --output /tmp/nxdi_test_f35b7a63-2afb-4c81-b35c-b768e754ca5a/compiler_workdir/GptOssTopKRouter/_tp0_bk0/model.MODULE_1a742a9f70c4e493a69a+529b46ce.neff --target=trn1 --enable-saturate-infinity --auto-cast=none --model-type=transformer -O1 --logfile=/tmp/nxdi_test_f35b7a63-2afb-4c81-b35c-b768e754ca5a/compiler_workdir/GptOssTopKRouter/_tp0_bk0/log-neuron-cc.txt --enable-internal-neff-wrapper --verbose=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:283: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Done compilation for the priority HLO in 2.2268388271331787 seconds\n",
      "Neuron: Done optimizing weight layout for all HLOs in 5.269050598144531e-05 seconds\n",
      "Neuron: Starting compilation for all HLOs\n",
      "Neuron: Finished Compilation for all HLOs in 0.0001804828643798828 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Done preparing weight layout transformation\n",
      "Neuron: Sharding Weights for ranks: 0...0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "[2025-12-06 19:42:54.665: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-12-06 19:42:54.665: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-12-06 19:42:54.666: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-12-06 19:42:54.666: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-12-06 19:42:54.667: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-12-06 19:42:54.667: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7ff4e5f33910>, 'Ascending Ring PG Group')>\n",
      "[2025-12-06 19:42:54.668: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-12-06 19:42:54.668: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-12-06 19:42:54.669: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-12-06 19:42:54.669: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-12-06 19:42:54.670: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-12-06 19:42:54.670: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/trace/trace.py:642: UserWarning: Removing redundant keys from checkpoint: []\n",
      "  warnings.warn(f\"Removing redundant keys from checkpoint: {keys_to_delete}\")\n",
      "Neuron: Done Sharding weights in 0.013567253998189699\n",
      "Neuron: NxD Model Initialized\n",
      "Neuron: Finished building model in 9.324246168136597 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NxDModelExecutor(\n",
       "  original_name=NxDModelExecutor\n",
       "  (nxd_model): RecursiveScriptModule(\n",
       "    original_name=NxDModel\n",
       "    (models): RecursiveScriptModule(\n",
       "      original_name=ModuleDict\n",
       "      (GptOssTopKRouter): RecursiveScriptModule(original_name=SPMDBucketModelScript)\n",
       "    )\n",
       "    (flattener_map): RecursiveScriptModule(\n",
       "      original_name=ModuleDict\n",
       "      (GptOssTopKRouter): JITWrapper(original_name=JITWrapper)\n",
       "    )\n",
       "    (packer): JITWrapper(original_name=JITWrapper)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = build_module(\n",
    "    GptOssTopKRouter,\n",
    "    [(inputs_and_outputs['hidden_states'],)],\n",
    "    checkpoint_path='model.layers.7.mlp.router.ckpt'\n",
    ")\n",
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13264613-f37b-4380-a2cd-800ba766b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = module(inputs_and_outputs['hidden_states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77f6fcc8-d110-4902-8b31-a02f8b6e38a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 0:\n",
      "\n",
      "Tensor Comparison Results:\n",
      "\n",
      "Absolute Error (min/median/max):  0/0/0.003906\n",
      "Relative Error (min/median/max):  0/0/0.007721\n",
      "\n",
      "Element 1:\n",
      "\n",
      "Tensor Comparison Results:\n",
      "\n",
      "Absolute Error (min/median/max):  0/0/0\n",
      "Relative Error (min/median/max):  0/0/0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_stats(output, inputs_and_outputs['return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428cddae-d98d-4648-81f1-3739726b3a22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
