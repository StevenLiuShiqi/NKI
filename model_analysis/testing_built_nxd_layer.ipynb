{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9518284-5d27-417e-989d-56795dc1494f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/parallel_layers/layers.py:14: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  from .mappings import (\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:68: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/blockwise.py:70: UserWarning: Warning: Failed to import blockwise_mm_baseline_shard_n_k1_while_2loops: No module named 'neuronxcc.nki._private_kernels.blockwise_matmul_while'\n",
      "  warnings.warn(f\"Warning: {error}\")\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:48: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.\n",
      "  component, error = import_nki(config)\n"
     ]
    }
   ],
   "source": [
    "from neuronx_distributed_inference.utils.testing import build_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88b850c9-94fe-486e-814c-2a06054ed0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def piecewise_safe_division(a, b):\n",
    "    \"\"\"\n",
    "    Piecewise safe division of tensors a/b with special rules for division by zero.\n",
    "    Unravels/flattens `a` and `b` if not already 1-D (handled by summary_stats).\n",
    "    \"\"\"\n",
    "    is_zero = b == 0\n",
    "\n",
    "    # Compute x/y where y != 0\n",
    "    div = a / b\n",
    "\n",
    "    # For x==0 & y==0 --> 0\n",
    "    case1 = (a == 0) & is_zero\n",
    "\n",
    "    # For x<0 & y==0 --> -inf\n",
    "    case2 = (a < 0) & is_zero\n",
    "\n",
    "    # For x>0 & y==0 --> inf\n",
    "    case3 = (a > 0) & is_zero\n",
    "\n",
    "    # Start with normal division\n",
    "    c = div\n",
    "\n",
    "    # Set 0 where x==0 & y==0\n",
    "    c = torch.where(case1, torch.zeros_like(c), c)\n",
    "\n",
    "    # Set -inf where x<0 & y==0\n",
    "    c = torch.where(case2, torch.full_like(c, -float('inf')), c)\n",
    "\n",
    "    # Set inf where x>0 & y==0\n",
    "    c = torch.where(case3, torch.full_like(c, float('inf')), c)\n",
    "\n",
    "    return c\n",
    "\n",
    "\n",
    "def summary_stats(A, B):\n",
    "    if isinstance(A, torch.Tensor) and isinstance(B, torch.Tensor):\n",
    "        with torch.no_grad():\n",
    "            # 1. Check matching shape\n",
    "            if A.shape != B.shape:\n",
    "                raise ValueError(f\"Tensor shape mismatch: {A.shape} vs {B.shape}\")\n",
    "\n",
    "            # 2. Flatten inputs\n",
    "            A = A.contiguous().view(-1)\n",
    "            B = B.contiguous().view(-1)\n",
    "\n",
    "            diff = A - B\n",
    "\n",
    "            abs_error = torch.abs(diff)\n",
    "            min_abs_error = torch.min(abs_error)\n",
    "            median_abs_error = torch.median(abs_error)\n",
    "            max_abs_error = torch.max(abs_error)\n",
    "\n",
    "            denominator = torch.abs(A) + torch.abs(B)\n",
    "            rel_error = piecewise_safe_division(abs_error, denominator)\n",
    "            min_rel_error = torch.min(rel_error)\n",
    "            median_rel_error = torch.median(rel_error)\n",
    "            max_rel_error = torch.max(rel_error)\n",
    "            \n",
    "            print(\"Tensor Comparison Results:\\n\")\n",
    "            print(f\"Absolute Error (min/median/max):  {min_abs_error:.4g}/{median_abs_error:.4g}/{max_abs_error:.4g}\")\n",
    "            print(f\"Relative Error (min/median/max):  {min_rel_error:.4g}/{median_rel_error:.4g}/{max_rel_error:.4g}\")\n",
    "    elif isinstance(A, tuple) and isinstance(B, tuple):\n",
    "        if len(A) != len(B):\n",
    "            raise ValueError(f\"Tuple length mismatch: {len(A)} vs {len(B)}\")\n",
    "\n",
    "        for i, (a, b) in enumerate(zip(A, B)):\n",
    "            print(f\"Element {i}:\")\n",
    "            print()\n",
    "\n",
    "            summary_stats(a, b)\n",
    "            print()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown and/or mismatching types: {type(A)} and {type(B)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225cefaa-8eae-461e-bdb8-d2e4f0ed84fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "MAX_POSITION_EMBEDDINGS = 131072\n",
    "MAX_LENGTH = 20\n",
    "TOP_K = 50\n",
    "EOS_TOKEN_ID = [200002, 199999]\n",
    "PAD_TOKEN_ID = 199999\n",
    "\n",
    "\n",
    "class GptOssTopKRouter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(32, 2880, dtype=torch.bfloat16))\n",
    "        self.bias = nn.Parameter(torch.empty(32, dtype=torch.bfloat16))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = hidden_states.reshape(-1, 2880)\n",
    "        router_logits = F.linear(hidden_states, self.weight, self.bias)  # (seq_len, num_experts)\n",
    "        router_top_value, router_indices = torch.topk(router_logits, 4, dim=-1)  # (seq_len, top_k)\n",
    "        router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n",
    "        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n",
    "        return router_scores, router_indices\n",
    "\n",
    "\n",
    "class GptOssRMSNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(2880, dtype=torch.bfloat16))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + 1e-05)\n",
    "        return (self.weight * hidden_states).to(input_dtype)\n",
    "\n",
    "def get_mscale(scale, mscale=1):\n",
    "    return 0.1 * mscale * math.log(scale) + 1.0\n",
    "\n",
    "\n",
    "def find_correction_dim(num_rotations, dim, base, max_position_embeddings):\n",
    "    return dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi)) / (2 * math.log(base))\n",
    "\n",
    "\n",
    "def find_correction_range(low_rot, high_rot, dim, base, max_position_embeddings):\n",
    "    low = find_correction_dim(low_rot, dim, base, max_position_embeddings)\n",
    "    high = find_correction_dim(high_rot, dim, base, max_position_embeddings)\n",
    "    return (max(low, 0), min(high, dim - 1))\n",
    "\n",
    "\n",
    "def linear_ramp_factor(min, max, dim):\n",
    "    linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n",
    "    ramp_func = torch.clamp(linear_func, 0, 1)\n",
    "    return ramp_func\n",
    "\n",
    "\n",
    "class GptOssRotaryEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attention_scaling = get_mscale(32.0)\n",
    "        \n",
    "        low, high = find_correction_range(32.0, 1.0, 64, 150000, 4096)\n",
    "        inv_freq_extrapolation_factor = 1 - linear_ramp_factor(low, high, 64 // 2)\n",
    "        pos_freqs = 150000 ** (torch.arange(0, 64, 2).to(dtype=torch.float) / 64)\n",
    "        inv_freq_extrapolation = 1.0 / pos_freqs\n",
    "        inv_freq_interpolation = 1.0 / (32.0 * pos_freqs)\n",
    "        inv_freq = inv_freq_interpolation * (1 - inv_freq_extrapolation_factor) + inv_freq_extrapolation * inv_freq_extrapolation_factor\n",
    "        \n",
    "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != 'mps' else 'cpu'\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = freqs\n",
    "            cos = emb.cos() * self.attention_scaling\n",
    "            sin = emb.sin() * self.attention_scaling\n",
    "        return (cos.to(x.dtype), sin.to(x.dtype))\n",
    "\n",
    "\n",
    "class GptOssExperts(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gate_up_proj = nn.Parameter(torch.empty(32, 2880, 2 * 2880, dtype=torch.bfloat16))\n",
    "        self.gate_up_proj_bias = nn.Parameter(torch.empty(32, 2 * 2880, dtype=torch.bfloat16))\n",
    "        self.down_proj = nn.Parameter(torch.empty((32, 2880, 2880), dtype=torch.bfloat16))\n",
    "        self.down_proj_bias = nn.Parameter(torch.empty(32, 2880, dtype=torch.bfloat16))\n",
    "        self.alpha = 1.702\n",
    "        self.limit = 7.0\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weights=None) -> torch.Tensor:\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        hidden_states = hidden_states.reshape(-1, 2880)\n",
    "        num_experts = routing_weights.shape[1]\n",
    "\n",
    "        next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n",
    "        with torch.no_grad():\n",
    "            expert_mask = torch.nn.functional.one_hot(router_indices, num_classes=num_experts + 1)\n",
    "            expert_mask = expert_mask.permute(2, 1, 0)\n",
    "            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n",
    "        for expert_idx in expert_hit[:]:\n",
    "            expert_idx = expert_idx[0]\n",
    "            with torch.no_grad():\n",
    "                _, token_idx = torch.where(expert_mask[expert_idx])\n",
    "            current_state = hidden_states[token_idx]\n",
    "            gate_up = current_state @ self.gate_up_proj[expert_idx] + self.gate_up_proj_bias[expert_idx]\n",
    "            gate, up = (gate_up[..., ::2], gate_up[..., 1::2])\n",
    "            gate = gate.clamp(min=None, max=self.limit)\n",
    "            up = up.clamp(min=-self.limit, max=self.limit)\n",
    "            glu = gate * torch.sigmoid(gate * self.alpha)\n",
    "            gated_output = (up + 1) * glu\n",
    "            out = gated_output @ self.down_proj[expert_idx] + self.down_proj_bias[expert_idx]\n",
    "            weighted_output = out * routing_weights[token_idx, expert_idx, None]\n",
    "            next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n",
    "        next_states = next_states.view(batch_size, -1, 2880)\n",
    "\n",
    "        return next_states\n",
    "\n",
    "\n",
    "CONFIG_LAYER_TYPES = (\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention'\n",
    ")\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    scaling: float,\n",
    "    dropout: float=0.0,\n",
    "    # **kwargs\n",
    "):\n",
    "    key_states = repeat_kv(key, 8)\n",
    "    value_states = repeat_kv(value, 8)\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = attention_mask[:, :, :, :key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "    sinks = module.sinks.reshape(1, -1, 1, 1).expand(query.shape[0], -1, query.shape[-2], -1)\n",
    "    combined_logits = torch.cat([attn_weights, sinks], dim=-1)\n",
    "    combined_logits = combined_logits - combined_logits.max(dim=-1, keepdim=True).values\n",
    "    probs = F.softmax(combined_logits, dim=-1, dtype=combined_logits.dtype)\n",
    "    scores = probs[..., :-1]\n",
    "    attn_weights = nn.functional.dropout(scores, p=dropout, training=False)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    return (attn_output, attn_weights)\n",
    "\n",
    "\n",
    "def _apply_rotary_emb(\n",
    "    x: torch.Tensor,\n",
    "    cos: torch.Tensor,\n",
    "    sin: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    first_half, second_half = torch.chunk(x, 2, dim=-1)\n",
    "    first_ = first_half * cos - second_half * sin\n",
    "    second_ = second_half * cos + first_half * sin\n",
    "    return torch.cat((first_, second_), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = _apply_rotary_emb(q, cos, sin)\n",
    "    k_embed = _apply_rotary_emb(k, cos, sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class GptOssAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(2880, 64 * 64, bias=True, dtype=torch.bfloat16)\n",
    "        self.k_proj = nn.Linear(2880, 8 * 64, bias=True, dtype=torch.bfloat16)\n",
    "        self.v_proj = nn.Linear(2880, 8 * 64, bias=True, dtype=torch.bfloat16)\n",
    "        self.o_proj = nn.Linear(64 * 64, 2880, bias=True, dtype=torch.bfloat16)\n",
    "        self.sinks = nn.Parameter(torch.empty(64, dtype=torch.bfloat16))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, 64)\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "        attn_output, attn_weights = eager_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0,\n",
    "            scaling=0.125\n",
    "        )\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return (attn_output, attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db9dcff-9ee5-4043-866d-503514ccd84c",
   "metadata": {},
   "source": [
    "# `model.layers.7.mlp.router: GptOssTopKRouter`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e33541ae-6f6b-410d-9d0d-a42b79f6e8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['hidden_states', 'return'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_and_outputs = torch.load('model.layers.7.mlp.router.pt')\n",
    "inputs_and_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "436ff784-c6f7-490a-96f0-43c208f58a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Saving to compiler workdir: /tmp/nxdi_test_d2a10e18-f786-4652-a18d-0e56893226d3/compiler_workdir\n",
      "Neuron: Using checkpoint path: model.layers.7.mlp.router.ckpt\n",
      "Neuron: Generating HLOs for the following models: ['GptOssTopKRouter']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-09 20:24:16.533: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-12-09 20:24:16.533: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-12-09 20:24:16.533: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-12-09 20:24:16.534: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-12-09 20:24:16.534: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-12-09 20:24:16.534: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x741ea2eb7c70>, 'Ascending Ring PG Group')>\n",
      "[2025-12-09 20:24:16.535: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-12-09 20:24:16.535: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-12-09 20:24:16.536: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-12-09 20:24:16.536: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-12-09 20:24:16.537: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-12-09 20:24:16.537: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Generating 1 hlos for key: GptOssTopKRouter\n",
      "Neuron: Started loading module GptOssTopKRouter\n",
      "Neuron: Finished loading module GptOssTopKRouter in 0.0012433528900146484 seconds\n",
      "Neuron: generating HLO: GptOssTopKRouter, input example shape = torch.Size([1, 7, 2880])\n",
      "Neuron: Finished generating HLO for GptOssTopKRouter in 0.010995626449584961 seconds, input example shape = torch.Size([1, 7, 2880])\n",
      "Neuron: Generated all HLOs in 0.024591922760009766 seconds\n",
      "Neuron: Starting compilation for the priority HLO\n",
      "Neuron: 'GptOssTopKRouter' is the priority model with bucket rank 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 20:24:16.000566:  4807  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/nxdi_test_d2a10e18-f786-4652-a18d-0e56893226d3/compiler_workdir/GptOssTopKRouter/_tp0_bk0/model.MODULE_1c23f89582a2c5788e67+405457e1.hlo_module.pb --output /tmp/nxdi_test_d2a10e18-f786-4652-a18d-0e56893226d3/compiler_workdir/GptOssTopKRouter/_tp0_bk0/model.MODULE_1c23f89582a2c5788e67+405457e1.neff --target=trn1 --enable-saturate-infinity --auto-cast=none --model-type=transformer -O1 --logfile=/tmp/nxdi_test_d2a10e18-f786-4652-a18d-0e56893226d3/compiler_workdir/GptOssTopKRouter/_tp0_bk0/log-neuron-cc.txt --enable-internal-neff-wrapper --verbose=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:283: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Done compilation for the priority HLO in 2.2458906173706055 seconds\n",
      "Neuron: Done optimizing weight layout for all HLOs in 4.935264587402344e-05 seconds\n",
      "Neuron: Starting compilation for all HLOs\n",
      "Neuron: Finished Compilation for all HLOs in 0.00043392181396484375 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Done preparing weight layout transformation\n",
      "Neuron: Sharding Weights for ranks: 0...0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "[2025-12-09 20:24:20.920: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-12-09 20:24:20.921: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-12-09 20:24:20.921: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-12-09 20:24:20.921: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-12-09 20:24:20.922: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-12-09 20:24:20.922: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x741ea2eb7c70>, 'Ascending Ring PG Group')>\n",
      "[2025-12-09 20:24:20.922: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-12-09 20:24:20.923: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-12-09 20:24:20.923: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-12-09 20:24:20.924: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-12-09 20:24:20.924: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-12-09 20:24:20.924: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/trace/trace.py:642: UserWarning: Removing redundant keys from checkpoint: []\n",
      "  warnings.warn(f\"Removing redundant keys from checkpoint: {keys_to_delete}\")\n",
      "Neuron: Done Sharding weights in 0.012145440000495\n",
      "Neuron: NxD Model Initialized\n",
      "Neuron: Finished building model in 9.677208662033081 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NxDModelExecutor(\n",
       "  original_name=NxDModelExecutor\n",
       "  (nxd_model): RecursiveScriptModule(\n",
       "    original_name=NxDModel\n",
       "    (models): RecursiveScriptModule(\n",
       "      original_name=ModuleDict\n",
       "      (GptOssTopKRouter): RecursiveScriptModule(original_name=SPMDBucketModelScript)\n",
       "    )\n",
       "    (flattener_map): RecursiveScriptModule(\n",
       "      original_name=ModuleDict\n",
       "      (GptOssTopKRouter): JITWrapper(original_name=JITWrapper)\n",
       "    )\n",
       "    (packer): JITWrapper(original_name=JITWrapper)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = build_module(\n",
    "    GptOssTopKRouter,\n",
    "    [(inputs_and_outputs['hidden_states'],)],\n",
    "    checkpoint_path='model.layers.7.mlp.router.ckpt'\n",
    ")\n",
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13264613-f37b-4380-a2cd-800ba766b9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = module(inputs_and_outputs['hidden_states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77f6fcc8-d110-4902-8b31-a02f8b6e38a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 0:\n",
      "\n",
      "Tensor Comparison Results:\n",
      "\n",
      "Absolute Error (min/median/max):  0/0/0.003906\n",
      "Relative Error (min/median/max):  0/0/0.007721\n",
      "\n",
      "Element 1:\n",
      "\n",
      "Tensor Comparison Results:\n",
      "\n",
      "Absolute Error (min/median/max):  0/0/0\n",
      "Relative Error (min/median/max):  0/0/0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_stats(output, inputs_and_outputs['return'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e0ecb2-9750-45b3-8f56-a82c42b649fa",
   "metadata": {},
   "source": [
    "# `model.layers.23.post_attention_layernorm: GptOssRMSNorm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da7a5fe6-7087-471c-a3ae-1bab66bd54ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['hidden_states', 'return'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_and_outputs = torch.load('model.layers.23.post_attention_layernorm.pt')\n",
    "inputs_and_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "483c7c88-78f6-43fa-a787-5a0e6c093649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Saving to compiler workdir: /tmp/nxdi_test_494ef06d-8656-4dfd-b396-831267fed7fb/compiler_workdir\n",
      "Neuron: Using checkpoint path: model.layers.23.post_attention_layernorm.ckpt\n",
      "Neuron: Generating HLOs for the following models: ['GptOssRMSNorm']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-09 20:24:26.240: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-12-09 20:24:26.240: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-12-09 20:24:26.240: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-12-09 20:24:26.241: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-12-09 20:24:26.241: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-12-09 20:24:26.241: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x741ea2eb7c70>, 'Ascending Ring PG Group')>\n",
      "[2025-12-09 20:24:26.242: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-12-09 20:24:26.242: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-12-09 20:24:26.243: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-12-09 20:24:26.243: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-12-09 20:24:26.243: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-12-09 20:24:26.244: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Generating 1 hlos for key: GptOssRMSNorm\n",
      "Neuron: Started loading module GptOssRMSNorm\n",
      "Neuron: Finished loading module GptOssRMSNorm in 0.0002257823944091797 seconds\n",
      "Neuron: generating HLO: GptOssRMSNorm, input example shape = torch.Size([1, 7, 2880])\n",
      "Neuron: Finished generating HLO for GptOssRMSNorm in 0.0027594566345214844 seconds, input example shape = torch.Size([1, 7, 2880])\n",
      "Neuron: Generated all HLOs in 0.016003847122192383 seconds\n",
      "Neuron: Starting compilation for the priority HLO\n",
      "Neuron: 'GptOssRMSNorm' is the priority model with bucket rank 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 20:24:26.000263:  4807  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/nxdi_test_494ef06d-8656-4dfd-b396-831267fed7fb/compiler_workdir/GptOssRMSNorm/_tp0_bk0/model.MODULE_9a5084630e05c9e2eed1+5f3e429c.hlo_module.pb --output /tmp/nxdi_test_494ef06d-8656-4dfd-b396-831267fed7fb/compiler_workdir/GptOssRMSNorm/_tp0_bk0/model.MODULE_9a5084630e05c9e2eed1+5f3e429c.neff --target=trn1 --enable-saturate-infinity --auto-cast=none --model-type=transformer -O1 --logfile=/tmp/nxdi_test_494ef06d-8656-4dfd-b396-831267fed7fb/compiler_workdir/GptOssRMSNorm/_tp0_bk0/log-neuron-cc.txt --enable-internal-neff-wrapper --verbose=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:283: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Done compilation for the priority HLO in 2.2351222038269043 seconds\n",
      "Neuron: No changes on weight layout, skip updating weight layout for other HLOs\n",
      "Neuron: Starting compilation for all HLOs\n",
      "Neuron: Finished Compilation for all HLOs in 0.000377655029296875 seconds\n",
      "Neuron: No changes on weight layout, falling back to the existing weight layout\n",
      "Neuron: Sharding Weights for ranks: 0...0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "[2025-12-09 20:24:28.535: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-12-09 20:24:28.535: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-12-09 20:24:28.536: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-12-09 20:24:28.536: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-12-09 20:24:28.537: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-12-09 20:24:28.537: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x741ea2eb7c70>, 'Ascending Ring PG Group')>\n",
      "[2025-12-09 20:24:28.538: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-12-09 20:24:28.538: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-12-09 20:24:28.538: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-12-09 20:24:28.539: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-12-09 20:24:28.539: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-12-09 20:24:28.540: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/trace/trace.py:642: UserWarning: Removing redundant keys from checkpoint: []\n",
      "  warnings.warn(f\"Removing redundant keys from checkpoint: {keys_to_delete}\")\n",
      "Neuron: Done Sharding weights in 0.012529089999588905\n",
      "Neuron: NxD Model Initialized\n",
      "Neuron: Finished building model in 2.320403814315796 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NxDModelExecutor(\n",
       "  original_name=NxDModelExecutor\n",
       "  (nxd_model): RecursiveScriptModule(\n",
       "    original_name=NxDModel\n",
       "    (models): RecursiveScriptModule(\n",
       "      original_name=ModuleDict\n",
       "      (GptOssRMSNorm): RecursiveScriptModule(original_name=SPMDBucketModelScript)\n",
       "    )\n",
       "    (flattener_map): RecursiveScriptModule(\n",
       "      original_name=ModuleDict\n",
       "      (GptOssRMSNorm): JITWrapper(original_name=JITWrapper)\n",
       "    )\n",
       "    (packer): JITWrapper(original_name=JITWrapper)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = build_module(\n",
    "    GptOssRMSNorm,\n",
    "    [(inputs_and_outputs['hidden_states'],)],\n",
    "    checkpoint_path='model.layers.23.post_attention_layernorm.ckpt'\n",
    ")\n",
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb32ff52-1f27-40c1-aecd-1ed8269a4502",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = module(inputs_and_outputs['hidden_states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2c432fc-829f-4c91-beed-f56236d9ea98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Comparison Results:\n",
      "\n",
      "Absolute Error (min/median/max):  0/0/0\n",
      "Relative Error (min/median/max):  0/0/0\n"
     ]
    }
   ],
   "source": [
    "summary_stats(output, inputs_and_outputs['return'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495b5ef-1213-4de5-8dfe-ea45c2b6380f",
   "metadata": {},
   "source": [
    "# `model.rotary_emb: GptOssRotaryEmbedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c8b6f96-3a1a-4520-bc33-9e21171697bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['x', 'position_ids', 'return'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_and_outputs = torch.load('model.rotary_emb.pt')\n",
    "inputs_and_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7089bed8-d85b-472a-b0a6-383d59b8d63d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Saving to compiler workdir: /tmp/nxdi_test_7f82e13d-aa4e-4caa-b4bf-53c5e8ebc22d/compiler_workdir\n",
      "Neuron: Using checkpoint path: /tmp/nxdi_test_7f82e13d-aa4e-4caa-b4bf-53c5e8ebc22d/checkpoint.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-09 20:24:28.589: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-12-09 20:24:28.589: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-12-09 20:24:28.589: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-12-09 20:24:28.590: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-12-09 20:24:28.590: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-12-09 20:24:28.620: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x741ea2eb7c70>, 'Ascending Ring PG Group')>\n",
      "[2025-12-09 20:24:28.620: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-12-09 20:24:28.621: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-12-09 20:24:28.621: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-12-09 20:24:28.621: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-12-09 20:24:28.622: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-12-09 20:24:28.622: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Generating HLOs for the following models: ['GptOssRotaryEmbedding']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-09 20:24:28.626: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-12-09 20:24:28.627: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-12-09 20:24:28.627: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-12-09 20:24:28.627: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-12-09 20:24:28.628: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-12-09 20:24:28.628: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x741ea2eb7c70>, 'Ascending Ring PG Group')>\n",
      "[2025-12-09 20:24:28.629: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-12-09 20:24:28.629: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-12-09 20:24:28.629: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-12-09 20:24:28.630: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-12-09 20:24:28.630: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-12-09 20:24:28.630: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Generating 1 hlos for key: GptOssRotaryEmbedding\n",
      "Neuron: Started loading module GptOssRotaryEmbedding\n",
      "Neuron: Finished loading module GptOssRotaryEmbedding in 0.00039076805114746094 seconds\n",
      "Neuron: generating HLO: GptOssRotaryEmbedding, input example shape = torch.Size([1, 7, 2880])\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=0, shape=torch.Size([1, 7, 2880]), dtype=torch.bfloat16). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "Neuron: Finished generating HLO for GptOssRotaryEmbedding in 0.0029935836791992188 seconds, input example shape = torch.Size([1, 7, 2880])\n",
      "Neuron: Generated all HLOs in 0.01627326011657715 seconds\n",
      "Neuron: Starting compilation for the priority HLO\n",
      "Neuron: 'GptOssRotaryEmbedding' is the priority model with bucket rank 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 20:24:28.000651:  4807  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/nxdi_test_7f82e13d-aa4e-4caa-b4bf-53c5e8ebc22d/compiler_workdir/GptOssRotaryEmbedding/_tp0_bk0/model.MODULE_c3c9371b7001a1bd251c+b4236ef3.hlo_module.pb --output /tmp/nxdi_test_7f82e13d-aa4e-4caa-b4bf-53c5e8ebc22d/compiler_workdir/GptOssRotaryEmbedding/_tp0_bk0/model.MODULE_c3c9371b7001a1bd251c+b4236ef3.neff --target=trn1 --enable-saturate-infinity --auto-cast=none --model-type=transformer -O1 --logfile=/tmp/nxdi_test_7f82e13d-aa4e-4caa-b4bf-53c5e8ebc22d/compiler_workdir/GptOssRotaryEmbedding/_tp0_bk0/log-neuron-cc.txt --enable-internal-neff-wrapper --verbose=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:283: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Done compilation for the priority HLO in 2.092477798461914 seconds\n",
      "Neuron: No changes on weight layout, skip updating weight layout for other HLOs\n",
      "Neuron: Starting compilation for all HLOs\n",
      "Neuron: Finished Compilation for all HLOs in 0.0003654956817626953 seconds\n",
      "Neuron: No changes on weight layout, falling back to the existing weight layout\n",
      "Neuron: Sharding Weights for ranks: 0...0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "[2025-12-09 20:24:30.781: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-12-09 20:24:30.782: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-12-09 20:24:30.782: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-12-09 20:24:30.783: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-12-09 20:24:30.783: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-12-09 20:24:30.783: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x741ea2eb7c70>, 'Ascending Ring PG Group')>\n",
      "[2025-12-09 20:24:30.784: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-12-09 20:24:30.784: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-12-09 20:24:30.785: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-12-09 20:24:30.785: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-12-09 20:24:30.786: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-12-09 20:24:30.786: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/trace/trace.py:642: UserWarning: Removing redundant keys from checkpoint: []\n",
      "  warnings.warn(f\"Removing redundant keys from checkpoint: {keys_to_delete}\")\n",
      "Neuron: Done Sharding weights in 0.012313436000113143\n",
      "Neuron: NxD Model Initialized\n",
      "Neuron: Finished building model in 2.1781702041625977 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NxDModelExecutor(\n",
       "  original_name=NxDModelExecutor\n",
       "  (nxd_model): RecursiveScriptModule(\n",
       "    original_name=NxDModel\n",
       "    (models): RecursiveScriptModule(\n",
       "      original_name=ModuleDict\n",
       "      (GptOssRotaryEmbedding): RecursiveScriptModule(original_name=SPMDBucketModelScript)\n",
       "    )\n",
       "    (flattener_map): RecursiveScriptModule(\n",
       "      original_name=ModuleDict\n",
       "      (GptOssRotaryEmbedding): JITWrapper(original_name=JITWrapper)\n",
       "    )\n",
       "    (packer): JITWrapper(original_name=JITWrapper)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = build_module(\n",
    "    GptOssRotaryEmbedding,\n",
    "    [(inputs_and_outputs['x'], inputs_and_outputs['position_ids'])],\n",
    ")\n",
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3a755e7-53de-4fa8-b864-4ee5b2c1fcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = module(inputs_and_outputs['x'], inputs_and_outputs['position_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff6cb674-38c0-44b2-b8e8-c044e64f482c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 0:\n",
      "\n",
      "Tensor Comparison Results:\n",
      "\n",
      "Absolute Error (min/median/max):  0/0/0\n",
      "Relative Error (min/median/max):  0/0/0\n",
      "\n",
      "Element 1:\n",
      "\n",
      "Tensor Comparison Results:\n",
      "\n",
      "Absolute Error (min/median/max):  0/0/0\n",
      "Relative Error (min/median/max):  0/0/0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_stats(output, inputs_and_outputs['return'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e7ef38-3c4a-45a9-a325-13491a8ef5a1",
   "metadata": {},
   "source": [
    "# `model.layers.4.mlp.experts: GptOssExperts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "844bd71f-b6ac-4811-9fa6-390c4dac1436",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['hidden_states', 'router_indices', 'routing_weights', 'return'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_and_outputs = torch.load('model.layers.4.mlp.experts.pt')\n",
    "inputs_and_outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db5aa1a6-e77e-4eb6-9f8a-37bfee02c876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Saving to compiler workdir: /tmp/nxdi_test_8ded87a6-8cad-4441-b6c0-31d291841060/compiler_workdir\n",
      "Neuron: Using checkpoint path: model.layers.4.mlp.experts.ckpt\n",
      "Neuron: Generating HLOs for the following models: ['GptOssExperts']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-09 20:24:30.836: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-12-09 20:24:30.836: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-12-09 20:24:30.836: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-12-09 20:24:30.837: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-12-09 20:24:30.837: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-12-09 20:24:30.838: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x741ea2eb7c70>, 'Ascending Ring PG Group')>\n",
      "[2025-12-09 20:24:30.838: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-12-09 20:24:30.839: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-12-09 20:24:30.839: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-12-09 20:24:30.840: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-12-09 20:24:30.840: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-12-09 20:24:30.840: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Generating 1 hlos for key: GptOssExperts\n",
      "Neuron: Started loading module GptOssExperts\n",
      "Neuron: Finished loading module GptOssExperts in 0.0003979206085205078 seconds\n",
      "Neuron: generating HLO: GptOssExperts, input example shape = torch.Size([1, 7, 2880])\n",
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:289: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([7, 4]), dtype=torch.int64). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.\n",
      "  warnings.warn(\n",
      "Neuron: Finished generating HLO for GptOssExperts in 2.261340856552124 seconds, input example shape = torch.Size([1, 7, 2880])\n",
      "Neuron: Generated all HLOs in 2.2778210639953613 seconds\n",
      "Neuron: Starting compilation for the priority HLO\n",
      "Neuron: 'GptOssExperts' is the priority model with bucket rank 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 20:24:33.000144:  4807  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/nxdi_test_8ded87a6-8cad-4441-b6c0-31d291841060/compiler_workdir/GptOssExperts/_tp0_bk0/model.MODULE_29366dfb042adeb577cd+cf4bb07b.hlo_module.pb --output /tmp/nxdi_test_8ded87a6-8cad-4441-b6c0-31d291841060/compiler_workdir/GptOssExperts/_tp0_bk0/model.MODULE_29366dfb042adeb577cd+cf4bb07b.neff --target=trn1 --enable-saturate-infinity --auto-cast=none --model-type=transformer -O1 --logfile=/tmp/nxdi_test_8ded87a6-8cad-4441-b6c0-31d291841060/compiler_workdir/GptOssExperts/_tp0_bk0/log-neuron-cc.txt --enable-internal-neff-wrapper --verbose=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:283: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Done compilation for the priority HLO in 12.811575174331665 seconds\n",
      "Neuron: Done optimizing weight layout for all HLOs in 6.246566772460938e-05 seconds\n",
      "Neuron: Starting compilation for all HLOs\n",
      "Neuron: Finished Compilation for all HLOs in 0.0003807544708251953 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Done preparing weight layout transformation\n",
      "Neuron: Sharding Weights for ranks: 0...0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "[2025-12-09 20:25:01.924: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-12-09 20:25:01.925: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-12-09 20:25:01.925: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-12-09 20:25:01.925: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-12-09 20:25:01.926: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-12-09 20:25:01.926: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x741ea2eb7c70>, 'Ascending Ring PG Group')>\n",
      "[2025-12-09 20:25:01.926: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-12-09 20:25:01.927: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-12-09 20:25:01.927: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-12-09 20:25:01.927: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-12-09 20:25:01.928: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-12-09 20:25:01.928: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/trace/trace.py:642: UserWarning: Removing redundant keys from checkpoint: []\n",
      "  warnings.warn(f\"Removing redundant keys from checkpoint: {keys_to_delete}\")\n",
      "Neuron: Done Sharding weights in 2.1754744519994347\n",
      "Neuron: NxD Model Initialized\n",
      "Neuron: Finished building model in 33.706507205963135 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NxDModelExecutor(\n",
       "  original_name=NxDModelExecutor\n",
       "  (nxd_model): RecursiveScriptModule(\n",
       "    original_name=NxDModel\n",
       "    (models): RecursiveScriptModule(\n",
       "      original_name=ModuleDict\n",
       "      (GptOssExperts): RecursiveScriptModule(original_name=SPMDBucketModelScript)\n",
       "    )\n",
       "    (flattener_map): RecursiveScriptModule(\n",
       "      original_name=ModuleDict\n",
       "      (GptOssExperts): JITWrapper(original_name=JITWrapper)\n",
       "    )\n",
       "    (packer): JITWrapper(original_name=JITWrapper)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = build_module(\n",
    "    GptOssExperts,\n",
    "    [(inputs_and_outputs['hidden_states'], inputs_and_outputs['router_indices'], inputs_and_outputs['routing_weights'])],\n",
    "    checkpoint_path='model.layers.4.mlp.experts.ckpt'\n",
    ")\n",
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17933da7-5783-4071-bdfe-6a1e1d81ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = module(inputs_and_outputs['hidden_states'], inputs_and_outputs['router_indices'], inputs_and_outputs['routing_weights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "01dd122e-a01d-45b3-8eba-2294c7c69912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Comparison Results:\n",
      "\n",
      "Absolute Error (min/median/max):  0/0.003906/1\n",
      "Relative Error (min/median/max):  0/0.004059/1\n"
     ]
    }
   ],
   "source": [
    "summary_stats(output, inputs_and_outputs['return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0c0ec8c-60f4-44ef-8969-eb0f45da7dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0918,  0.3867,  0.2090,  ...,  0.3652, -0.3984, -0.2188],\n",
       "         [-0.1357, -0.1484,  0.1797,  ..., -0.2695,  0.0559, -0.2139],\n",
       "         [ 0.6797, -0.0820,  0.5430,  ..., -0.1006, -0.3555, -0.5117],\n",
       "         ...,\n",
       "         [-0.1074, -0.6250, -0.0918,  ...,  0.2891,  0.1699,  0.2168],\n",
       "         [ 0.6562, -0.7969,  0.8203,  ..., -0.9609, -0.3008,  0.2168],\n",
       "         [-0.4805, -2.3750,  0.5078,  ..., -0.0166,  0.5234,  0.2637]]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2434280c-cad6-4659-a344-bb899cd616a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0918,  0.3906,  0.2100,  ...,  0.3633, -0.3984, -0.2207],\n",
       "         [-0.1289, -0.1621,  0.1836,  ..., -0.2676,  0.0559, -0.2188],\n",
       "         [ 0.6719, -0.0781,  0.5547,  ..., -0.1011, -0.3535, -0.5078],\n",
       "         ...,\n",
       "         [-0.0996, -0.6094, -0.0850,  ...,  0.2988,  0.1650,  0.2148],\n",
       "         [ 0.6562, -0.7852,  0.8203,  ..., -0.9609, -0.2988,  0.2158],\n",
       "         [-0.4844, -2.3594,  0.4941,  ..., -0.0195,  0.5312,  0.2656]]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_and_outputs['return']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e234a0b6-9d38-4af9-b0ee-c5069245b61f",
   "metadata": {},
   "source": [
    "# `model.layers.8.self_attn: GptOssAttention`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98e261e6-0f9e-418e-ad35-46e195bb104c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['hidden_states', 'attention_mask', 'position_embeddings', 'return'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_and_outputs = torch.load('model.layers.8.self_attn.pt')\n",
    "inputs_and_outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3cf2cf-c00b-4693-b10f-e00d5970c853",
   "metadata": {},
   "source": [
    "```\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Cell In[24], line 1\n",
    "----> 1 module = build_module(\n",
    "      2     GptOssAttention,\n",
    "      3     [(inputs_and_outputs['hidden_states'], inputs_and_outputs['attention_mask'], inputs_and_outputs['position_embeddings'])],\n",
    "      4     checkpoint_path='model.layers.8.self_attn.ckpt'\n",
    "      5 )\n",
    "      6 module\n",
    "\n",
    "File /opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed_inference/utils/testing.py:199, in build_module(module_cls, example_inputs, module_init_kwargs, tp_degree, compiler_args, compiler_workdir, checkpoint_path, priority_model_idx, logical_nc_config, dry_run, checkpoint_loader_fn)\n",
    "    173 \"\"\"\n",
    "    174 Compiles a module to Neuron.\n",
    "    175 \n",
    "   (...)\n",
    "    196     The Neuron model, or None if dry run mode is enabled.\n",
    "    197 \"\"\"\n",
    "    198 if not _is_tensor_tuple_list(example_inputs):\n",
    "--> 199     raise ValueError(\"example_inputs must be a list of tensor tuples\")\n",
    "    200 if len(example_inputs) != 1:\n",
    "    201     # Bucketing isn't currently supported for this utility.\n",
    "    202     raise ValueError(\"example_inputs must contain exactly one input\")\n",
    "\n",
    "ValueError: example_inputs must be a list of tensor tuples\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec112862-7966-46a0-8db4-fe17c7be3687",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(2880, 64 * 64, bias=True, dtype=torch.bfloat16)\n",
    "        self.k_proj = nn.Linear(2880, 8 * 64, bias=True, dtype=torch.bfloat16)\n",
    "        self.v_proj = nn.Linear(2880, 8 * 64, bias=True, dtype=torch.bfloat16)\n",
    "        self.o_proj = nn.Linear(64 * 64, 2880, bias=True, dtype=torch.bfloat16)\n",
    "        self.sinks = nn.Parameter(torch.empty(64, dtype=torch.bfloat16))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        cos: torch.Tensor,\n",
    "        sin: torch.Tensor,\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, 64)\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "        attn_output, attn_weights = eager_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0,\n",
    "            scaling=0.125\n",
    "        )\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return (attn_output, attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7fcfb26-af3b-4116-89d3-1b21f136df7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Saving to compiler workdir: /tmp/nxdi_test_b1a04fe3-81d7-4865-925a-b2937c0679f2/compiler_workdir\n",
      "Neuron: Using checkpoint path: model.layers.8.self_attn.ckpt\n",
      "Neuron: Generating HLOs for the following models: ['GptOssAttention']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-12-09 20:25:04.637: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-12-09 20:25:04.637: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-12-09 20:25:04.638: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-12-09 20:25:04.638: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-12-09 20:25:04.638: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-12-09 20:25:04.639: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x741ea2eb7c70>, 'Ascending Ring PG Group')>\n",
      "[2025-12-09 20:25:04.639: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-12-09 20:25:04.639: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-12-09 20:25:04.640: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-12-09 20:25:04.640: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-12-09 20:25:04.640: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-12-09 20:25:04.641: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Generating 1 hlos for key: GptOssAttention\n",
      "Neuron: Started loading module GptOssAttention\n",
      "Neuron: Finished loading module GptOssAttention in 0.1647791862487793 seconds\n",
      "Neuron: generating HLO: GptOssAttention, input example shape = torch.Size([1, 7, 2880])\n",
      "Neuron: Finished generating HLO for GptOssAttention in 0.0631873607635498 seconds, input example shape = torch.Size([1, 7, 2880])\n",
      "Neuron: Generated all HLOs in 0.24099278450012207 seconds\n",
      "Neuron: Starting compilation for the priority HLO\n",
      "Neuron: 'GptOssAttention' is the priority model with bucket rank 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-09 20:25:04.000891:  4807  INFO ||NEURON_CC_WRAPPER||: Call compiler with cmd: neuronx-cc compile --framework=XLA /tmp/nxdi_test_b1a04fe3-81d7-4865-925a-b2937c0679f2/compiler_workdir/GptOssAttention/_tp0_bk0/model.MODULE_68acbf1f8e142092eeee+3c85e442.hlo_module.pb --output /tmp/nxdi_test_b1a04fe3-81d7-4865-925a-b2937c0679f2/compiler_workdir/GptOssAttention/_tp0_bk0/model.MODULE_68acbf1f8e142092eeee+3c85e442.neff --target=trn1 --enable-saturate-infinity --auto-cast=none --model-type=transformer -O1 --logfile=/tmp/nxdi_test_b1a04fe3-81d7-4865-925a-b2937c0679f2/compiler_workdir/GptOssAttention/_tp0_bk0/log-neuron-cc.txt --enable-internal-neff-wrapper --verbose=35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/libneuronxla/neuron_cc_wrapper.py:283: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.\n",
      "  warnings.warn(SyntaxWarning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Done compilation for the priority HLO in 4.13390851020813 seconds\n",
      "Neuron: Done optimizing weight layout for all HLOs in 8.20159912109375e-05 seconds\n",
      "Neuron: Starting compilation for all HLOs\n",
      "Neuron: Finished Compilation for all HLOs in 0.00045418739318847656 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Neuron: Done preparing weight layout transformation\n",
      "Neuron: Sharding Weights for ranks: 0...0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed run_backend_driver.\n",
      "\n",
      "Compiler status PASS\n",
      "[2025-12-09 20:25:12.171: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 1\n",
      "[2025-12-09 20:25:12.171: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1\n",
      "[2025-12-09 20:25:12.171: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1\n",
      "[2025-12-09 20:25:12.172: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1\n",
      "[2025-12-09 20:25:12.172: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 1\n",
      "[2025-12-09 20:25:12.172: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x741ea2eb7c70>, 'Ascending Ring PG Group')>\n",
      "[2025-12-09 20:25:12.173: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0]]\n",
      "[2025-12-09 20:25:12.173: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0]]\n",
      "[2025-12-09 20:25:12.173: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0]]\n",
      "[2025-12-09 20:25:12.173: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0]]\n",
      "[2025-12-09 20:25:12.174: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0]]\n",
      "[2025-12-09 20:25:12.174: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/aws_neuronx_venv_pytorch_2_8_nxd_inference/lib/python3.10/site-packages/neuronx_distributed/trace/trace.py:642: UserWarning: Removing redundant keys from checkpoint: []\n",
      "  warnings.warn(f\"Removing redundant keys from checkpoint: {keys_to_delete}\")\n",
      "Neuron: Done Sharding weights in 0.04947821399946406\n",
      "Neuron: NxD Model Initialized\n",
      "Neuron: Finished building model in 7.660136938095093 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NxDModelExecutor(\n",
       "  original_name=NxDModelExecutor\n",
       "  (nxd_model): RecursiveScriptModule(\n",
       "    original_name=NxDModel\n",
       "    (models): RecursiveScriptModule(\n",
       "      original_name=ModuleDict\n",
       "      (GptOssAttention): RecursiveScriptModule(original_name=SPMDBucketModelScript)\n",
       "    )\n",
       "    (flattener_map): RecursiveScriptModule(\n",
       "      original_name=ModuleDict\n",
       "      (GptOssAttention): JITWrapper(original_name=JITWrapper)\n",
       "    )\n",
       "    (packer): JITWrapper(original_name=JITWrapper)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = build_module(\n",
    "    GptOssAttention,\n",
    "    [(inputs_and_outputs['hidden_states'], inputs_and_outputs['attention_mask'], inputs_and_outputs['position_embeddings'][0], inputs_and_outputs['position_embeddings'][1])],\n",
    "    checkpoint_path='model.layers.8.self_attn.ckpt'\n",
    ")\n",
    "module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d70e442f-59fc-409a-bb70-0005d3df4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = module(inputs_and_outputs['hidden_states'], inputs_and_outputs['attention_mask'], inputs_and_outputs['position_embeddings'][0], inputs_and_outputs['position_embeddings'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3d1931e-e693-4681-832c-0e70f9b77a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element 0:\n",
      "\n",
      "Tensor Comparison Results:\n",
      "\n",
      "Absolute Error (min/median/max):  0/0.01562/2\n",
      "Relative Error (min/median/max):  0/0.007568/1\n",
      "\n",
      "Element 1:\n",
      "\n",
      "Tensor Comparison Results:\n",
      "\n",
      "Absolute Error (min/median/max):  0/0/0.02344\n",
      "Relative Error (min/median/max):  0/0/0.08447\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_stats(output, inputs_and_outputs['return'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7212e77e-fa58-44b8-aaa4-9ccfabdb92df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0000,  6.9375, -1.8281,  ...,  3.0625, -1.3438,  0.4102],\n",
       "         [ 0.2148, -1.4297,  0.7617,  ..., -0.5234, -0.0154, -0.2441],\n",
       "         [-1.6797, -0.8516,  0.3418,  ..., -0.6172, -0.0894, -0.2021],\n",
       "         ...,\n",
       "         [-2.1094, -0.4727,  0.2812,  ..., -1.0625,  0.7578, -0.3496],\n",
       "         [ 0.3203, -2.2969,  0.7812,  ..., -0.8828,  0.1011,  0.1953],\n",
       "         [-2.1875,  0.8750,  2.2969,  ..., -2.3125,  0.0535, -0.6094]]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7676b2c9-dfff-4077-9b66-3b302a01f251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.9922,  6.9062, -1.8359,  ...,  3.0469, -1.3281,  0.4219],\n",
       "         [ 0.2090, -1.4297,  0.7461,  ..., -0.5000, -0.0143, -0.2383],\n",
       "         [-1.7031, -0.8594,  0.3535,  ..., -0.6016, -0.0942, -0.1865],\n",
       "         ...,\n",
       "         [-2.1094, -0.5078,  0.2754,  ..., -1.0312,  0.7578, -0.3281],\n",
       "         [ 0.3340, -2.2812,  0.7734,  ..., -0.8828,  0.1045,  0.1846],\n",
       "         [-2.1562,  0.9102,  2.2812,  ..., -2.2969,  0.0447, -0.6289]]],\n",
       "       dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_and_outputs['return'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a854f7fa-d441-4a1a-9f28-467a8d87d149",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
