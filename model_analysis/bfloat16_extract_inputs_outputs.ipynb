{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cae2da0-5a2a-4d31-90a2-c3ead6b41c64",
   "metadata": {},
   "source": [
    "This file produces the following `.pt` files:\n",
    "\n",
    "```python\n",
    "In [1]: import torch\n",
    "\n",
    "In [2]: inputs_and_outputs = torch.load('model.layers.4.mlp.experts.pt')\n",
    "\n",
    "In [3]: inputs_and_outputs.keys()\n",
    "Out[3]: odict_keys(['hidden_states', 'router_indices', 'routing_weights', 'return'])\n",
    "\n",
    "In [4]: inputs_and_outputs['hidden_states']\n",
    "Out[4]: \n",
    "tensor([[[-0.0488, -0.6914,  1.2109,  ..., -1.4141,  0.1621, -0.6445],\n",
    "         [ 0.8047,  0.2090,  1.3906,  ..., -1.1250,  0.2891,  0.4570],\n",
    "         [ 1.2266, -0.6211,  0.1641,  ...,  0.2207, -0.2119,  0.2754],\n",
    "         ...,\n",
    "         [-0.0302,  0.5508, -0.5352,  ..., -0.0256, -1.2266,  0.4629],\n",
    "         [ 0.2227, -0.1807,  1.0469,  ...,  0.2715, -0.0396,  0.5547],\n",
    "         [-0.5977,  0.9297,  0.2246,  ...,  0.8164, -1.1094, -0.4082]]],\n",
    "       dtype=torch.bfloat16)\n",
    "\n",
    "In [5]: inputs_and_outputs['router_indices']\n",
    "Out[5]: \n",
    "tensor([[23, 31, 19, 24],\n",
    "        [ 6, 18, 20,  4],\n",
    "        [20,  6,  4, 18],\n",
    "        [ 4, 15,  6, 18],\n",
    "        [ 6,  4, 30, 15],\n",
    "        [24, 18,  6, 14],\n",
    "        [ 6, 18,  4, 30]])\n",
    "\n",
    "In [6]: inputs_and_outputs['routing_weights']\n",
    "Out[6]: \n",
    "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.2031, 0.0000, 0.0000, 0.0000, 0.3184, 0.1953, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.2832],\n",
    "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1748, 0.0000, 0.2910, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.2773, 0.0000, 0.2559, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1650, 0.0000, 0.2871, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.1592, 0.0000, 0.3867, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.0000, 0.0000, 0.3223, 0.0000, 0.2461, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2539, 0.0000, 0.0000,\n",
    "         0.1777, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.0000, 0.0000, 0.2852, 0.0000, 0.3027, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1953, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.2168, 0.0000],\n",
    "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2236, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2207, 0.0000, 0.0000, 0.0000,\n",
    "         0.2754, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2812, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
    "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1846, 0.0000, 0.3945, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.2637, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
    "         0.0000, 0.0000, 0.0000, 0.1562, 0.0000]], dtype=torch.bfloat16)\n",
    "\n",
    "In [7]: inputs_and_outputs['return']\n",
    "Out[7]: \n",
    "tensor([[[-0.0918,  0.3906,  0.2100,  ...,  0.3633, -0.3984, -0.2207],\n",
    "         [-0.1289, -0.1621,  0.1836,  ..., -0.2676,  0.0559, -0.2188],\n",
    "         [ 0.6719, -0.0781,  0.5547,  ..., -0.1011, -0.3535, -0.5078],\n",
    "         ...,\n",
    "         [-0.0996, -0.6094, -0.0850,  ...,  0.2988,  0.1650,  0.2148],\n",
    "         [ 0.6562, -0.7852,  0.8203,  ..., -0.9609, -0.2988,  0.2158],\n",
    "         [-0.4844, -2.3594,  0.4941,  ..., -0.0195,  0.5312,  0.2656]]],\n",
    "       dtype=torch.bfloat16)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd6be55-cb30-42e2-890a-538b0b0e8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "MODEL_DIRECTORY_PATH = os.path.expanduser('~/models/gpt-oss-20b/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3d55e4c-ae73-42b0-9b58-287274c156e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from safetensors import safe_open\n",
    "\n",
    "\n",
    "def yield_keys_and_tensors(safetensors_file_names):\n",
    "    for safetensors_file_name in safetensors_file_names:\n",
    "        with safe_open(safetensors_file_name, framework='pt') as f:\n",
    "            for k in f.keys():\n",
    "                yield k, f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67a50756-ed4c-4e6c-a7d7-5e125470a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b48d8c13-0a48-4dbd-b46c-af76bf20acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dbc6276-beba-4b0a-89e8-94239209c21d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ubuntu/models/gpt-oss-20b/model-00004-of-00009.safetensors',\n",
       " '/home/ubuntu/models/gpt-oss-20b/model-00002-of-00009.safetensors',\n",
       " '/home/ubuntu/models/gpt-oss-20b/model-00006-of-00009.safetensors',\n",
       " '/home/ubuntu/models/gpt-oss-20b/model-00003-of-00009.safetensors',\n",
       " '/home/ubuntu/models/gpt-oss-20b/model-00005-of-00009.safetensors',\n",
       " '/home/ubuntu/models/gpt-oss-20b/model-00009-of-00009.safetensors',\n",
       " '/home/ubuntu/models/gpt-oss-20b/model-00001-of-00009.safetensors',\n",
       " '/home/ubuntu/models/gpt-oss-20b/model-00008-of-00009.safetensors',\n",
       " '/home/ubuntu/models/gpt-oss-20b/model-00007-of-00009.safetensors']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safetensors_file_names = glob(os.path.join(MODEL_DIRECTORY_PATH, '*.safetensors'))\n",
    "safetensors_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "939bc664-ce3e-4937-884a-6136a9563bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "MAX_POSITION_EMBEDDINGS = 131072\n",
    "MAX_LENGTH = 20\n",
    "TOP_K = 50\n",
    "EOS_TOKEN_ID = [200002, 199999]\n",
    "PAD_TOKEN_ID = 199999\n",
    "\n",
    "\n",
    "class GptOssTopKRouter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(32, 2880, dtype=torch.bfloat16))\n",
    "        self.bias = nn.Parameter(torch.empty(32, dtype=torch.bfloat16))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = hidden_states.reshape(-1, 2880)\n",
    "        router_logits = F.linear(hidden_states, self.weight, self.bias)  # (seq_len, num_experts)\n",
    "        router_top_value, router_indices = torch.topk(router_logits, 4, dim=-1)  # (seq_len, top_k)\n",
    "        router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n",
    "        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n",
    "        return router_scores, router_indices\n",
    "\n",
    "\n",
    "class GptOssRMSNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(2880, dtype=torch.bfloat16))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + 1e-05)\n",
    "        return (self.weight * hidden_states).to(input_dtype)\n",
    "\n",
    "def get_mscale(scale, mscale=1):\n",
    "    return 0.1 * mscale * math.log(scale) + 1.0\n",
    "\n",
    "\n",
    "def find_correction_dim(num_rotations, dim, base, max_position_embeddings):\n",
    "    return dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi)) / (2 * math.log(base))\n",
    "\n",
    "\n",
    "def find_correction_range(low_rot, high_rot, dim, base, max_position_embeddings):\n",
    "    low = find_correction_dim(low_rot, dim, base, max_position_embeddings)\n",
    "    high = find_correction_dim(high_rot, dim, base, max_position_embeddings)\n",
    "    return (max(low, 0), min(high, dim - 1))\n",
    "\n",
    "\n",
    "def linear_ramp_factor(min, max, dim):\n",
    "    linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n",
    "    ramp_func = torch.clamp(linear_func, 0, 1)\n",
    "    return ramp_func\n",
    "\n",
    "\n",
    "class GptOssRotaryEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attention_scaling = get_mscale(32.0)\n",
    "        \n",
    "        low, high = find_correction_range(32.0, 1.0, 64, 150000, 4096)\n",
    "        inv_freq_extrapolation_factor = 1 - linear_ramp_factor(low, high, 64 // 2)\n",
    "        pos_freqs = 150000 ** (torch.arange(0, 64, 2).to(dtype=torch.float) / 64)\n",
    "        inv_freq_extrapolation = 1.0 / pos_freqs\n",
    "        inv_freq_interpolation = 1.0 / (32.0 * pos_freqs)\n",
    "        inv_freq = inv_freq_interpolation * (1 - inv_freq_extrapolation_factor) + inv_freq_extrapolation * inv_freq_extrapolation_factor\n",
    "        \n",
    "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != 'mps' else 'cpu'\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = freqs\n",
    "            cos = emb.cos() * self.attention_scaling\n",
    "            sin = emb.sin() * self.attention_scaling\n",
    "        return (cos.to(x.dtype), sin.to(x.dtype))\n",
    "\n",
    "\n",
    "class GptOssExperts(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gate_up_proj = nn.Parameter(torch.empty(32, 2880, 2 * 2880, dtype=torch.bfloat16))\n",
    "        self.gate_up_proj_bias = nn.Parameter(torch.empty(32, 2 * 2880, dtype=torch.bfloat16))\n",
    "        self.down_proj = nn.Parameter(torch.empty((32, 2880, 2880), dtype=torch.bfloat16))\n",
    "        self.down_proj_bias = nn.Parameter(torch.empty(32, 2880, dtype=torch.bfloat16))\n",
    "        self.alpha = 1.702\n",
    "        self.limit = 7.0\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weights=None) -> torch.Tensor:\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        hidden_states = hidden_states.reshape(-1, 2880)\n",
    "        num_experts = routing_weights.shape[1]\n",
    "\n",
    "        next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n",
    "        with torch.no_grad():\n",
    "            expert_mask = torch.nn.functional.one_hot(router_indices, num_classes=num_experts + 1)\n",
    "            expert_mask = expert_mask.permute(2, 1, 0)\n",
    "            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n",
    "        for expert_idx in expert_hit[:]:\n",
    "            expert_idx = expert_idx[0]\n",
    "            with torch.no_grad():\n",
    "                _, token_idx = torch.where(expert_mask[expert_idx])\n",
    "            current_state = hidden_states[token_idx]\n",
    "            gate_up = current_state @ self.gate_up_proj[expert_idx] + self.gate_up_proj_bias[expert_idx]\n",
    "            gate, up = (gate_up[..., ::2], gate_up[..., 1::2])\n",
    "            gate = gate.clamp(min=None, max=self.limit)\n",
    "            up = up.clamp(min=-self.limit, max=self.limit)\n",
    "            glu = gate * torch.sigmoid(gate * self.alpha)\n",
    "            gated_output = (up + 1) * glu\n",
    "            out = gated_output @ self.down_proj[expert_idx] + self.down_proj_bias[expert_idx]\n",
    "            weighted_output = out * routing_weights[token_idx, expert_idx, None]\n",
    "            next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n",
    "        next_states = next_states.view(batch_size, -1, 2880)\n",
    "\n",
    "        return next_states\n",
    "\n",
    "\n",
    "CONFIG_LAYER_TYPES = (\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention'\n",
    ")\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    scaling: float,\n",
    "    dropout: float=0.0,\n",
    "    # **kwargs\n",
    "):\n",
    "    key_states = repeat_kv(key, 8)\n",
    "    value_states = repeat_kv(value, 8)\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = attention_mask[:, :, :, :key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "    sinks = module.sinks.reshape(1, -1, 1, 1).expand(query.shape[0], -1, query.shape[-2], -1)\n",
    "    combined_logits = torch.cat([attn_weights, sinks], dim=-1)\n",
    "    combined_logits = combined_logits - combined_logits.max(dim=-1, keepdim=True).values\n",
    "    probs = F.softmax(combined_logits, dim=-1, dtype=combined_logits.dtype)\n",
    "    scores = probs[..., :-1]\n",
    "    attn_weights = nn.functional.dropout(scores, p=dropout, training=False)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    return (attn_output, attn_weights)\n",
    "\n",
    "\n",
    "def _apply_rotary_emb(\n",
    "    x: torch.Tensor,\n",
    "    cos: torch.Tensor,\n",
    "    sin: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    first_half, second_half = torch.chunk(x, 2, dim=-1)\n",
    "    first_ = first_half * cos - second_half * sin\n",
    "    second_ = second_half * cos + first_half * sin\n",
    "    return torch.cat((first_, second_), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = _apply_rotary_emb(q, cos, sin)\n",
    "    k_embed = _apply_rotary_emb(k, cos, sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class GptOssAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(2880, 64 * 64, bias=True, dtype=torch.bfloat16)\n",
    "        self.k_proj = nn.Linear(2880, 8 * 64, bias=True, dtype=torch.bfloat16)\n",
    "        self.v_proj = nn.Linear(2880, 8 * 64, bias=True, dtype=torch.bfloat16)\n",
    "        self.o_proj = nn.Linear(64 * 64, 2880, bias=True, dtype=torch.bfloat16)\n",
    "        self.sinks = nn.Parameter(torch.empty(64, dtype=torch.bfloat16))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, 64)\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "        attn_output, attn_weights = eager_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0,\n",
    "            scaling=0.125\n",
    "        )\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return (attn_output, attn_weights)\n",
    "\n",
    "\n",
    "class GptOssMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.router = GptOssTopKRouter()\n",
    "        self.experts = GptOssExperts()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        router_scores, router_indices = self.router(hidden_states)\n",
    "        routed_out = self.experts(hidden_states, router_indices=router_indices, routing_weights=router_scores)\n",
    "        return (routed_out, router_scores)\n",
    "\n",
    "\n",
    "class GptOssDecoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.self_attn = GptOssAttention()\n",
    "        self.mlp = GptOssMLP()\n",
    "        self.input_layernorm = GptOssRMSNorm()\n",
    "        self.post_attention_layernorm = GptOssRMSNorm()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor]=None,\n",
    "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]]=None,\n",
    "    ) -> torch.Tensor:\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        hidden_states, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_embeddings=position_embeddings,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states, _ = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "def _vmap_for_bhqkv(mask_function):\n",
    "    dimensions = [(None, None, None, 0), (None, None, 0, None), (None, 0, None, None), (0, None, None, None)]\n",
    "    for dims in dimensions:\n",
    "        mask_function = torch.vmap(mask_function, in_dims=dims, out_dims=0)\n",
    "    return mask_function\n",
    "\n",
    "\n",
    "def create_causal_mask(\n",
    "    attention_mask,\n",
    "    dtype,\n",
    "    device,\n",
    "):\n",
    "    batch_size = attention_mask.shape[0]\n",
    "    cur_len = attention_mask.shape[1]\n",
    "    \n",
    "    batch_arange = torch.arange(batch_size, device=device)\n",
    "    head_arange = torch.arange(1, device=device)\n",
    "    q_arange = torch.arange(cur_len, device=device)\n",
    "    kv_arange = torch.arange(cur_len, device=device)\n",
    "\n",
    "    def mask_function(batch_idx, head_idx, q_idx, kv_idx):\n",
    "        return q_idx.new_ones((), dtype=torch.bool) & (kv_idx <= q_idx).to(device) & (attention_mask[batch_idx, kv_idx]).to(device)\n",
    "    \n",
    "    with TransformGetItemToIndex():\n",
    "        mask = _vmap_for_bhqkv(mask_function)(batch_arange, head_arange, q_arange, kv_arange)\n",
    "    \n",
    "    mask = torch.where(\n",
    "        mask,\n",
    "        torch.tensor(0.0, device=device, dtype=dtype),\n",
    "        torch.finfo(dtype).min\n",
    "    )\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def create_sliding_window_causal_mask(\n",
    "    attention_mask,\n",
    "    dtype,\n",
    "    device,\n",
    "):\n",
    "    batch_size = attention_mask.shape[0]\n",
    "    cur_len = attention_mask.shape[1]\n",
    "    \n",
    "    batch_arange = torch.arange(batch_size, device=device)\n",
    "    head_arange = torch.arange(1, device=device)\n",
    "    q_arange = torch.arange(cur_len, device=device)\n",
    "    kv_arange = torch.arange(cur_len, device=device)\n",
    "\n",
    "    def mask_function(batch_idx, head_idx, q_idx, kv_idx):\n",
    "        return q_idx.new_ones((), dtype=torch.bool) & (kv_idx > q_idx - 128).to(device) & (kv_idx <= q_idx).to(device) & (attention_mask[batch_idx, kv_idx]).to(device)\n",
    "    \n",
    "    with TransformGetItemToIndex():\n",
    "        mask = _vmap_for_bhqkv(mask_function)(batch_arange, head_arange, q_arange, kv_arange)\n",
    "    \n",
    "    mask = torch.where(\n",
    "        mask,\n",
    "        torch.tensor(0.0, device=device, dtype=dtype),\n",
    "        torch.finfo(dtype).min\n",
    "    )\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "class GptOssModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(201088, 2880, 199999, dtype=torch.bfloat16)\n",
    "        self.layers = nn.ModuleList([GptOssDecoderLayer() for _ in range(24)])\n",
    "        self.norm = GptOssRMSNorm()\n",
    "        self.rotary_emb = GptOssRotaryEmbedding()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_ids: torch.LongTensor,\n",
    "    ):\n",
    "        input_embeddings = self.embed_tokens(input_ids)\n",
    "        causal_mask_mapping = {\n",
    "            'full_attention': create_causal_mask(\n",
    "                attention_mask=attention_mask,\n",
    "                dtype=input_embeddings.dtype,\n",
    "                device=input_embeddings.device,\n",
    "            ),\n",
    "            'sliding_attention': create_sliding_window_causal_mask(\n",
    "                attention_mask=attention_mask,\n",
    "                dtype=input_embeddings.dtype,\n",
    "                device=input_embeddings.device,\n",
    "            )\n",
    "        }\n",
    "        hidden_states = input_embeddings\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "        for decoder_layer, layer_type in zip(self.layers, CONFIG_LAYER_TYPES):\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask_mapping[layer_type],\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class GptOssForCausalLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = GptOssModel()\n",
    "        self.lm_head = nn.Linear(2880, 201088, bias=False, dtype=torch.bfloat16)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_ids: torch.LongTensor,\n",
    "    ):\n",
    "        hidden_states = self.model(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids)\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        return logits\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(\n",
    "    model,\n",
    "    input_ids,\n",
    "    attention_mask,\n",
    "):  \n",
    "    batch_size = input_ids.shape[0]\n",
    "    cur_len = input_ids.shape[1]\n",
    "\n",
    "    max_length = min(MAX_LENGTH, MAX_POSITION_EMBEDDINGS)\n",
    "    pad_token_tensor = torch.tensor(PAD_TOKEN_ID, device=input_ids.device, dtype=torch.long)\n",
    "    eos_token_tensor = torch.tensor(EOS_TOKEN_ID, device=input_ids.device, dtype=torch.long)\n",
    "\n",
    "    all_sequences_finished = False\n",
    "    unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "    while not all_sequences_finished:\n",
    "        # Fully recompute position_ids for new length\n",
    "        position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "        position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "\n",
    "        # Stateless: only pass input_ids, attention_mask, position_ids\n",
    "        logits = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        # Get probs for next token in sequence\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        top_k = min(max(TOP_K, 1), next_token_logits.size(-1))\n",
    "        indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "        next_token_scores = next_token_logits.masked_fill(indices_to_remove, -float('Inf'))\n",
    "        probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
    "        \n",
    "        next_tokens = (\n",
    "            torch.multinomial(probs, num_samples=1).squeeze(1) * unfinished_sequences\n",
    "            + pad_token_tensor * (1 - unfinished_sequences)\n",
    "        )\n",
    "\n",
    "        input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "        attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)\n",
    "\n",
    "        is_max_length = torch.full((input_ids.shape[0],), input_ids.shape[1] >= max_length, device=input_ids.device, dtype=torch.bool)\n",
    "        is_eos_token_generated = torch.isin(input_ids[:, -1], eos_token_tensor)\n",
    "        is_stopping = is_max_length | is_eos_token_generated\n",
    "        \n",
    "        unfinished_sequences = unfinished_sequences & ~is_stopping\n",
    "        all_sequences_finished = unfinished_sequences.max() == 0\n",
    "        cur_len += 1\n",
    "\n",
    "        del logits\n",
    "\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afea395b-943f-4356-a60a-23951931a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GptOssForCausalLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "583251f6-92a0-4639-87d5-0be4ccf2d3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()   # map of key->parameter/buffer (references, not clones)\n",
    "for key, tensor in yield_keys_and_tensors(safetensors_file_names):\n",
    "    if key not in state_dict:\n",
    "        print(f\"Warning: {key} not in model's state dict\")\n",
    "        continue\n",
    "    state_tensor = state_dict[key]\n",
    "    # Copy tensor data to the parameter/buffer (move to proper device if necessary)\n",
    "    state_tensor.copy_(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c867879a-9c4e-4b71-af55-8d6a69caff7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from inspect import signature\n",
    "from json import dump, dumps\n",
    "from time import time\n",
    "from types import MethodType\n",
    "from torch import Tensor, save\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "\n",
    "class ForwardWrapper(object):\n",
    "    def __init__(self, forward, name):\n",
    "        if not isinstance(forward, MethodType):\n",
    "            raise TypeError('not isinstance(forward, MethodType)')\n",
    "\n",
    "        self.forward = forward\n",
    "        self.parameters = list(signature(forward).parameters.keys())\n",
    "        self.instance = forward.__self__\n",
    "        self.name = name\n",
    "        self.recorded = False\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if not self.recorded:\n",
    "            exported_parameters = OrderedDict()\n",
    "            \n",
    "            for (parameter, arg) in zip(self.parameters, args):\n",
    "                exported_parameters[parameter] = arg\n",
    "            \n",
    "            for (parameter, arg) in kwargs.items():\n",
    "                exported_parameters[parameter] = arg\n",
    "            \n",
    "            result = self.forward(*args, **kwargs)\n",
    "            \n",
    "            exported_parameters['return'] = result\n",
    "\n",
    "            torch.save(exported_parameters, '%s.pt' % (self.name,))\n",
    "            \n",
    "            self.recorded = True\n",
    "            \n",
    "            return result\n",
    "        else:\n",
    "            return self.forward(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d6f7e93-b905-4e58-9a06-73494ed58e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a12b87f-1beb-4e1c-a1ff-26c4f098ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "module_types_to_names_and_modules = {}\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    module_type = type(module)\n",
    "    if not module_type.__module__.startswith('torch'):\n",
    "        module_types_to_names_and_modules.setdefault(module_type, []).append((name, module))\n",
    "\n",
    "for names_and_modules in module_types_to_names_and_modules.values():\n",
    "    name, module = random.choice(names_and_modules)\n",
    "\n",
    "    if name in ('', 'model'): continue\n",
    "    \n",
    "    module.forward = ForwardWrapper(module.forward, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "92a695f6-6f96-435c-86d0-27c4b6746781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f4efe8a-1e52-49d4-bca7-f1e8be477549",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_DIRECTORY_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0aa3e33-8959-485f-ab4c-124331b6c059",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.LongTensor([[   40,  6423,   290, 10915,   328,  2615,   382]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59777236-23af-4f73-a155-4ba6d1cd01be",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.BoolTensor([[True, True, True, True, True, True, True]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6eca4cf5-9db5-4393-a0ac-8af641df42ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   40,  6423,   290, 10915,   328,  2615,   382,   484,   480,   382,\n",
       "           261, 79030,   326,  3832,  8496,   484,   382, 73759,   306, 50655]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_token_sequences = generate(model, input_ids, attention_mask)\n",
    "output_token_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17a08587-59f2-499d-b15b-20868f7d91df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I believe the meaning of life is that it is a philosophical and personal concept that is rooted in one's\"]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(output_token_sequence) for output_token_sequence in output_token_sequences]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
