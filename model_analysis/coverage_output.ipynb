{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13307221-54c8-40c9-847e-d92656a7847b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c09cc79-d80e-464b-9066-5abfb8d973ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sketches.json', 'r') as fp:\n",
    "    sketches = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6900ebc0-db56-43cf-8f18-838ac47372be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from typing import Callable, Optional, Union\n",
      "import torch\n",
      "from torch import nn\n",
      "from torch.nn import functional as F\n",
      "from ...cache_utils import Cache, DynamicCache\n",
      "from ...generation import GenerationMixin\n",
      "from ...integrations.hub_kernels import use_kernel_forward_from_hub\n",
      "from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n",
      "from ...modeling_layers import GenericForSequenceClassification, GenericForTokenClassification, GradientCheckpointingLayer\n",
      "from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n",
      "from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n",
      "from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n",
      "from ...processing_utils import Unpack\n",
      "from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n",
      "from ...utils.deprecation import deprecate_kwarg\n",
      "from ...utils.generic import OutputRecorder, check_model_inputs\n",
      "from .configuration_gpt_oss import GptOssConfig\n",
      "\n",
      "@use_kernel_forward_from_hub('RMSNorm')\n",
      "class GptOssRMSNorm(nn.Module):\n",
      "\n",
      "    def __init__(self, hidden_size, eps=1e-06):...\n",
      "        super().__init__()\n",
      "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
      "        self.variance_epsilon = eps\n",
      "\n",
      "    def forward(self, hidden_states):\n",
      "        input_dtype = hidden_states.dtype\n",
      "        hidden_states = hidden_states.to(torch.float32)\n",
      "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
      "        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
      "        return (self.weight * hidden_states).to(input_dtype)\n",
      "\n",
      "    def extra_repr(self):...\n",
      "\n",
      "class GptOssExperts(nn.Module):\n",
      "\n",
      "    def __init__(self, config):\n",
      "        super().__init__()\n",
      "        self.intermediate_size = config.intermediate_size\n",
      "        self.num_experts = config.num_local_experts\n",
      "        self.hidden_size = config.hidden_size\n",
      "        self.expert_dim = self.intermediate_size\n",
      "        self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n",
      "        self.gate_up_proj_bias = nn.Parameter(torch.empty(self.num_experts, 2 * self.expert_dim))\n",
      "        self.down_proj = nn.Parameter(torch.empty((self.num_experts, self.expert_dim, self.hidden_size)))\n",
      "        self.down_proj_bias = nn.Parameter(torch.empty(self.num_experts, self.hidden_size))\n",
      "        self.alpha = 1.702\n",
      "        self.limit = 7.0\n",
      "\n",
      "    def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weights=None) -> torch.Tensor:...\n",
      "        batch_size = hidden_states.shape[0]\n",
      "        hidden_states = hidden_states.reshape(-1, self.hidden_size)\n",
      "        num_experts = routing_weights.shape[1]\n",
      "        if hidden_states.device.type == 'cpu' or self.training:\n",
      "            next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n",
      "            with torch.no_grad():\n",
      "                expert_mask = torch.nn.functional.one_hot(router_indices, num_classes=num_experts + 1)\n",
      "                expert_mask = expert_mask.permute(2, 1, 0)\n",
      "                expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n",
      "            for expert_idx in expert_hit[:]:\n",
      "                expert_idx = expert_idx[0]\n",
      "                if expert_idx == num_experts:...\n",
      "                with torch.no_grad():\n",
      "                    _, token_idx = torch.where(expert_mask[expert_idx])\n",
      "                current_state = hidden_states[token_idx]\n",
      "                gate_up = current_state @ self.gate_up_proj[expert_idx] + self.gate_up_proj_bias[expert_idx]\n",
      "                gate, up = (gate_up[..., ::2], gate_up[..., 1::2])\n",
      "                gate = gate.clamp(min=None, max=self.limit)\n",
      "                up = up.clamp(min=-self.limit, max=self.limit)\n",
      "                glu = gate * torch.sigmoid(gate * self.alpha)\n",
      "                gated_output = (up + 1) * glu\n",
      "                out = gated_output @ self.down_proj[expert_idx] + self.down_proj_bias[expert_idx]\n",
      "                weighted_output = out * routing_weights[token_idx, expert_idx, None]\n",
      "                next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n",
      "            next_states = next_states.view(batch_size, -1, self.hidden_size)\n",
      "        else:...\n",
      "        return next_states\n",
      "\n",
      "class GptOssTopKRouter(nn.Module):\n",
      "\n",
      "    def __init__(self, config):\n",
      "        super().__init__()\n",
      "        self.top_k = config.num_experts_per_tok\n",
      "        self.num_experts = config.num_local_experts\n",
      "        self.hidden_dim = config.hidden_size\n",
      "        self.weight = nn.Parameter(torch.empty(self.num_experts, self.hidden_dim))\n",
      "        self.bias = nn.Parameter(torch.empty(self.num_experts))\n",
      "\n",
      "    def forward(self, hidden_states):\n",
      "        hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n",
      "        router_logits = F.linear(hidden_states, self.weight, self.bias)\n",
      "        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)\n",
      "        router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n",
      "        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n",
      "        return (router_scores, router_indices)\n",
      "\n",
      "@use_kernel_forward_from_hub('MegaBlocksMoeMLP')\n",
      "class GptOssMLP(nn.Module):\n",
      "\n",
      "    def __init__(self, config):\n",
      "        super().__init__()\n",
      "        self.router = GptOssTopKRouter(config)\n",
      "        self.experts = GptOssExperts(config)\n",
      "\n",
      "    def forward(self, hidden_states):\n",
      "        router_scores, router_indices = self.router(hidden_states)\n",
      "        routed_out = self.experts(hidden_states, router_indices=router_indices, routing_weights=router_scores)\n",
      "        return (routed_out, router_scores)\n",
      "\n",
      "class GptOssRotaryEmbedding(nn.Module):\n",
      "    inv_freq: torch.Tensor\n",
      "\n",
      "    def __init__(self, config: GptOssConfig, device=None):\n",
      "        super().__init__()\n",
      "        if hasattr(config, 'rope_scaling') and isinstance(config.rope_scaling, dict):\n",
      "            self.rope_type = config.rope_scaling.get('rope_type', config.rope_scaling.get('type'))\n",
      "        else:...\n",
      "        self.max_seq_len_cached = config.max_position_embeddings\n",
      "        self.original_max_seq_len = config.max_position_embeddings\n",
      "        self.config = config\n",
      "        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n",
      "        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n",
      "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
      "        self.original_inv_freq = self.inv_freq\n",
      "\n",
      "    @torch.no_grad()\n",
      "    @dynamic_rope_update\n",
      "    def forward(self, x, position_ids):\n",
      "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
      "        position_ids_expanded = position_ids[:, None, :].float()\n",
      "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != 'mps' else 'cpu'\n",
      "        with torch.autocast(device_type=device_type, enabled=False):\n",
      "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
      "            emb = freqs\n",
      "            cos = emb.cos() * self.attention_scaling\n",
      "            sin = emb.sin() * self.attention_scaling\n",
      "        return (cos.to(x.dtype), sin.to(x.dtype))\n",
      "\n",
      "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:...\n",
      "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
      "    if n_rep == 1:...\n",
      "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
      "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
      "\n",
      "def _apply_rotary_emb(x: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor) -> torch.Tensor:\n",
      "    first_half, second_half = torch.chunk(x, 2, dim=-1)\n",
      "    first_ = first_half * cos - second_half * sin\n",
      "    second_ = second_half * cos + first_half * sin\n",
      "    return torch.cat((first_, second_), dim=-1)\n",
      "\n",
      "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
      "    cos = cos.unsqueeze(unsqueeze_dim)\n",
      "    sin = sin.unsqueeze(unsqueeze_dim)\n",
      "    q_embed = _apply_rotary_emb(q, cos, sin)\n",
      "    k_embed = _apply_rotary_emb(k, cos, sin)\n",
      "    return (q_embed, k_embed)\n",
      "\n",
      "def eager_attention_forward(module: nn.Module, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attention_mask: Optional[torch.Tensor], scaling: float, dropout: float=0.0, **kwargs):\n",
      "    key_states = repeat_kv(key, module.num_key_value_groups)\n",
      "    value_states = repeat_kv(value, module.num_key_value_groups)\n",
      "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
      "    if attention_mask is not None:\n",
      "        causal_mask = attention_mask[:, :, :, :key_states.shape[-2]]\n",
      "        attn_weights = attn_weights + causal_mask\n",
      "    sinks = module.sinks.reshape(1, -1, 1, 1).expand(query.shape[0], -1, query.shape[-2], -1)\n",
      "    combined_logits = torch.cat([attn_weights, sinks], dim=-1)\n",
      "    combined_logits = combined_logits - combined_logits.max(dim=-1, keepdim=True).values\n",
      "    probs = F.softmax(combined_logits, dim=-1, dtype=combined_logits.dtype)\n",
      "    scores = probs[..., :-1]\n",
      "    attn_weights = nn.functional.dropout(scores, p=dropout, training=module.training)\n",
      "    attn_output = torch.matmul(attn_weights, value_states)\n",
      "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
      "    return (attn_output, attn_weights)\n",
      "\n",
      "class GptOssAttention(nn.Module):\n",
      "\n",
      "    def __init__(self, config: GptOssConfig, layer_idx: int):\n",
      "        super().__init__()\n",
      "        self.config = config\n",
      "        self.layer_idx = layer_idx\n",
      "        self.head_dim = getattr(config, 'head_dim', config.hidden_size // config.num_attention_heads)\n",
      "        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n",
      "        self.scaling = self.head_dim ** (-0.5)\n",
      "        self.attention_dropout = config.attention_dropout\n",
      "        self.is_causal = True\n",
      "        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias)\n",
      "        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
      "        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n",
      "        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias)\n",
      "        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == 'sliding_attention' else None\n",
      "        self.sinks = nn.Parameter(torch.empty(config.num_attention_heads))\n",
      "\n",
      "    @deprecate_kwarg('past_key_value', new_name='past_key_values', version='4.58')\n",
      "    def forward(self, hidden_states: torch.Tensor, position_embeddings: tuple[torch.Tensor, torch.Tensor], attention_mask: Optional[torch.Tensor], past_key_values: Optional[Cache]=None, cache_position: Optional[torch.LongTensor]=None, **kwargs: Unpack[TransformersKwargs]) -> tuple[torch.Tensor, torch.Tensor]:\n",
      "        input_shape = hidden_states.shape[:-1]\n",
      "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
      "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
      "        cos, sin = position_embeddings\n",
      "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
      "        if past_key_values is not None:\n",
      "            cache_kwargs = {'cache_position': cache_position}\n",
      "            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
      "        attention_interface: Callable = eager_attention_forward\n",
      "        if self.config._attn_implementation != 'eager':...\n",
      "        attn_output, attn_weights = attention_interface(self, query_states, key_states, value_states, attention_mask, dropout=0.0 if not self.training else self.attention_dropout, scaling=self.scaling, sliding_window=self.sliding_window, s_aux=self.sinks, **kwargs)\n",
      "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
      "        attn_output = self.o_proj(attn_output)\n",
      "        return (attn_output, attn_weights)\n",
      "\n",
      "class GptOssDecoderLayer(GradientCheckpointingLayer):\n",
      "\n",
      "    def __init__(self, config: GptOssConfig, layer_idx: int):\n",
      "        super().__init__()\n",
      "        self.hidden_size = config.hidden_size\n",
      "        self.self_attn = GptOssAttention(config=config, layer_idx=layer_idx)\n",
      "        self.mlp = GptOssMLP(config)\n",
      "        self.input_layernorm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
      "        self.post_attention_layernorm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
      "        self.attention_type = config.layer_types[layer_idx]\n",
      "\n",
      "    @deprecate_kwarg('past_key_value', new_name='past_key_values', version='4.58')\n",
      "    def forward(self, hidden_states: torch.Tensor, attention_mask: Optional[torch.Tensor]=None, position_ids: Optional[torch.LongTensor]=None, past_key_values: Optional[Cache]=None, use_cache: Optional[bool]=False, cache_position: Optional[torch.LongTensor]=None, position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]]=None, **kwargs: Unpack[TransformersKwargs]) -> torch.Tensor:\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.input_layernorm(hidden_states)\n",
      "        hidden_states, _ = self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, position_ids=position_ids, past_key_values=past_key_values, use_cache=use_cache, cache_position=cache_position, position_embeddings=position_embeddings, **kwargs)\n",
      "        hidden_states = residual + hidden_states\n",
      "        residual = hidden_states\n",
      "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
      "        hidden_states, _ = self.mlp(hidden_states)\n",
      "        hidden_states = residual + hidden_states\n",
      "        return hidden_states\n",
      "\n",
      "@auto_docstring\n",
      "class GptOssPreTrainedModel(PreTrainedModel):\n",
      "    config: GptOssConfig\n",
      "    base_model_prefix = 'model'\n",
      "    supports_gradient_checkpointing = True\n",
      "    _no_split_modules = ['GptOssDecoderLayer']\n",
      "    _skip_keys_device_placement = ['past_key_values']\n",
      "    _supports_flash_attn = True\n",
      "    _supports_sdpa = False\n",
      "    _supports_flex_attn = True\n",
      "    _can_compile_fullgraph = True\n",
      "    _supports_attention_backend = True\n",
      "    _can_record_outputs = {'router_logits': OutputRecorder(GptOssTopKRouter, index=0), 'hidden_states': GptOssDecoderLayer, 'attentions': GptOssAttention}\n",
      "    _keep_in_fp32_modules = ['post_attention_layernorm', 'input_layernorm', 'norm']\n",
      "    _supports_flash_attention = False\n",
      "    _supports_flex_attention = False\n",
      "\n",
      "    def _init_weights(self, module):\n",
      "        std = self.config.initializer_range\n",
      "        if isinstance(module, nn.Linear):...\n",
      "        elif isinstance(module, nn.Parameter):...\n",
      "        elif isinstance(module, nn.Embedding):...\n",
      "        elif isinstance(module, GptOssRMSNorm):...\n",
      "        elif isinstance(module, GptOssExperts):...\n",
      "        elif isinstance(module, GptOssAttention):...\n",
      "        elif isinstance(module, GptOssTopKRouter):...\n",
      "\n",
      "@auto_docstring\n",
      "class GptOssModel(GptOssPreTrainedModel):\n",
      "    _no_split_modules = ['GptOssDecoderLayer']\n",
      "\n",
      "    def __init__(self, config: GptOssConfig):\n",
      "        super().__init__(config)\n",
      "        self.padding_idx = config.pad_token_id\n",
      "        self.vocab_size = config.vocab_size\n",
      "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
      "        self.layers = nn.ModuleList([GptOssDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)])\n",
      "        self.norm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
      "        self.rotary_emb = GptOssRotaryEmbedding(config=config)\n",
      "        self.gradient_checkpointing = False\n",
      "        self.post_init()\n",
      "\n",
      "    @check_model_inputs\n",
      "    @auto_docstring\n",
      "    def forward(self, input_ids: Optional[torch.LongTensor]=None, attention_mask: Optional[torch.Tensor]=None, position_ids: Optional[torch.LongTensor]=None, past_key_values: Optional[Cache]=None, inputs_embeds: Optional[torch.FloatTensor]=None, use_cache: Optional[bool]=None, cache_position: Optional[torch.LongTensor]=None, **kwargs: Unpack[TransformersKwargs]) -> MoeModelOutputWithPast:\n",
      "        if (input_ids is None) ^ (inputs_embeds is not None):...\n",
      "        if use_cache and past_key_values is None:...\n",
      "        if inputs_embeds is None:\n",
      "            inputs_embeds = self.embed_tokens(input_ids)\n",
      "        if cache_position is None:...\n",
      "        if position_ids is None:...\n",
      "        if not isinstance((causal_mask_mapping := attention_mask), dict):\n",
      "            mask_kwargs = {'config': self.config, 'input_embeds': inputs_embeds, 'attention_mask': attention_mask, 'cache_position': cache_position, 'past_key_values': past_key_values}\n",
      "            causal_mask_mapping = {'full_attention': create_causal_mask(**mask_kwargs), 'sliding_attention': create_sliding_window_causal_mask(**mask_kwargs)}\n",
      "        hidden_states = inputs_embeds\n",
      "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
      "        for decoder_layer in self.layers:\n",
      "            hidden_states = decoder_layer(hidden_states, attention_mask=causal_mask_mapping[decoder_layer.attention_type], position_ids=position_ids, past_key_values=past_key_values, use_cache=use_cache, cache_position=cache_position, position_embeddings=position_embeddings, **kwargs)\n",
      "        hidden_states = self.norm(hidden_states)\n",
      "        return MoeModelOutputWithPast(last_hidden_state=hidden_states, past_key_values=past_key_values)\n",
      "\n",
      "def load_balancing_loss_func(gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None], num_experts: Optional[int]=None, top_k=2, attention_mask: Optional[torch.Tensor]=None) -> Union[torch.Tensor, int]:...\n",
      "\n",
      "@auto_docstring\n",
      "class GptOssForCausalLM(GptOssPreTrainedModel, GenerationMixin):\n",
      "    _tied_weights_keys = ['lm_head.weight']\n",
      "    _tp_plan = {'lm_head': 'colwise_rep'}\n",
      "    _pp_plan = {'lm_head': (['hidden_states'], ['logits'])}\n",
      "\n",
      "    def __init__(self, config):\n",
      "        super().__init__(config)\n",
      "        self.model = GptOssModel(config)\n",
      "        self.vocab_size = config.vocab_size\n",
      "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
      "        self.router_aux_loss_coef = config.router_aux_loss_coef\n",
      "        self.num_experts = config.num_local_experts\n",
      "        self.num_experts_per_tok = config.num_experts_per_tok\n",
      "        self.post_init()\n",
      "\n",
      "    @can_return_tuple\n",
      "    @auto_docstring\n",
      "    def forward(self, input_ids: Optional[torch.LongTensor]=None, attention_mask: Optional[torch.Tensor]=None, position_ids: Optional[torch.LongTensor]=None, past_key_values: Optional[Cache]=None, inputs_embeds: Optional[torch.FloatTensor]=None, labels: Optional[torch.LongTensor]=None, use_cache: Optional[bool]=None, output_router_logits: Optional[bool]=None, cache_position: Optional[torch.LongTensor]=None, logits_to_keep: Union[int, torch.Tensor]=0, **kwargs: Unpack[TransformersKwargs]) -> MoeCausalLMOutputWithPast:...\n",
      "        output_router_logits = output_router_logits if output_router_logits is not None else self.config.output_router_logits\n",
      "        outputs: MoeModelOutputWithPast = self.model(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, past_key_values=past_key_values, inputs_embeds=inputs_embeds, use_cache=use_cache, output_router_logits=output_router_logits, cache_position=cache_position, **kwargs)\n",
      "        hidden_states = outputs.last_hidden_state\n",
      "        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n",
      "        logits = self.lm_head(hidden_states[:, slice_indices, :])\n",
      "        loss = None\n",
      "        if labels is not None:...\n",
      "        aux_loss = None\n",
      "        if output_router_logits:...\n",
      "        return MoeCausalLMOutputWithPast(loss=loss, aux_loss=aux_loss, logits=logits, past_key_values=outputs.past_key_values, hidden_states=outputs.hidden_states, attentions=outputs.attentions, router_logits=outputs.router_logits)\n",
      "\n",
      "class GptOssForSequenceClassification(GenericForSequenceClassification, GptOssPreTrainedModel):\n",
      "    pass\n",
      "\n",
      "class GptOssForTokenClassification(GenericForTokenClassification, GptOssPreTrainedModel):\n",
      "    pass\n",
      "__all__ = [...]\n"
     ]
    }
   ],
   "source": [
    "print(sketches['/home/jw2858/miniconda3/envs/NKI/lib/python3.12/site-packages/transformers/models/gpt_oss/modeling_gpt_oss.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6fd1009-31e3-44bb-aef4-57d27c311006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import math\n",
      "from functools import wraps\n",
      "from typing import Optional\n",
      "from .configuration_utils import PretrainedConfig\n",
      "from .utils import is_torch_available, logging\n",
      "logger = logging.get_logger(__name__)\n",
      "if is_torch_available():\n",
      "    import torch\n",
      "\n",
      "def dynamic_rope_update(rope_forward):...\n",
      "\n",
      "    def longrope_frequency_update(self, position_ids, device):...\n",
      "\n",
      "    def dynamic_frequency_update(self, position_ids, device):...\n",
      "\n",
      "    @wraps(rope_forward)\n",
      "    def wrapper(self, x, position_ids):\n",
      "        if 'dynamic' in self.rope_type:...\n",
      "        elif self.rope_type == 'longrope':...\n",
      "        return rope_forward(self, x, position_ids)\n",
      "    return wrapper\n",
      "\n",
      "def _compute_default_rope_parameters(config: Optional[PretrainedConfig]=None, device: Optional['torch.device']=None, seq_len: Optional[int]=None) -> tuple['torch.Tensor', float]:...\n",
      "\n",
      "def _compute_linear_scaling_rope_parameters(config: Optional[PretrainedConfig]=None, device: Optional['torch.device']=None, seq_len: Optional[int]=None) -> tuple['torch.Tensor', float]:...\n",
      "\n",
      "def _compute_dynamic_ntk_parameters(config: Optional[PretrainedConfig]=None, device: Optional['torch.device']=None, seq_len: Optional[int]=None) -> tuple['torch.Tensor', float]:...\n",
      "\n",
      "def _compute_yarn_parameters(config: PretrainedConfig, device: 'torch.device', seq_len: Optional[int]=None) -> tuple['torch.Tensor', float]:...\n",
      "    base = config.rope_theta\n",
      "    partial_rotary_factor = getattr(config, 'partial_rotary_factor', 1.0)\n",
      "    head_dim = getattr(config, 'head_dim', config.hidden_size // config.num_attention_heads)\n",
      "    dim = int(head_dim * partial_rotary_factor)\n",
      "    factor = config.rope_scaling['factor']\n",
      "    attention_factor = config.rope_scaling.get('attention_factor')\n",
      "    mscale = config.rope_scaling.get('mscale')\n",
      "    mscale_all_dim = config.rope_scaling.get('mscale_all_dim')\n",
      "    original_max_position_embeddings = config.rope_scaling.get('original_max_position_embeddings') or config.max_position_embeddings\n",
      "\n",
      "    def get_mscale(scale, mscale=1):\n",
      "        if scale <= 1:...\n",
      "        return 0.1 * mscale * math.log(scale) + 1.0\n",
      "    if attention_factor is None:\n",
      "        if mscale and mscale_all_dim:...\n",
      "        else:\n",
      "            attention_factor = get_mscale(factor)\n",
      "    beta_fast = config.rope_scaling.get('beta_fast') or 32\n",
      "    beta_slow = config.rope_scaling.get('beta_slow') or 1\n",
      "\n",
      "    def find_correction_dim(num_rotations, dim, base, max_position_embeddings):...\n",
      "        return dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi)) / (2 * math.log(base))\n",
      "\n",
      "    def find_correction_range(low_rot, high_rot, dim, base, max_position_embeddings, truncate):...\n",
      "        low = find_correction_dim(low_rot, dim, base, max_position_embeddings)\n",
      "        high = find_correction_dim(high_rot, dim, base, max_position_embeddings)\n",
      "        if truncate:...\n",
      "        return (max(low, 0), min(high, dim - 1))\n",
      "\n",
      "    def linear_ramp_factor(min, max, dim):\n",
      "        if min == max:...\n",
      "        linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n",
      "        ramp_func = torch.clamp(linear_func, 0, 1)\n",
      "        return ramp_func\n",
      "    pos_freqs = base ** (torch.arange(0, dim, 2).to(device=device, dtype=torch.float) / dim)\n",
      "    inv_freq_extrapolation = 1.0 / pos_freqs\n",
      "    inv_freq_interpolation = 1.0 / (factor * pos_freqs)\n",
      "    truncate = config.rope_scaling.get('truncate', True)\n",
      "    low, high = find_correction_range(beta_fast, beta_slow, dim, base, original_max_position_embeddings, truncate)\n",
      "    inv_freq_extrapolation_factor = 1 - linear_ramp_factor(low, high, dim // 2).to(device=device, dtype=torch.float)\n",
      "    inv_freq = inv_freq_interpolation * (1 - inv_freq_extrapolation_factor) + inv_freq_extrapolation * inv_freq_extrapolation_factor\n",
      "    return (inv_freq, attention_factor)\n",
      "\n",
      "def _compute_longrope_parameters(config: PretrainedConfig, device: 'torch.device', seq_len: Optional[int]=None) -> tuple['torch.Tensor', float]:...\n",
      "\n",
      "def _compute_llama3_parameters(config: PretrainedConfig, device: 'torch.device', seq_len: Optional[int]=None) -> tuple['torch.Tensor', float]:...\n",
      "ROPE_INIT_FUNCTIONS = {'default': _compute_default_rope_parameters, 'linear': _compute_linear_scaling_rope_parameters, 'dynamic': _compute_dynamic_ntk_parameters, 'yarn': _compute_yarn_parameters, 'longrope': _compute_longrope_parameters, 'llama3': _compute_llama3_parameters}\n",
      "\n",
      "def _check_received_keys(rope_type: str, received_keys: set, required_keys: set, optional_keys: Optional[set]=None, ignore_keys: Optional[set]=None):...\n",
      "    if 'type' in received_keys:...\n",
      "    if ignore_keys is not None:...\n",
      "    missing_keys = required_keys - received_keys\n",
      "    if missing_keys:...\n",
      "    if optional_keys is not None:\n",
      "        unused_keys = received_keys - required_keys - optional_keys\n",
      "    else:...\n",
      "    if unused_keys:...\n",
      "\n",
      "def _validate_default_rope_parameters(config: PretrainedConfig, ignore_keys: Optional[set]=None):...\n",
      "\n",
      "def _validate_linear_scaling_rope_parameters(config: PretrainedConfig, ignore_keys: Optional[set]=None):...\n",
      "\n",
      "def _validate_dynamic_scaling_rope_parameters(config: PretrainedConfig, ignore_keys: Optional[set]=None):...\n",
      "\n",
      "def _validate_yarn_parameters(config: PretrainedConfig, ignore_keys: Optional[set]=None):\n",
      "    rope_scaling = config.rope_scaling\n",
      "    rope_type = rope_scaling.get('rope_type', rope_scaling.get('type', None))\n",
      "    required_keys = {'rope_type', 'factor'}\n",
      "    optional_keys = {...}\n",
      "    received_keys = set(rope_scaling.keys())\n",
      "    _check_received_keys(rope_type, received_keys, required_keys, optional_keys, ignore_keys=ignore_keys)\n",
      "    factor = rope_scaling['factor']\n",
      "    if factor is None or not isinstance(factor, float) or factor < 1.0:...\n",
      "    attention_factor = rope_scaling.get('attention_factor')\n",
      "    if attention_factor is not None and (not isinstance(attention_factor, float) or attention_factor < 0):...\n",
      "    beta_fast = rope_scaling.get('beta_fast')\n",
      "    if beta_fast is not None and (not isinstance(beta_fast, float)):...\n",
      "    beta_slow = rope_scaling.get('beta_slow')\n",
      "    if beta_slow is not None and (not isinstance(beta_slow, float)):...\n",
      "    if (beta_fast or 32) < (beta_slow or 1):...\n",
      "    original_max_position_embeddings = config.rope_scaling.get('original_max_position_embeddings')\n",
      "    if original_max_position_embeddings is not None:\n",
      "        implicit_factor = config.max_position_embeddings / original_max_position_embeddings\n",
      "        if implicit_factor != factor:...\n",
      "    else:...\n",
      "\n",
      "def _validate_longrope_parameters(config: PretrainedConfig, ignore_keys: Optional[set]=None):...\n",
      "\n",
      "def _validate_llama3_parameters(config: PretrainedConfig, ignore_keys: Optional[set]=None):...\n",
      "ROPE_VALIDATION_FUNCTIONS = {'default': _validate_default_rope_parameters, 'linear': _validate_linear_scaling_rope_parameters, 'dynamic': _validate_dynamic_scaling_rope_parameters, 'yarn': _validate_yarn_parameters, 'longrope': _validate_longrope_parameters, 'llama3': _validate_llama3_parameters}\n",
      "\n",
      "def rope_config_validation(config: PretrainedConfig, ignore_keys: Optional[set]=None):...\n",
      "    rope_scaling = getattr(config, 'rope_scaling', None)\n",
      "    if rope_scaling is None:...\n",
      "    rope_type = rope_scaling.get('rope_type', rope_scaling.get('type', 'default'))\n",
      "    validation_fn = ROPE_VALIDATION_FUNCTIONS.get(rope_type)\n",
      "    if validation_fn is not None:\n",
      "        validation_fn(config, ignore_keys=ignore_keys)\n",
      "    else:...\n"
     ]
    }
   ],
   "source": [
    "print(sketches['/home/jw2858/miniconda3/envs/NKI/lib/python3.12/site-packages/transformers/modeling_rope_utils.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84e64cbe-b269-45b8-8f82-80a1a9dbb597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from abc import ABC, abstractmethod\n",
      "from collections.abc import Iterable\n",
      "from typing import Any, Optional\n",
      "import torch\n",
      "from .configuration_utils import PretrainedConfig\n",
      "from .utils import is_hqq_available, is_quanto_greater, is_torch_greater_or_equal, is_torchdynamo_compiling, logging\n",
      "if is_hqq_available():...\n",
      "_is_torch_greater_or_equal_than_2_7 = is_torch_greater_or_equal('2.7', accept_dev=True)\n",
      "logger = logging.get_logger(__name__)\n",
      "\n",
      "class CacheLayerMixin(ABC):\n",
      "    is_compileable = False\n",
      "\n",
      "    def __init__(self):\n",
      "        self.keys: Optional[torch.Tensor] = None\n",
      "        self.values: Optional[torch.Tensor] = None\n",
      "        self.is_initialized = False\n",
      "\n",
      "    def __repr__(self):...\n",
      "\n",
      "    @abstractmethod\n",
      "    def lazy_initialization(self, key_states: torch.Tensor):\n",
      "        ...\n",
      "\n",
      "    @abstractmethod\n",
      "    def update(self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]]=None) -> tuple[torch.Tensor, torch.Tensor]:\n",
      "        ...\n",
      "\n",
      "    @abstractmethod\n",
      "    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n",
      "        ...\n",
      "\n",
      "    @abstractmethod\n",
      "    def get_seq_length(self) -> int:\n",
      "        ...\n",
      "\n",
      "    @abstractmethod\n",
      "    def get_max_cache_shape(self) -> int:\n",
      "        ...\n",
      "\n",
      "    def offload(self):...\n",
      "\n",
      "    def prefetch(self):...\n",
      "\n",
      "    def reset(self) -> None:...\n",
      "\n",
      "    def reorder_cache(self, beam_idx: torch.LongTensor) -> None:...\n",
      "\n",
      "class DynamicLayer(CacheLayerMixin):\n",
      "    is_sliding = False\n",
      "\n",
      "    def lazy_initialization(self, key_states: torch.Tensor):\n",
      "        self.dtype, self.device = (key_states.dtype, key_states.device)\n",
      "        self.keys = torch.tensor([], dtype=self.dtype, device=self.device)\n",
      "        self.values = torch.tensor([], dtype=self.dtype, device=self.device)\n",
      "        self.is_initialized = True\n",
      "\n",
      "    def update(self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]]=None) -> tuple[torch.Tensor, torch.Tensor]:...\n",
      "        if not self.is_initialized:\n",
      "            self.lazy_initialization(key_states)\n",
      "        self.keys = torch.cat([self.keys, key_states], dim=-2)\n",
      "        self.values = torch.cat([self.values, value_states], dim=-2)\n",
      "        return (self.keys, self.values)\n",
      "\n",
      "    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:...\n",
      "        kv_offset = 0\n",
      "        query_length = cache_position.shape[0]\n",
      "        kv_length = self.get_seq_length() + query_length\n",
      "        return (kv_length, kv_offset)\n",
      "\n",
      "    def get_seq_length(self) -> int:...\n",
      "        if not self.is_initialized or self.keys.numel() == 0:\n",
      "            return 0\n",
      "        return self.keys.shape[-2]\n",
      "\n",
      "    def get_max_cache_shape(self) -> int:...\n",
      "\n",
      "    def crop(self, max_length: int) -> None:...\n",
      "\n",
      "    def batch_repeat_interleave(self, repeats: int) -> None:...\n",
      "\n",
      "    def batch_select_indices(self, indices: torch.Tensor) -> None:...\n",
      "\n",
      "class DynamicSlidingWindowLayer(DynamicLayer):\n",
      "    is_sliding = True\n",
      "\n",
      "    def __init__(self, sliding_window: int):\n",
      "        super().__init__()\n",
      "        self.sliding_window = sliding_window\n",
      "        self.cumulative_length = 0\n",
      "\n",
      "    def update(self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]]=None) -> tuple[torch.Tensor, torch.Tensor]:...\n",
      "        if not self.is_initialized:\n",
      "            self.lazy_initialization(key_states)\n",
      "        self.cumulative_length += key_states.shape[-2]\n",
      "        full_key_states = torch.cat([self.keys, key_states], dim=-2)\n",
      "        full_value_states = torch.cat([self.values, value_states], dim=-2)\n",
      "        self.keys = full_key_states[:, :, -self.sliding_window + 1:, :]\n",
      "        self.values = full_value_states[:, :, -self.sliding_window + 1:, :]\n",
      "        return (full_key_states, full_value_states)\n",
      "\n",
      "    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:...\n",
      "        query_length = cache_position.shape[0]\n",
      "        is_full = self.cumulative_length >= self.sliding_window\n",
      "        kv_offset = max(self.cumulative_length - self.sliding_window + 1, 0)\n",
      "        if is_full:...\n",
      "        else:\n",
      "            kv_length = self.cumulative_length + query_length\n",
      "        return (kv_length, kv_offset)\n",
      "\n",
      "    def get_seq_length(self) -> int:...\n",
      "        return self.cumulative_length\n",
      "\n",
      "    def get_max_cache_shape(self) -> int:...\n",
      "\n",
      "    def crop(self, max_length: int) -> None:...\n",
      "\n",
      "class StaticLayer(CacheLayerMixin):\n",
      "    is_compileable = True\n",
      "    is_sliding = False\n",
      "\n",
      "    def __init__(self, max_cache_len: int):...\n",
      "\n",
      "    def lazy_initialization(self, key_states: torch.Tensor):...\n",
      "\n",
      "    def update(self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]]=None) -> tuple[torch.Tensor, torch.Tensor]:...\n",
      "\n",
      "    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:...\n",
      "\n",
      "    def get_seq_length(self) -> int:...\n",
      "\n",
      "    def get_max_cache_shape(self) -> int:...\n",
      "\n",
      "class StaticSlidingWindowLayer(StaticLayer):\n",
      "    is_sliding = True\n",
      "\n",
      "    def __init__(self, max_cache_len: int, sliding_window: int):...\n",
      "\n",
      "    def update(self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]]=None) -> tuple[torch.Tensor, torch.Tensor]:...\n",
      "\n",
      "    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:...\n",
      "\n",
      "    def get_seq_length(self) -> int:...\n",
      "\n",
      "class QuantizedLayer(DynamicLayer):\n",
      "\n",
      "    def __init__(self, nbits: int=4, axis_key: int=0, axis_value: int=0, q_group_size: int=64, residual_length: int=128):...\n",
      "\n",
      "    def update(self, key_states: torch.Tensor, value_states: torch.Tensor, cache_kwargs: Optional[dict[str, Any]]=None) -> tuple[torch.Tensor, torch.Tensor]:...\n",
      "\n",
      "    @abstractmethod\n",
      "    def _quantize(self, tensor, axis):\n",
      "        ...\n",
      "\n",
      "    @abstractmethod\n",
      "    def _dequantize(self, q_tensor):\n",
      "        ...\n",
      "\n",
      "    def get_seq_length(self) -> int:...\n",
      "\n",
      "class QuantoQuantizedLayer(QuantizedLayer):\n",
      "\n",
      "    def __init__(self, nbits: int=4, axis_key: int=0, axis_value: int=0, q_group_size: int=64, residual_length: int=128):...\n",
      "\n",
      "    def _quantize(self, tensor, axis):...\n",
      "\n",
      "    def _dequantize(self, qtensor):...\n",
      "\n",
      "class HQQQuantizedLayer(QuantizedLayer):\n",
      "\n",
      "    def __init__(self, nbits: int=4, axis_key: int=0, axis_value: int=0, q_group_size: int=64, residual_length: int=128):...\n",
      "\n",
      "    def _quantize(self, tensor, axis):...\n",
      "\n",
      "    def _dequantize(self, qtensor):...\n",
      "\n",
      "class Cache:\n",
      "\n",
      "    def __init__(self, layers: Optional[list[CacheLayerMixin]]=None, layer_class_to_replicate: Optional[type[CacheLayerMixin]]=None, offloading: bool=False, offload_only_non_sliding: bool=True):\n",
      "        if layers is not None and layer_class_to_replicate is not None:...\n",
      "        if layers is None and layer_class_to_replicate is None:...\n",
      "        self.layers = layers if layers is not None else []\n",
      "        self.layer_class_to_replicate = layer_class_to_replicate\n",
      "        self.offloading = offloading\n",
      "        if self.offloading:...\n",
      "\n",
      "    def __repr__(self):...\n",
      "\n",
      "    def prefetch(self, layer_idx: int, only_non_sliding: bool=True):...\n",
      "\n",
      "    def offload(self, layer_idx: int, only_non_sliding: bool=True):...\n",
      "\n",
      "    def update(self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[dict[str, Any]]=None) -> tuple[torch.Tensor, torch.Tensor]:...\n",
      "        if self.layer_class_to_replicate is not None:...\n",
      "        if self.offloading:...\n",
      "        keys, values = self.layers[layer_idx].update(key_states, value_states, cache_kwargs)\n",
      "        if self.offloading:...\n",
      "        return (keys, values)\n",
      "\n",
      "    def early_initialization(self, batch_size: int, num_heads: int, head_dim: int, dtype: torch.dtype, device: torch.device):...\n",
      "\n",
      "    def get_seq_length(self, layer_idx: int=0) -> int:...\n",
      "        if layer_idx >= len(self.layers):...\n",
      "        return self.layers[layer_idx].get_seq_length()\n",
      "\n",
      "    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:...\n",
      "        if layer_idx >= len(self.layers):...\n",
      "        return self.layers[layer_idx].get_mask_sizes(cache_position)\n",
      "\n",
      "    def get_max_cache_shape(self, layer_idx: int=0) -> int:...\n",
      "\n",
      "    def reset(self):...\n",
      "\n",
      "    def reorder_cache(self, beam_idx: torch.LongTensor):...\n",
      "\n",
      "    def crop(self, max_length: int):...\n",
      "\n",
      "    def batch_repeat_interleave(self, repeats: int):...\n",
      "\n",
      "    def batch_select_indices(self, indices: torch.Tensor):...\n",
      "\n",
      "    @property\n",
      "    def max_batch_size(self) -> int:...\n",
      "\n",
      "    @property\n",
      "    def max_cache_len(self) -> int:...\n",
      "\n",
      "    @property\n",
      "    def is_compileable(self) -> bool:...\n",
      "        if len(self.layers) == 0:...\n",
      "        return all((layer.is_compileable for layer in self.layers))\n",
      "\n",
      "    @property\n",
      "    def is_initialized(self) -> bool:...\n",
      "\n",
      "    @property\n",
      "    def is_sliding(self) -> list[bool]:...\n",
      "        return [getattr(layer, 'is_sliding', False) for layer in self.layers]\n",
      "\n",
      "    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor]:...\n",
      "\n",
      "    def __iter__(self):...\n",
      "\n",
      "    def __len__(self):...\n",
      "\n",
      "class DynamicCache(Cache):\n",
      "\n",
      "    def __init__(self, ddp_cache_data: Optional[Iterable[tuple[torch.Tensor, torch.Tensor]]]=None, config: Optional[PretrainedConfig]=None, offloading: bool=False, offload_only_non_sliding: bool=False):\n",
      "        layers = []\n",
      "        if config is not None:\n",
      "            decoder_config = config.get_text_config(decoder=True)\n",
      "            sliding_window = getattr(decoder_config, 'sliding_window', None) or getattr(...)\n",
      "            layer_types = getattr(decoder_config, 'layer_types', None)\n",
      "            if layer_types is None:...\n",
      "            if hasattr(decoder_config, 'num_kv_shared_layers'):...\n",
      "            for layer_type in layer_types:\n",
      "                if layer_type in ('sliding_attention', 'chunked_attention'):\n",
      "                    layers.append(DynamicSlidingWindowLayer(sliding_window=sliding_window))\n",
      "                else:\n",
      "                    layers.append(DynamicLayer())\n",
      "        if ddp_cache_data is not None:...\n",
      "        if len(layers) == 0:...\n",
      "        else:\n",
      "            super().__init__(layers=layers, offloading=offloading, offload_only_non_sliding=offload_only_non_sliding)\n",
      "\n",
      "    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor, torch.Tensor]]:...\n",
      "\n",
      "    @classmethod\n",
      "    def from_legacy_cache(cls, past_key_values: tuple[tuple[torch.Tensor, torch.Tensor]]) -> 'DynamicCache':...\n",
      "\n",
      "class StaticCache(Cache):\n",
      "\n",
      "    def __init__(self, config: PretrainedConfig, max_cache_len: int, offloading: bool=False, offload_only_non_sliding: bool=True, **kwargs):...\n",
      "\n",
      "class QuantizedCache(Cache):\n",
      "\n",
      "    def __init__(self, backend: str, config: PretrainedConfig, nbits: int=4, axis_key: int=0, axis_value: int=0, q_group_size: int=64, residual_length: int=128):...\n",
      "\n",
      "class EncoderDecoderCache(Cache):\n",
      "\n",
      "    def __init__(self, *caches) -> None:...\n",
      "\n",
      "    def __repr__(self) -> str:...\n",
      "\n",
      "    def __iter__(self):...\n",
      "\n",
      "    def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:...\n",
      "\n",
      "    def __len__(self):...\n",
      "\n",
      "    def to_legacy_cache(self) -> tuple[tuple[torch.Tensor]]:...\n",
      "\n",
      "    @classmethod\n",
      "    def from_legacy_cache(cls, past_key_values: Optional[Iterable[tuple[torch.FloatTensor, ...]]]) -> 'EncoderDecoderCache':...\n",
      "\n",
      "    def get_seq_length(self, layer_idx: int=0) -> int:...\n",
      "\n",
      "    def reset(self):...\n",
      "\n",
      "    def reorder_cache(self, beam_idx: torch.LongTensor):...\n",
      "\n",
      "    def check_dynamic_cache(self, method: str):...\n",
      "\n",
      "    def crop(self, maximum_length: int):...\n",
      "\n",
      "    def batch_split(self, full_batch_size: int, split_size: int) -> 'list[EncoderDecoderCache]':...\n",
      "\n",
      "    def batch_repeat_interleave(self, repeats: int):...\n",
      "\n",
      "    def batch_select_indices(self, indices: torch.Tensor):...\n",
      "\n",
      "    def get_max_cache_shape(self) -> int:...\n",
      "\n",
      "    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:...\n",
      "\n",
      "    @property\n",
      "    def is_sliding(self):...\n",
      "\n",
      "    @property\n",
      "    def is_compileable(self) -> bool:...\n",
      "\n",
      "class SlidingWindowLayer(StaticSlidingWindowLayer):\n",
      "\n",
      "    def __init__(self, max_cache_len: int, sliding_window: int):...\n",
      "\n",
      "class ChunkedSlidingLayer(StaticSlidingWindowLayer):\n",
      "\n",
      "    def __init__(self, max_cache_len: int, sliding_window: int):...\n",
      "\n",
      "class OffloadedCache(DynamicCache):\n",
      "\n",
      "    def __init__(self) -> None:...\n",
      "\n",
      "class OffloadedStaticCache(StaticCache):\n",
      "\n",
      "    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):...\n",
      "\n",
      "class SlidingWindowCache(StaticCache):\n",
      "\n",
      "    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):...\n",
      "\n",
      "class HybridCache(StaticCache):\n",
      "\n",
      "    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):...\n",
      "\n",
      "class HybridChunkedCache(StaticCache):\n",
      "\n",
      "    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):...\n",
      "\n",
      "class OffloadedHybridCache(StaticCache):\n",
      "\n",
      "    def __init__(self, config: PretrainedConfig, max_cache_len: int, *args, **kwargs):...\n",
      "\n",
      "class QuantoQuantizedCache(QuantizedCache):\n",
      "\n",
      "    def __init__(self, config: PretrainedConfig, nbits: int=4, axis_key: int=0, axis_value: int=0, q_group_size: int=64, residual_length: int=128):...\n",
      "\n",
      "class HQQQuantizedCache(QuantizedCache):\n",
      "\n",
      "    def __init__(self, config: PretrainedConfig, nbits: int=4, axis_key: int=0, axis_value: int=0, q_group_size: int=64, residual_length: int=128):...\n",
      "\n",
      "class SinkCache(Cache):\n",
      "\n",
      "    def __init__(self, **kwargs) -> None:...\n"
     ]
    }
   ],
   "source": [
    "print(sketches['/home/jw2858/miniconda3/envs/NKI/lib/python3.12/site-packages/transformers/cache_utils.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a078a13-e432-427e-854f-105c326e4741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import collections\n",
      "import copy\n",
      "import functools\n",
      "import gc\n",
      "import importlib.metadata\n",
      "import inspect\n",
      "import json\n",
      "import os\n",
      "import re\n",
      "import sys\n",
      "import warnings\n",
      "from abc import abstractmethod\n",
      "from collections import defaultdict\n",
      "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "from contextlib import contextmanager\n",
      "from enum import Enum\n",
      "from functools import partial, wraps\n",
      "from threading import Thread\n",
      "from typing import Any, Callable, Optional, TypeVar, Union, get_type_hints\n",
      "from zipfile import is_zipfile\n",
      "import torch\n",
      "from huggingface_hub import split_torch_state_dict_into_shards\n",
      "from packaging import version\n",
      "from safetensors import safe_open\n",
      "from safetensors.torch import load_file as safe_load_file\n",
      "from safetensors.torch import save_file as safe_save_file\n",
      "from torch import Tensor, nn\n",
      "from torch.distributions import constraints\n",
      "from torch.utils.checkpoint import checkpoint\n",
      "from .configuration_utils import PretrainedConfig\n",
      "from .distributed import DistributedConfig\n",
      "from .dynamic_module_utils import custom_object_save\n",
      "from .generation import CompileConfig, GenerationConfig\n",
      "from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled, is_fsdp_enabled\n",
      "from .integrations.accelerate import find_tied_parameters, init_empty_weights\n",
      "from .integrations.deepspeed import _load_state_dict_into_zero3_model\n",
      "from .integrations.eager_paged import eager_paged_attention_forward\n",
      "from .integrations.flash_attention import flash_attention_forward\n",
      "from .integrations.flash_paged import paged_attention_forward\n",
      "from .integrations.flex_attention import flex_attention_forward\n",
      "from .integrations.hub_kernels import is_kernel, load_and_register_kernel\n",
      "from .integrations.sdpa_attention import sdpa_attention_forward\n",
      "from .integrations.sdpa_paged import sdpa_attention_paged_forward\n",
      "from .integrations.tensor_parallel import _get_parameter_tp_plan, distribute_model, initialize_tensor_parallelism, repack_weights, replace_state_dict_local_with_dtensor, shard_and_distribute_module, verify_tp_plan\n",
      "from .loss.loss_utils import LOSS_MAPPING\n",
      "from .modeling_flash_attention_utils import lazy_import_flash_attention\n",
      "from .pytorch_utils import id_tensor_storage\n",
      "from .quantizers import HfQuantizer\n",
      "from .quantizers.auto import get_hf_quantizer\n",
      "from .quantizers.quantizers_utils import get_module_from_name\n",
      "from .safetensors_conversion import auto_conversion\n",
      "from .utils import ADAPTER_SAFE_WEIGHTS_NAME, ADAPTER_WEIGHTS_NAME, CONFIG_NAME, DUMMY_INPUTS, FLAX_WEIGHTS_NAME, SAFE_WEIGHTS_INDEX_NAME, SAFE_WEIGHTS_NAME, TF2_WEIGHTS_NAME, TF_WEIGHTS_NAME, WEIGHTS_INDEX_NAME, WEIGHTS_NAME, ContextManagers, PushToHubMixin, cached_file, check_torch_load_is_safe, copy_func, download_url, extract_commit_hash, has_file, is_accelerate_available, is_bitsandbytes_available, is_flash_attn_2_available, is_flash_attn_3_available, is_kernels_available, is_offline_mode, is_optimum_available, is_peft_available, is_remote_url, is_torch_flex_attn_available, is_torch_greater_or_equal, is_torch_mlu_available, is_torch_npu_available, is_torch_xla_available, is_torch_xpu_available, logging\n",
      "from .utils.generic import _CAN_RECORD_REGISTRY, GeneralInterface, OutputRecorder\n",
      "from .utils.hub import create_and_tag_model_card, get_checkpoint_shard_files\n",
      "from .utils.import_utils import ENV_VARS_TRUE_VALUES, is_huggingface_hub_greater_or_equal, is_sagemaker_mp_enabled, is_torch_fx_proxy, is_torchdynamo_compiling\n",
      "from .utils.quantization_config import BitsAndBytesConfig, QuantizationMethod\n",
      "if is_accelerate_available():...\n",
      "if is_peft_available():...\n",
      "_torch_distributed_available = torch.distributed.is_available()\n",
      "_is_dtensor_available = _torch_distributed_available and is_torch_greater_or_equal('2.5')\n",
      "if _is_dtensor_available:\n",
      "    from torch.distributed.tensor import DTensor\n",
      "if is_sagemaker_mp_enabled():...\n",
      "else:\n",
      "    IS_SAGEMAKER_MP_POST_1_10 = False\n",
      "logger = logging.get_logger(__name__)\n",
      "XLA_USE_BF16 = os.environ.get('XLA_USE_BF16', '0').upper()\n",
      "XLA_DOWNCAST_BF16 = os.environ.get('XLA_DOWNCAST_BF16', '0').upper()\n",
      "SpecificPreTrainedModelType = TypeVar('SpecificPreTrainedModelType', bound='PreTrainedModel')\n",
      "_init_weights = True\n",
      "_is_quantized = False\n",
      "_is_ds_init_called = False\n",
      "\n",
      "def is_local_dist_rank_0():...\n",
      "TORCH_INIT_FUNCTIONS = {'uniform_': nn.init.uniform_, 'normal_': nn.init.normal_, 'trunc_normal_': nn.init.trunc_normal_, 'constant_': nn.init.constant_, 'xavier_uniform_': nn.init.xavier_uniform_, 'xavier_normal_': nn.init.xavier_normal_, 'kaiming_uniform_': nn.init.kaiming_uniform_, 'kaiming_normal_': nn.init.kaiming_normal_, 'uniform': nn.init.uniform, 'normal': nn.init.normal, 'xavier_uniform': nn.init.xavier_uniform, 'xavier_normal': nn.init.xavier_normal, 'kaiming_uniform': nn.init.kaiming_uniform, 'kaiming_normal': nn.init.kaiming_normal}\n",
      "VLMS = [...]\n",
      "\n",
      "@contextmanager\n",
      "def no_init_weights():...\n",
      "    old_init_weights = _init_weights\n",
      "    _init_weights = False\n",
      "\n",
      "    def _skip_init(*args, **kwargs):\n",
      "        pass\n",
      "    for name, init_func in TORCH_INIT_FUNCTIONS.items():\n",
      "        setattr(torch.nn.init, name, _skip_init)\n",
      "    try:\n",
      "        yield\n",
      "    finally:\n",
      "        _init_weights = old_init_weights\n",
      "        for name, init_func in TORCH_INIT_FUNCTIONS.items():\n",
      "            setattr(torch.nn.init, name, init_func)\n",
      "\n",
      "@contextmanager\n",
      "def set_quantized_state():...\n",
      "\n",
      "@contextmanager\n",
      "def set_zero3_state():...\n",
      "\n",
      "def restore_default_dtype(func):...\n",
      "\n",
      "    @wraps(func)\n",
      "    def _wrapper(*args, **kwargs):\n",
      "        old_dtype = torch.get_default_dtype()\n",
      "        try:\n",
      "            return func(*args, **kwargs)\n",
      "        finally:\n",
      "            torch.set_default_dtype(old_dtype)\n",
      "    return _wrapper\n",
      "\n",
      "def get_torch_context_manager_or_global_device():...\n",
      "    device_in_context = torch.tensor([]).device\n",
      "    default_device = torch.get_default_device() if is_torch_greater_or_equal('2.3') else torch.device('cpu')\n",
      "    if device_in_context == default_device:\n",
      "        if default_device != torch.device('cpu'):...\n",
      "        return None...\n",
      "\n",
      "def get_parameter_device(parameter: Union[nn.Module, 'ModuleUtilsMixin']):\n",
      "    try:\n",
      "        return next(parameter.parameters()).device\n",
      "    except StopIteration:\n",
      "\n",
      "        def find_tensor_attributes(module: nn.Module) -> list[tuple[str, Tensor]]:\n",
      "            tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n",
      "            return tuples\n",
      "        gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n",
      "        first_tuple = next(gen)\n",
      "        return first_tuple[1].device\n",
      "\n",
      "def get_parameter_dtype(parameter: Union[nn.Module, 'ModuleUtilsMixin']):...\n",
      "\n",
      "def get_state_dict_dtype(state_dict):...\n",
      "\n",
      "def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):...\n",
      "str_to_torch_dtype = {'BOOL': torch.bool, 'U8': torch.uint8, 'I8': torch.int8, 'I16': torch.int16, 'F16': torch.float16, 'BF16': torch.bfloat16, 'I32': torch.int32, 'F32': torch.float32, 'F64': torch.float64, 'I64': torch.int64, 'F8_E4M3': torch.float8_e4m3fn, 'F8_E5M2': torch.float8_e5m2}\n",
      "if is_torch_greater_or_equal('2.3.0'):\n",
      "    str_to_torch_dtype['U16'] = torch.uint16\n",
      "    str_to_torch_dtype['U32'] = torch.uint32\n",
      "    str_to_torch_dtype['U64'] = torch.uint64\n",
      "\n",
      "def load_state_dict(checkpoint_file: Union[str, os.PathLike], is_quantized: bool=False, map_location: Optional[Union[str, torch.device]]='cpu', weights_only: bool=True):...\n",
      "    if checkpoint_file.endswith('.safetensors'):\n",
      "        with safe_open(checkpoint_file, framework='pt') as f:\n",
      "            metadata = f.metadata()\n",
      "            if metadata is not None and metadata.get('format') not in ['pt', 'tf', 'flax', 'mlx']:...\n",
      "            state_dict = {}\n",
      "            for k in f.keys():\n",
      "                if map_location == 'meta':\n",
      "                    _slice = f.get_slice(k)\n",
      "                    k_dtype = _slice.get_dtype()\n",
      "                    if k_dtype in str_to_torch_dtype:\n",
      "                        dtype = str_to_torch_dtype[k_dtype]\n",
      "                    else:...\n",
      "                    state_dict[k] = torch.empty(size=_slice.get_shape(), dtype=dtype, device='meta')\n",
      "                else:...\n",
      "            return state_dict...\n",
      "\n",
      "def _end_ptr(tensor: torch.Tensor) -> int:...\n",
      "\n",
      "def _get_tied_weight_keys(module: nn.Module, prefix=''):...\n",
      "\n",
      "def _find_disjoint(tensors: list[set[str]], state_dict: dict[str, torch.Tensor]) -> tuple[list[set[str]], list[str]]:...\n",
      "\n",
      "def _find_identical(tensors: list[set[str]], state_dict: dict[str, torch.Tensor]) -> tuple[list[set[str]], set[str]]:...\n",
      "\n",
      "def _infer_parameter_dtype(model: 'PreTrainedModel', param_name: str, empty_param: torch.Tensor, keep_in_fp32_regex: Optional[re.Pattern]=None, hf_quantizer: Optional[HfQuantizer]=None) -> Union[bool, Optional[torch.dtype]]:\n",
      "    try:\n",
      "        old_param = model.get_parameter_or_buffer(param_name)\n",
      "    except Exception as e:\n",
      "        if hf_quantizer is not None and hf_quantizer.quantization_config.quant_method in {QuantizationMethod.HQQ, QuantizationMethod.QUARK, QuantizationMethod.MXFP4, QuantizationMethod.BITS_AND_BYTES}:\n",
      "            return (True, None)\n",
      "        else:\n",
      "            raise e\n",
      "    is_torch_e4m3fn_available = hasattr(torch, 'float8_e4m3fn')\n",
      "    casting_dtype = None\n",
      "    is_param_float8_e4m3fn = is_torch_e4m3fn_available and empty_param.dtype == torch.float8_e4m3fn\n",
      "    if empty_param.dtype.is_floating_point and (not is_param_float8_e4m3fn):\n",
      "        if keep_in_fp32_regex is not None and keep_in_fp32_regex.search(param_name):...\n",
      "        elif hf_quantizer is not None:...\n",
      "        else:\n",
      "            casting_dtype = old_param.dtype\n",
      "    return (old_param is not None and old_param.is_contiguous(), casting_dtype)\n",
      "\n",
      "def _load_parameter_into_model(model: 'PreTrainedModel', param_name: str, tensor: torch.Tensor):...\n",
      "    module, param_type = get_module_from_name(model, param_name)\n",
      "    module.load_state_dict({param_type: tensor}, strict=False, assign=True)\n",
      "\n",
      "@torch.no_grad()\n",
      "def _load_state_dict_into_meta_model(model: 'PreTrainedModel', state_dict: dict, shard_file: str, reverse_renaming_mapping: dict[str, str], device_map: Optional[dict]=None, disk_offload_folder: Optional[str]=None, disk_offload_index: Optional[dict]=None, hf_quantizer: Optional[HfQuantizer]=None, keep_in_fp32_regex: Optional[re.Pattern]=None, device_mesh: Optional['torch.distributed.device_mesh.DeviceMesh']=None) -> tuple[Optional[dict], Optional[dict]]:...\n",
      "    tensor_device = 'cpu'\n",
      "    if device_map is not None and device_map.get('', None) is not None:...\n",
      "    if device_map is not None:...\n",
      "    is_quantized = hf_quantizer is not None\n",
      "    is_safetensors = shard_file.endswith('.safetensors')\n",
      "    is_meta_state_dict = is_safetensors\n",
      "    file_pointer = safe_open(shard_file, framework='pt', device=tensor_device) if is_meta_state_dict else None\n",
      "    params_to_load = list(state_dict.keys())\n",
      "    for param_name in params_to_load:\n",
      "        empty_param = state_dict[param_name]\n",
      "        if is_meta_state_dict:\n",
      "            serialized_param_name = reverse_renaming_mapping[param_name]\n",
      "            param = file_pointer.get_slice(serialized_param_name)\n",
      "        else:...\n",
      "        to_contiguous, casting_dtype = _infer_parameter_dtype(model, param_name, empty_param, keep_in_fp32_regex, hf_quantizer)\n",
      "        if device_mesh is not None:...\n",
      "        else:\n",
      "            param = param[...]\n",
      "            if casting_dtype is not None:\n",
      "                param = param.to(casting_dtype)\n",
      "            if to_contiguous:\n",
      "                param = param.contiguous()\n",
      "            if device_map is None:\n",
      "                param_device = 'cpu'\n",
      "            else:...\n",
      "            if param_device == 'disk':...\n",
      "            elif not is_quantized or not hf_quantizer.param_needs_quantization(model, param_name):\n",
      "                if is_fsdp_enabled():...\n",
      "                _load_parameter_into_model(model, param_name, param.to(param_device))\n",
      "            else:...\n",
      "        if not is_meta_state_dict:...\n",
      "    if file_pointer is not None:\n",
      "        file_pointer.__exit__(None, None, None)\n",
      "    return disk_offload_index\n",
      "\n",
      "def load_shard_file(args):\n",
      "    shard_file, state_dict, disk_only_shard_files, is_quantized, device_map, hf_quantizer, key_renaming_mapping, weights_only, model, reverse_key_renaming_mapping, disk_offload_folder, disk_offload_index, keep_in_fp32_regex, device_mesh = args\n",
      "    if shard_file in disk_only_shard_files:...\n",
      "    map_location = 'cpu'\n",
      "    if shard_file.endswith('.safetensors') and (not (is_deepspeed_zero3_enabled() and (not is_quantized))):\n",
      "        map_location = 'meta'\n",
      "    if shard_file != '':\n",
      "        state_dict = load_state_dict(shard_file, is_quantized=is_quantized, map_location=map_location, weights_only=weights_only)\n",
      "    state_dict = {key_renaming_mapping[k]: v for k, v in state_dict.items() if k in key_renaming_mapping}\n",
      "    error_msgs = []\n",
      "    if is_deepspeed_zero3_enabled() and (not is_quantized):...\n",
      "    elif not (is_fsdp_enabled() and (not is_local_dist_rank_0()) and (not is_quantized)):\n",
      "        disk_offload_index = _load_state_dict_into_meta_model(model, state_dict, shard_file, reverse_key_renaming_mapping, device_map=device_map, disk_offload_folder=disk_offload_folder, disk_offload_index=disk_offload_index, hf_quantizer=hf_quantizer, keep_in_fp32_regex=keep_in_fp32_regex, device_mesh=device_mesh)\n",
      "    return (error_msgs, disk_offload_index)\n",
      "\n",
      "def load_shard_files_with_threadpool(args_list):...\n",
      "\n",
      "def _add_variant(weights_name: str, variant: Optional[str]=None) -> str:\n",
      "    if variant is not None:...\n",
      "    return weights_name\n",
      "\n",
      "def _get_resolved_checkpoint_files(pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], subfolder: str, variant: Optional[str], gguf_file: Optional[str], from_tf: bool, from_flax: bool, use_safetensors: bool, cache_dir: str, force_download: bool, proxies: Optional[dict[str, str]], local_files_only: bool, token: Optional[Union[str, bool]], user_agent: dict, revision: str, commit_hash: Optional[str], is_remote_code: bool, transformers_explicit_filename: Optional[str]=None) -> tuple[Optional[list[str]], Optional[dict]]:...\n",
      "    is_sharded = False\n",
      "    if pretrained_model_name_or_path is not None and gguf_file is None:\n",
      "        pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n",
      "        is_local = os.path.isdir(pretrained_model_name_or_path)\n",
      "        if is_local:\n",
      "            if transformers_explicit_filename is not None:...\n",
      "            elif from_tf and os.path.isfile(...):...\n",
      "            elif from_tf and os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, TF2_WEIGHTS_NAME)):...\n",
      "            elif from_flax and os.path.isfile(...):...\n",
      "            elif use_safetensors is not False and os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_NAME, variant))):...\n",
      "            elif use_safetensors is not False and os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_INDEX_NAME, variant))):\n",
      "                archive_file = os.path.join(pretrained_model_name_or_path, subfolder, _add_variant(SAFE_WEIGHTS_INDEX_NAME, variant))\n",
      "                is_sharded = True\n",
      "            else:...\n",
      "        else:...\n",
      "        if is_local:\n",
      "            logger.info(f'loading weights file {archive_file}')\n",
      "            resolved_archive_file = archive_file\n",
      "        else:...\n",
      "    else:...\n",
      "    sharded_metadata = None\n",
      "    if is_sharded:\n",
      "        checkpoint_files, sharded_metadata = get_checkpoint_shard_files(pretrained_model_name_or_path, resolved_archive_file, cache_dir=cache_dir, force_download=force_download, proxies=proxies, local_files_only=local_files_only, token=token, user_agent=user_agent, revision=revision, subfolder=subfolder, _commit_hash=commit_hash)\n",
      "    else:...\n",
      "    return (checkpoint_files, sharded_metadata)\n",
      "\n",
      "def _get_dtype(cls, dtype: Optional[Union[str, torch.dtype, dict]], checkpoint_files: Optional[list[str]], config: PretrainedConfig, sharded_metadata: Optional[dict], state_dict: Optional[dict], weights_only: bool) -> tuple[PretrainedConfig, Optional[torch.dtype], Optional[torch.dtype]]:...\n",
      "    dtype_orig = None\n",
      "    is_sharded = sharded_metadata is not None\n",
      "    if dtype is not None:...\n",
      "    else:\n",
      "        default_dtype = torch.get_default_dtype()\n",
      "        config.dtype = default_dtype\n",
      "        for key in config.sub_configs:...\n",
      "    return (config, dtype, dtype_orig)\n",
      "\n",
      "def _get_device_map(model: 'PreTrainedModel', device_map: Optional[Union[dict, str]], max_memory: Optional[dict], hf_quantizer: Optional[HfQuantizer], dtype: Optional[torch.dtype], keep_in_fp32_regex: Optional[re.Pattern]) -> dict:...\n",
      "\n",
      "def _find_missing_and_unexpected_keys(model: 'PreTrainedModel', original_checkpoint_keys: list[str], checkpoint_keys: list[str], loading_base_model_from_task_state_dict: bool, hf_quantizer: Optional[HfQuantizer]) -> tuple[list[str], list[str]]:...\n",
      "    prefix = model.base_model_prefix\n",
      "    expected_keys = list(model.state_dict().keys())\n",
      "    if hf_quantizer is not None:...\n",
      "    missing_keys = sorted(set(expected_keys) - set(checkpoint_keys))\n",
      "    unexpected_keys = set(checkpoint_keys) - set(expected_keys)\n",
      "    if loading_base_model_from_task_state_dict:...\n",
      "    model_buffers = {n for n, _ in model.named_buffers()}\n",
      "    unexpected_keys = sorted(unexpected_keys - model_buffers)\n",
      "    tied_params = find_tied_parameters(model)\n",
      "    for group in tied_params:...\n",
      "    if hf_quantizer is not None:...\n",
      "    return (missing_keys, unexpected_keys)\n",
      "\n",
      "def _find_mismatched_keys(model: 'PreTrainedModel', state_dict: Optional[dict], checkpoint_files: Optional[list[str]], ignore_mismatched_sizes: bool, keys_to_rename_mapping: dict[str, str], is_quantized: bool, weights_only: bool) -> tuple[list[str], list[tuple[int, int]]]:...\n",
      "    if not ignore_mismatched_sizes:\n",
      "        return ([], [])...\n",
      "\n",
      "class PipelineParallel(Enum):\n",
      "    inputs = 0\n",
      "    outputs = 1\n",
      "\n",
      "class ModuleUtilsMixin:\n",
      "\n",
      "    @staticmethod\n",
      "    def _hook_rss_memory_pre_forward(module, *args, **kwargs):...\n",
      "\n",
      "    @staticmethod\n",
      "    def _hook_rss_memory_post_forward(module, *args, **kwargs):...\n",
      "\n",
      "    def add_memory_hooks(self):...\n",
      "\n",
      "    def reset_memory_hooks_state(self):...\n",
      "\n",
      "    @property\n",
      "    def device(self) -> torch.device:...\n",
      "        return get_parameter_device(self)\n",
      "\n",
      "    @property\n",
      "    def dtype(self) -> torch.dtype:...\n",
      "\n",
      "    def invert_attention_mask(self, encoder_attention_mask: Tensor) -> Tensor:...\n",
      "\n",
      "    @staticmethod\n",
      "    def create_extended_attention_mask_for_decoder(input_shape, attention_mask, device=None):...\n",
      "\n",
      "    def get_extended_attention_mask(self, attention_mask: Tensor, input_shape: tuple[int, ...], device: Optional[torch.device]=None, dtype: Optional[torch.dtype]=None) -> Tensor:...\n",
      "\n",
      "    def get_head_mask(self, head_mask: Optional[Tensor], num_hidden_layers: int, is_attention_chunked: bool=False) -> Tensor:...\n",
      "\n",
      "    def _convert_head_mask_to_5d(self, head_mask, num_hidden_layers):...\n",
      "\n",
      "    def num_parameters(self, only_trainable: bool=False, exclude_embeddings: bool=False) -> int:...\n",
      "\n",
      "    def estimate_tokens(self, input_dict: dict[str, Union[torch.Tensor, Any]]) -> int:...\n",
      "\n",
      "    def floating_point_ops(self, input_dict: dict[str, Union[torch.Tensor, Any]], exclude_embeddings: bool=True) -> int:...\n",
      "\n",
      "class EmbeddingAccessMixin:\n",
      "    _input_embed_layer = 'embed_tokens'\n",
      "\n",
      "    def get_input_embeddings(self) -> nn.Module:...\n",
      "\n",
      "    def set_input_embeddings(self, value: nn.Module):...\n",
      "\n",
      "    def get_output_embeddings(self):...\n",
      "\n",
      "    def set_output_embeddings(self, new_embeddings):...\n",
      "\n",
      "class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToHubMixin, PeftAdapterMixin):\n",
      "    config_class = None\n",
      "    base_model_prefix = ''\n",
      "    main_input_name = 'input_ids'\n",
      "    model_tags = None\n",
      "    _checkpoint_conversion_mapping = {}\n",
      "    _auto_class = None\n",
      "    _no_split_modules = None\n",
      "    _skip_keys_device_placement = None\n",
      "    _keep_in_fp32_modules = None\n",
      "    _keep_in_fp32_modules_strict = None\n",
      "    _keys_to_ignore_on_load_missing = None\n",
      "    _keys_to_ignore_on_load_unexpected = None\n",
      "    _keys_to_ignore_on_save = None\n",
      "    _tied_weights_keys = None\n",
      "    is_parallelizable = False\n",
      "    supports_gradient_checkpointing = False\n",
      "    _is_stateful = False\n",
      "    _supports_flash_attn = False\n",
      "    _supports_sdpa = False\n",
      "    _supports_flex_attn = False\n",
      "    _can_compile_fullgraph = False\n",
      "    _tp_plan = None\n",
      "    _tp_size = None\n",
      "    _pp_plan = None\n",
      "    _supports_attention_backend = False\n",
      "    _can_record_outputs = None\n",
      "\n",
      "    @property\n",
      "    @torch._dynamo.allow_in_graph\n",
      "    def can_record_outputs(self) -> dict[str, OutputRecorder]:...\n",
      "\n",
      "    @property\n",
      "    def dummy_inputs(self) -> dict[str, torch.Tensor]:...\n",
      "\n",
      "    @property\n",
      "    def framework(self) -> str:...\n",
      "\n",
      "    def __init_subclass__(cls, **kwargs):\n",
      "        super().__init_subclass__(**kwargs)\n",
      "        child_annotation = cls.__dict__.get('__annotations__', {}).get('config', None)\n",
      "        child_attribute = cls.__dict__.get('config_class', None)\n",
      "        full_annotation = get_type_hints(cls).get('config', None)\n",
      "        full_attribute = cls.config_class\n",
      "        if child_attribute is not None:...\n",
      "        elif child_annotation is not None:\n",
      "            cls.config_class = child_annotation\n",
      "        elif full_attribute is not None:\n",
      "            cls.config_class = full_attribute\n",
      "        elif full_annotation is not None:...\n",
      "\n",
      "    def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n",
      "        super().__init__()\n",
      "        if not isinstance(config, PretrainedConfig):...\n",
      "        self.config = config\n",
      "        self.config._attn_implementation_internal = self._check_and_adjust_attn_implementation(self.config._attn_implementation, is_init_check=True)\n",
      "        loss_type = self.__class__.__name__\n",
      "        if loss_type not in LOSS_MAPPING:\n",
      "            loss_groups = f\"({'|'.join(LOSS_MAPPING)})\"\n",
      "            loss_type = re.findall(loss_groups, self.__class__.__name__)\n",
      "            if len(loss_type) > 0:\n",
      "                loss_type = loss_type[0]\n",
      "            else:\n",
      "                loss_type = None\n",
      "        self.loss_type = loss_type\n",
      "        self.name_or_path = config.name_or_path\n",
      "        self.warnings_issued = {}\n",
      "        self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None\n",
      "        self._keep_in_fp32_modules = copy.copy(self.__class__._keep_in_fp32_modules)\n",
      "        self._keep_in_fp32_modules_strict = copy.copy(self.__class__._keep_in_fp32_modules_strict)\n",
      "        self._no_split_modules = self._no_split_modules or []\n",
      "        _CAN_RECORD_REGISTRY[str(self.__class__)] = self._can_record_outputs\n",
      "\n",
      "    def post_init(self):...\n",
      "        self.init_weights()\n",
      "        self._backward_compatibility_gradient_checkpointing()\n",
      "        if self._keep_in_fp32_modules is not None or self._keep_in_fp32_modules_strict is not None:\n",
      "            all_parameters = {name for name, _ in self.named_parameters() if len(name) > 0}\n",
      "            unique_module_names = set()\n",
      "            for param in all_parameters:\n",
      "                unique_module_names.update([name for name in param.split('.') if not name.isnumeric() and name not in ['weight', 'bias']])\n",
      "            if self._keep_in_fp32_modules is not None:\n",
      "                for module in self._keep_in_fp32_modules:\n",
      "                    if module not in unique_module_names:...\n",
      "            if self._keep_in_fp32_modules_strict is not None:...\n",
      "        self._pp_plan = self.config.base_model_pp_plan.copy() if self.config.base_model_pp_plan is not None else {}\n",
      "        self._tp_plan = self.config.base_model_tp_plan.copy() if self.config.base_model_tp_plan is not None else {}\n",
      "        self._ep_plan = self.config.base_model_ep_plan.copy() if self.config.base_model_ep_plan is not None else {}\n",
      "        for name, module in self.named_children():\n",
      "            if (plan := getattr(module, '_ep_plan', None)):...\n",
      "            if (plan := getattr(module, '_tp_plan', None)):\n",
      "                self._tp_plan.update({f'{name}.{k}': v for k, v in plan.copy().items()})\n",
      "            if (plan := getattr(module, '_pp_plan', None)):\n",
      "                self._pp_plan.update({f'{name}.{k}': v for k, v in plan.copy().items()})\n",
      "\n",
      "    @property\n",
      "    def tp_plan(self) -> dict[str, str]:...\n",
      "\n",
      "    @property\n",
      "    def pp_plan(self) -> dict[str, tuple[str, str]]:...\n",
      "\n",
      "    @tp_plan.setter\n",
      "    def tp_plan(self, plan: dict[str, str]):...\n",
      "\n",
      "    @pp_plan.setter\n",
      "    def pp_plan(self, plan: dict[str, tuple[str, str]]):...\n",
      "\n",
      "    def dequantize(self):...\n",
      "\n",
      "    def _backward_compatibility_gradient_checkpointing(self):\n",
      "        if self.supports_gradient_checkpointing and getattr(self.config, 'gradient_checkpointing', False):...\n",
      "\n",
      "    def add_model_tags(self, tags: Union[list[str], str]) -> None:...\n",
      "\n",
      "    @classmethod\n",
      "    @restore_default_dtype\n",
      "    def _from_config(cls, config, **kwargs):...\n",
      "\n",
      "    @classmethod\n",
      "    def _set_default_dtype(cls, dtype: torch.dtype) -> torch.dtype:...\n",
      "\n",
      "    @property\n",
      "    def base_model(self) -> nn.Module:...\n",
      "\n",
      "    @classmethod\n",
      "    def can_generate(cls) -> bool:...\n",
      "        if 'GenerationMixin' in str(cls.__bases__):\n",
      "            return True\n",
      "        for base in cls.__bases__:\n",
      "            if not hasattr(base, 'can_generate'):...\n",
      "            if 'PreTrainedModel' not in str(base) and base.can_generate():...\n",
      "        if hasattr(cls, 'prepare_inputs_for_generation'):...\n",
      "        return False\n",
      "\n",
      "    def _flash_attn_2_can_dispatch(self, is_init_check: bool=False) -> bool:...\n",
      "\n",
      "    def _flash_attn_3_can_dispatch(self, is_init_check: bool=False) -> bool:...\n",
      "\n",
      "    def _sdpa_can_dispatch(self, is_init_check: bool=False) -> bool:...\n",
      "        if not self._supports_sdpa:\n",
      "            raise ValueError(f'{self.__class__.__name__} does not support an attention implementation through torch.nn.functional.scaled_dot_product_attention yet. Please request the support for this architecture: https://github.com/huggingface/transformers/issues/28005. If you believe this error is a bug, please open an issue in Transformers GitHub repository and load your model with the argument `attn_implementation=\"eager\"` meanwhile. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`')...\n",
      "\n",
      "    def _flex_attn_can_dispatch(self, is_init_check: bool=False) -> bool:...\n",
      "\n",
      "    def _check_and_adjust_attn_implementation(self, attn_implementation: Optional[str], is_init_check: bool=False) -> str:...\n",
      "        applicable_attn_implementation = attn_implementation\n",
      "        if attn_implementation is not None and attn_implementation.startswith('flash_attention') and ...:...\n",
      "        if is_kernel(applicable_attn_implementation):...\n",
      "        else:\n",
      "            applicable_attn_implementation = self.get_correct_attn_implementation(applicable_attn_implementation, is_init_check)\n",
      "            if applicable_attn_implementation.startswith('flash_attention'):...\n",
      "        return applicable_attn_implementation\n",
      "\n",
      "    def get_correct_attn_implementation(self, requested_attention: Optional[str], is_init_check: bool=False) -> str:\n",
      "        applicable_attention = 'sdpa' if requested_attention is None else requested_attention\n",
      "        if applicable_attention not in ['eager'] + ALL_ATTENTION_FUNCTIONS.valid_keys():...\n",
      "        if applicable_attention == 'flash_attention_2':...\n",
      "        elif applicable_attention == 'flash_attention_3':...\n",
      "        elif applicable_attention == 'flex_attention':...\n",
      "        elif applicable_attention == 'sdpa':\n",
      "            try:\n",
      "                self._sdpa_can_dispatch(is_init_check)\n",
      "            except (ValueError, ImportError) as e:\n",
      "                if requested_attention == 'sdpa':...\n",
      "                applicable_attention = 'eager'\n",
      "        return applicable_attention\n",
      "\n",
      "    @classmethod\n",
      "    def _can_set_attn_implementation(cls) -> bool:...\n",
      "\n",
      "    def set_attn_implementation(self, attn_implementation: Union[str, dict]):...\n",
      "\n",
      "    def enable_input_require_grads(self):...\n",
      "\n",
      "    def disable_input_require_grads(self):...\n",
      "\n",
      "    def get_decoder(self):...\n",
      "\n",
      "    def set_decoder(self, decoder):...\n",
      "\n",
      "    def _init_weights(self, module):...\n",
      "\n",
      "    def _initialize_weights(self, module):...\n",
      "        if getattr(module, '_is_hf_initialized', False):\n",
      "            return\n",
      "        self._init_weights(module)\n",
      "        module._is_hf_initialized = True\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def initialize_weights(self):...\n",
      "        if not hasattr(torch.nn.Module, 'smart_apply'):\n",
      "\n",
      "            def smart_apply(self, fn):\n",
      "                for module in self.children():\n",
      "                    if isinstance(module, PreTrainedModel):\n",
      "                        module.smart_apply(module._initialize_weights)\n",
      "                    else:\n",
      "                        module.smart_apply(fn)\n",
      "                fn(self)\n",
      "                return self\n",
      "            torch.nn.Module.smart_apply = smart_apply\n",
      "        self.smart_apply(self._initialize_weights)\n",
      "\n",
      "    def tie_embeddings_and_encoder_decoder(self):...\n",
      "        if getattr(self.config.get_text_config(decoder=True), 'tie_word_embeddings', True):...\n",
      "        if getattr(self.config, 'is_encoder_decoder', False) and getattr(self.config, 'tie_encoder_decoder', False):...\n",
      "\n",
      "    def tie_weights(self):...\n",
      "        for module in self.modules():\n",
      "            if isinstance(module, PreTrainedModel):\n",
      "                module.tie_embeddings_and_encoder_decoder()\n",
      "            if hasattr(module, '_tie_weights'):...\n",
      "\n",
      "    @staticmethod\n",
      "    def _tie_encoder_decoder_weights(encoder: nn.Module, decoder: nn.Module, base_model_prefix: str, base_encoder_name: str):...\n",
      "\n",
      "    def _tie_or_clone_weights(self, output_embeddings, input_embeddings):...\n",
      "\n",
      "    def _get_no_split_modules(self, device_map: str):...\n",
      "\n",
      "    def resize_token_embeddings(self, new_num_tokens: Optional[int]=None, pad_to_multiple_of: Optional[int]=None, mean_resizing: bool=True) -> nn.Embedding:...\n",
      "\n",
      "    def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None, mean_resizing=True):...\n",
      "\n",
      "    def _get_resized_embeddings(self, old_embeddings: nn.Embedding, new_num_tokens: Optional[int]=None, pad_to_multiple_of: Optional[int]=None, mean_resizing: bool=True) -> nn.Embedding:...\n",
      "\n",
      "    def _get_resized_lm_head(self, old_lm_head: nn.Linear, new_num_tokens: Optional[int]=None, transposed: bool=False, mean_resizing: bool=True) -> nn.Linear:...\n",
      "\n",
      "    def _init_added_embeddings_weights_with_mean(self, old_embeddings, new_embeddings, old_embedding_dim, old_num_tokens, added_num_tokens):...\n",
      "\n",
      "    def _init_added_lm_head_weights_with_mean(self, old_lm_head, new_lm_head, old_lm_head_dim, old_num_tokens, added_num_tokens, transposed: bool=False):...\n",
      "\n",
      "    def _init_added_lm_head_bias_with_mean(self, old_lm_head, new_lm_head, added_num_tokens):...\n",
      "\n",
      "    def _copy_lm_head_original_to_resized(self, new_lm_head, old_lm_head, num_tokens_to_copy, transposed, has_new_lm_head_bias):...\n",
      "\n",
      "    def resize_position_embeddings(self, new_num_position_embeddings: int):...\n",
      "\n",
      "    def get_position_embeddings(self) -> Union[nn.Embedding, tuple[nn.Embedding]]:...\n",
      "\n",
      "    def init_weights(self):...\n",
      "        if self.config.pruned_heads:...\n",
      "        if _init_weights:...\n",
      "\n",
      "    def prune_heads(self, heads_to_prune: dict[int, list[int]]):...\n",
      "\n",
      "    def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):...\n",
      "\n",
      "    def _set_gradient_checkpointing(self, enable: bool=True, gradient_checkpointing_func: Callable=checkpoint):...\n",
      "\n",
      "    def gradient_checkpointing_disable(self):...\n",
      "\n",
      "    @property\n",
      "    def is_gradient_checkpointing(self) -> bool:...\n",
      "\n",
      "    def save_pretrained(self, save_directory: Union[str, os.PathLike], is_main_process: bool=True, state_dict: Optional[dict]=None, save_function: Callable=torch.save, push_to_hub: bool=False, max_shard_size: Union[int, str]='5GB', safe_serialization: bool=True, variant: Optional[str]=None, token: Optional[Union[str, bool]]=None, save_peft_format: bool=True, **kwargs):...\n",
      "\n",
      "    @wraps(PushToHubMixin.push_to_hub)\n",
      "    def push_to_hub(self, *args, **kwargs):...\n",
      "\n",
      "    def get_memory_footprint(self, return_buffers=True):...\n",
      "\n",
      "    @wraps(torch.nn.Module.cuda)\n",
      "    def cuda(self, *args, **kwargs):...\n",
      "\n",
      "    @wraps(torch.nn.Module.to)\n",
      "    def to(self, *args, **kwargs):...\n",
      "\n",
      "    def half(self, *args):...\n",
      "\n",
      "    def float(self, *args):...\n",
      "\n",
      "    @classmethod\n",
      "    def get_init_context(cls, is_quantized: bool, _is_ds_init_called: bool):\n",
      "        if is_deepspeed_zero3_enabled():...\n",
      "        else:\n",
      "            init_contexts = [no_init_weights(), init_empty_weights()]\n",
      "        return init_contexts\n",
      "\n",
      "    @classmethod\n",
      "    @restore_default_dtype\n",
      "    def from_pretrained(cls: type[SpecificPreTrainedModelType], pretrained_model_name_or_path: Optional[Union[str, os.PathLike]], *model_args, config: Optional[Union[PretrainedConfig, str, os.PathLike]]=None, cache_dir: Optional[Union[str, os.PathLike]]=None, ignore_mismatched_sizes: bool=False, force_download: bool=False, local_files_only: bool=False, token: Optional[Union[str, bool]]=None, revision: str='main', use_safetensors: Optional[bool]=None, weights_only: bool=True, **kwargs) -> SpecificPreTrainedModelType:...\n",
      "        state_dict = kwargs.pop('state_dict', None)\n",
      "        from_tf = kwargs.pop('from_tf', False)\n",
      "        from_flax = kwargs.pop('from_flax', False)\n",
      "        proxies = kwargs.pop('proxies', None)\n",
      "        output_loading_info = kwargs.pop('output_loading_info', False)\n",
      "        use_auth_token = kwargs.pop('use_auth_token', None)\n",
      "        from_pipeline = kwargs.pop('_from_pipeline', None)\n",
      "        from_auto_class = kwargs.pop('_from_auto', False)\n",
      "        dtype = kwargs.pop('dtype', None)\n",
      "        torch_dtype = kwargs.pop('torch_dtype', None)\n",
      "        device_map = kwargs.pop('device_map', None)\n",
      "        max_memory = kwargs.pop('max_memory', None)\n",
      "        offload_folder = kwargs.pop('offload_folder', None)\n",
      "        offload_buffers = kwargs.pop('offload_buffers', False)\n",
      "        load_in_8bit = kwargs.pop('load_in_8bit', False)\n",
      "        load_in_4bit = kwargs.pop('load_in_4bit', False)\n",
      "        quantization_config = kwargs.pop('quantization_config', None)\n",
      "        subfolder = kwargs.pop('subfolder', '')\n",
      "        commit_hash = kwargs.pop('_commit_hash', None)\n",
      "        variant = kwargs.pop('variant', None)\n",
      "        adapter_kwargs = kwargs.pop('adapter_kwargs', {})\n",
      "        adapter_name = kwargs.pop('adapter_name', 'default')\n",
      "        generation_config = kwargs.pop('generation_config', None)\n",
      "        gguf_file = kwargs.pop('gguf_file', None)\n",
      "        tp_plan = kwargs.pop('tp_plan', None)\n",
      "        tp_size = kwargs.pop('tp_size', None)\n",
      "        distributed_config: DistributedConfig = kwargs.pop('distributed_config', None)\n",
      "        device_mesh = kwargs.pop('device_mesh', None)\n",
      "        trust_remote_code = kwargs.pop('trust_remote_code', None)\n",
      "        use_kernels = kwargs.pop('use_kernels', False)\n",
      "        key_mapping = kwargs.pop('key_mapping', None)\n",
      "        if key_mapping is None and any((allowed_name in class_name.__name__.lower() for class_name in cls.__mro__[:-1] for allowed_name in VLMS)):...\n",
      "        if distributed_config is not None:...\n",
      "        _ = kwargs.pop('resume_download', None)\n",
      "        _ = kwargs.pop('mirror', None)\n",
      "        _ = kwargs.pop('_fast_init', True)\n",
      "        _ = kwargs.pop('low_cpu_mem_usage', None)\n",
      "        _ = kwargs.pop('offload_state_dict', None)\n",
      "        if torch_dtype is not None:...\n",
      "        if state_dict is not None and (pretrained_model_name_or_path is not None or gguf_file is not None):...\n",
      "        if tp_size is not None and tp_plan is None:...\n",
      "        if tp_plan is not None and tp_plan != 'auto':...\n",
      "        if tp_plan is not None and device_map is not None:...\n",
      "        if device_map == 'auto' and int(os.environ.get('WORLD_SIZE', '0')):...\n",
      "        if tp_plan is not None:...\n",
      "        if use_auth_token is not None:...\n",
      "        if token is not None and adapter_kwargs is not None and ('token' not in adapter_kwargs):...\n",
      "        if gguf_file is not None and (not is_accelerate_available()):...\n",
      "        if commit_hash is None:\n",
      "            if not isinstance(config, PretrainedConfig):...\n",
      "            else:\n",
      "                commit_hash = getattr(config, '_commit_hash', None)\n",
      "        if is_peft_available():...\n",
      "        else:\n",
      "            _adapter_model_path = None\n",
      "        if device_map is None and (not is_deepspeed_zero3_enabled()):\n",
      "            device_in_context = get_torch_context_manager_or_global_device()\n",
      "            if device_in_context == torch.device('meta'):...\n",
      "            device_map = device_in_context\n",
      "        if isinstance(device_map, torch.device):...\n",
      "        elif isinstance(device_map, str) and device_map not in ['auto', 'balanced', 'balanced_low_0', 'sequential']:...\n",
      "        elif isinstance(device_map, int):...\n",
      "        if device_map is not None:...\n",
      "        if load_in_4bit or load_in_8bit:...\n",
      "        from_pt = not from_tf | from_flax\n",
      "        user_agent = {'file_type': 'model', 'framework': 'pytorch', 'from_auto_class': from_auto_class}\n",
      "        if from_pipeline is not None:...\n",
      "        if is_offline_mode() and (not local_files_only):...\n",
      "        if not isinstance(config, PretrainedConfig):...\n",
      "        else:\n",
      "            config = copy.deepcopy(config)\n",
      "            model_kwargs = kwargs\n",
      "        if 'attn_implementation' in kwargs:...\n",
      "        transformers_explicit_filename = getattr(config, 'transformers_weights', None)\n",
      "        if transformers_explicit_filename is not None:...\n",
      "        hf_quantizer, config, dtype, device_map = get_hf_quantizer(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent)\n",
      "        if gguf_file is not None and hf_quantizer is not None:...\n",
      "        if gguf_file and ...:...\n",
      "        checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(pretrained_model_name_or_path=pretrained_model_name_or_path, subfolder=subfolder, variant=variant, gguf_file=gguf_file, from_tf=from_tf, from_flax=from_flax, use_safetensors=use_safetensors, cache_dir=cache_dir, force_download=force_download, proxies=proxies, local_files_only=local_files_only, token=token, user_agent=user_agent, revision=revision, commit_hash=commit_hash, is_remote_code=cls._auto_class is not None, transformers_explicit_filename=transformers_explicit_filename)\n",
      "        is_sharded = sharded_metadata is not None\n",
      "        is_quantized = hf_quantizer is not None\n",
      "        is_from_file = pretrained_model_name_or_path is not None or gguf_file is not None\n",
      "        if is_from_file and (not is_sharded) and checkpoint_files[0].endswith('.safetensors'):...\n",
      "        from_pt = not from_tf | from_flax\n",
      "        if from_pt:\n",
      "            if gguf_file:...\n",
      "            config, dtype, dtype_orig = _get_dtype(cls, dtype, checkpoint_files, config, sharded_metadata, state_dict, weights_only)\n",
      "        config.name_or_path = pretrained_model_name_or_path\n",
      "        model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)\n",
      "        config = copy.deepcopy(config)\n",
      "        with ContextManagers(model_init_context):\n",
      "            model = cls(config, *model_args, **model_kwargs)\n",
      "        model.tie_weights()\n",
      "        config = model.config\n",
      "        keep_in_fp32_modules = []\n",
      "        if model._keep_in_fp32_modules is not None and (dtype == torch.float16 or getattr(hf_quantizer, 'use_keep_in_fp32_modules', False)):...\n",
      "        if model._keep_in_fp32_modules_strict is not None and (dtype == torch.float16 or dtype == torch.bfloat16):...\n",
      "        keep_in_fp32_regex = None\n",
      "        if keep_in_fp32_modules:...\n",
      "        if hf_quantizer is not None:...\n",
      "        if _torch_distributed_available and device_mesh is not None:...\n",
      "        if device_map is not None:...\n",
      "        if from_tf:...\n",
      "        elif from_flax:...\n",
      "        elif from_pt:\n",
      "            if dtype_orig is not None:...\n",
      "            model, missing_keys, unexpected_keys, mismatched_keys, offload_index, error_msgs = cls._load_pretrained_model(model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes=ignore_mismatched_sizes, sharded_metadata=sharded_metadata, device_map=device_map, disk_offload_folder=offload_folder, dtype=dtype, hf_quantizer=hf_quantizer, keep_in_fp32_regex=keep_in_fp32_regex, device_mesh=device_mesh, key_mapping=key_mapping, weights_only=weights_only)\n",
      "        model.tie_weights()\n",
      "        model.eval()\n",
      "        if use_kernels:...\n",
      "        if model.can_generate() and generation_config is not None:...\n",
      "        elif model.can_generate() and pretrained_model_name_or_path is not None:\n",
      "            repo_loading_kwargs = {'cache_dir': cache_dir, 'force_download': force_download, 'proxies': proxies, 'local_files_only': local_files_only, 'token': token, 'revision': revision, 'subfolder': subfolder, **kwargs}\n",
      "            try:\n",
      "                model.generation_config = GenerationConfig.from_pretrained(pretrained_model_name_or_path, _from_auto=from_auto_class, _from_pipeline=from_pipeline, **repo_loading_kwargs)\n",
      "            except OSError:\n",
      "                logger.info('Generation config file not found, using a generation config created from the model config.')\n",
      "                pass\n",
      "            if hasattr(model, 'load_custom_generate'):\n",
      "                try:\n",
      "                    custom_generate = model.load_custom_generate(pretrained_model_name_or_path, trust_remote_code=trust_remote_code, **repo_loading_kwargs)...\n",
      "                except OSError:\n",
      "                    pass\n",
      "        if device_map is not None and device_mesh is None:...\n",
      "        if hf_quantizer is not None:...\n",
      "        if _adapter_model_path is not None:...\n",
      "        if output_loading_info:...\n",
      "        return model\n",
      "\n",
      "    @staticmethod\n",
      "    def _fix_state_dict_key_on_load(key: str) -> tuple[str, bool]:...\n",
      "        if key.endswith('LayerNorm.beta'):...\n",
      "        if key.endswith('LayerNorm.gamma'):...\n",
      "        if hasattr(nn.utils.parametrizations, 'weight_norm'):\n",
      "            if key.endswith('weight_g'):...\n",
      "            if key.endswith('weight_v'):...\n",
      "        else:...\n",
      "        return (key, False)\n",
      "\n",
      "    def _get_key_renaming_mapping(self, checkpoint_keys: list[str], key_mapping: Optional[dict[str, str]]=None, loading_base_model_from_task_state_dict: bool=False, loading_task_model_from_base_state_dict: bool=False):...\n",
      "        prefix = self.base_model_prefix\n",
      "        _prefix = f'{prefix}.'\n",
      "        if loading_task_model_from_base_state_dict:...\n",
      "        renamed_keys = {}\n",
      "        key_renaming_mapping = {}\n",
      "        for key in checkpoint_keys:\n",
      "            new_key, has_changed = self._fix_state_dict_key_on_load(key)\n",
      "            if key_mapping is not None:...\n",
      "            if loading_task_model_from_base_state_dict:...\n",
      "            elif loading_base_model_from_task_state_dict:...\n",
      "            key_renaming_mapping[key] = new_key\n",
      "            if has_changed:...\n",
      "        if renamed_keys:...\n",
      "        return key_renaming_mapping\n",
      "\n",
      "    @staticmethod\n",
      "    def _fix_state_dict_key_on_save(key) -> tuple[str, bool]:...\n",
      "\n",
      "    def _fix_state_dict_keys_on_save(self, state_dict):...\n",
      "\n",
      "    @classmethod\n",
      "    def _load_pretrained_model(cls, model: 'PreTrainedModel', state_dict: Optional[dict], checkpoint_files: Optional[list[str]], pretrained_model_name_or_path: Optional[str], ignore_mismatched_sizes: bool=False, sharded_metadata: Optional[dict]=None, device_map: Optional[dict]=None, disk_offload_folder: Optional[str]=None, dtype: Optional[torch.dtype]=None, hf_quantizer: Optional[HfQuantizer]=None, keep_in_fp32_regex: Optional[re.Pattern]=None, device_mesh: Optional['torch.distributed.device_mesh.DeviceMesh']=None, key_mapping: Optional[dict[str, str]]=None, weights_only: bool=True):\n",
      "        is_quantized = hf_quantizer is not None\n",
      "        is_hqq_or_quark = is_quantized and hf_quantizer.quantization_config.quant_method in {...}\n",
      "        if sharded_metadata is not None:\n",
      "            original_checkpoint_keys = sharded_metadata['all_checkpoint_keys']\n",
      "        else:...\n",
      "        prefix = model.base_model_prefix\n",
      "        has_prefix_module = any((s.startswith(prefix) for s in original_checkpoint_keys)) if len(prefix) > 0 else False\n",
      "        expects_prefix_module = hasattr(model, prefix) if len(prefix) > 0 else False\n",
      "        loading_task_model_from_base_state_dict = not has_prefix_module and expects_prefix_module\n",
      "        loading_base_model_from_task_state_dict = has_prefix_module and (not expects_prefix_module)\n",
      "        key_renaming_mapping = model._get_key_renaming_mapping(original_checkpoint_keys, key_mapping, loading_base_model_from_task_state_dict, loading_task_model_from_base_state_dict)\n",
      "        checkpoint_keys = list(key_renaming_mapping.values())\n",
      "        missing_keys, unexpected_keys = _find_missing_and_unexpected_keys(model, original_checkpoint_keys, checkpoint_keys, loading_base_model_from_task_state_dict, hf_quantizer)\n",
      "        mismatched_keys, mismatched_shapes = _find_mismatched_keys(model, state_dict, checkpoint_files, ignore_mismatched_sizes, key_renaming_mapping, is_quantized, weights_only)\n",
      "        key_renaming_mapping = {k: v for k, v in key_renaming_mapping.items() if v not in mismatched_keys and v not in unexpected_keys}\n",
      "        checkpoint_keys = list(key_renaming_mapping.values())\n",
      "        model._move_missing_keys_from_meta_to_cpu(missing_keys + mismatched_keys, dtype, hf_quantizer)\n",
      "        model._initialize_missing_keys(missing_keys + mismatched_keys, is_quantized)\n",
      "        if keep_in_fp32_regex is not None:...\n",
      "        reverse_key_renaming_mapping = {v: k for k, v in key_renaming_mapping.items()}\n",
      "        is_offloaded_safetensors = False\n",
      "        disk_offload_index = None\n",
      "        disk_only_shard_files = []\n",
      "        if device_map is not None and 'disk' in device_map.values():...\n",
      "        elif state_dict is not None:...\n",
      "        expected_keys = list(model.state_dict().keys())\n",
      "        if hf_quantizer is not None:...\n",
      "        if logger.level >= logging.WARNING:...\n",
      "        if device_map is not None and (not is_hqq_or_quark):...\n",
      "        args_list = [(shard_file, state_dict, disk_only_shard_files, is_quantized, device_map, hf_quantizer, key_renaming_mapping, weights_only, model, reverse_key_renaming_mapping, disk_offload_folder, disk_offload_index, keep_in_fp32_regex, device_mesh) for shard_file in checkpoint_files]\n",
      "        error_msgs = []\n",
      "        if os.environ.get('HF_ENABLE_PARALLEL_LOADING', '').upper() in ENV_VARS_TRUE_VALUES and ...:...\n",
      "        else:\n",
      "            if len(args_list) > 1:\n",
      "                args_list = logging.tqdm(args_list, desc='Loading checkpoint shards')\n",
      "            for args in args_list:\n",
      "                _error_msgs, disk_offload_index = load_shard_file(args)\n",
      "                error_msgs += _error_msgs\n",
      "        if disk_offload_index is not None and len(disk_offload_index) > 0 and (not is_offloaded_safetensors):...\n",
      "        if device_mesh is not None:...\n",
      "        missing_keys, unexpected_keys = model._adjust_missing_and_unexpected_keys(missing_keys, unexpected_keys, loading_task_model_from_base_state_dict)\n",
      "        if len(error_msgs) > 0:...\n",
      "        if len(unexpected_keys) > 0:...\n",
      "        if len(missing_keys) > 0:...\n",
      "        if len(mismatched_keys) > 0:...\n",
      "        return (model, missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, error_msgs)\n",
      "\n",
      "    @classmethod\n",
      "    def _load_from_tf(cls, model, config, checkpoint_files):...\n",
      "\n",
      "    @classmethod\n",
      "    def _load_from_flax(cls, model, checkpoint_files):...\n",
      "\n",
      "    def retrieve_modules_from_names(self, names, add_prefix=False, remove_prefix=False):...\n",
      "\n",
      "    @classmethod\n",
      "    def register_for_auto_class(cls, auto_class='AutoModel'):...\n",
      "\n",
      "    def to_bettertransformer(self) -> 'PreTrainedModel':...\n",
      "\n",
      "    def reverse_bettertransformer(self):...\n",
      "\n",
      "    def warn_if_padding_and_no_attention_mask(self, input_ids, attention_mask):...\n",
      "\n",
      "    @property\n",
      "    def supports_tp_plan(self):...\n",
      "\n",
      "    @property\n",
      "    def tp_size(self):...\n",
      "\n",
      "    @property\n",
      "    def supports_pp_plan(self):...\n",
      "\n",
      "    @property\n",
      "    def loss_function(self):...\n",
      "\n",
      "    @loss_function.setter\n",
      "    def loss_function(self, value):...\n",
      "\n",
      "    def kernelize(self):...\n",
      "\n",
      "    @property\n",
      "    def use_kernels(self) -> bool:\n",
      "        return getattr(self, '_use_kernels', False)\n",
      "\n",
      "    @use_kernels.setter\n",
      "    def use_kernels(self, value: bool) -> None:...\n",
      "\n",
      "    def get_compiled_call(self, compile_config: Optional[CompileConfig]) -> Callable:...\n",
      "\n",
      "    @classmethod\n",
      "    def is_backend_compatible(cls):...\n",
      "\n",
      "    def _move_missing_keys_from_meta_to_cpu(self, missing_keys: list[str], dtype: torch.dtype, hf_quantizer: Optional[HfQuantizer]) -> None:...\n",
      "        is_quantized = hf_quantizer is not None\n",
      "        if is_fsdp_enabled() and (not is_local_dist_rank_0()) and (not is_quantized):...\n",
      "        model_state_dict = self.state_dict()\n",
      "        for key in missing_keys:...\n",
      "\n",
      "    def _initialize_missing_keys(self, missing_keys: list[str], is_quantized: bool) -> None:...\n",
      "        for key in self.state_dict():\n",
      "            if key not in missing_keys:\n",
      "                param_or_buffer = self.get_parameter_or_buffer(key)\n",
      "                param_or_buffer._is_hf_initialized = True\n",
      "\n",
      "        def set_is_initialized_for_modules(module):\n",
      "            if all((getattr(child, '_is_hf_initialized', False) for child in module.children())) and all((getattr(param, '_is_hf_initialized', False) for param in module.parameters(recurse=False))) and all((getattr(buffer, '_is_hf_initialized', False) for buffer in module.buffers(recurse=False) if buffer not in module._non_persistent_buffers_set)):\n",
      "                module._is_hf_initialized = True\n",
      "        self.apply(set_is_initialized_for_modules)\n",
      "        if is_deepspeed_zero3_enabled() and (not is_quantized):...\n",
      "        else:\n",
      "            self.initialize_weights()\n",
      "\n",
      "    def _adjust_missing_and_unexpected_keys(self, missing_keys: list[str], unexpected_keys: list[str], loading_task_model_from_base_state_dict: bool) -> tuple[list[str], list[str]]:...\n",
      "        has_inv_freq_buffers = any((buffer.endswith('rotary_emb.inv_freq') for buffer, _ in self.named_buffers()))\n",
      "        additional_unexpected_patterns = ['rotary_emb\\\\.inv_freq'] if has_inv_freq_buffers else []\n",
      "        missing_patterns = self._keys_to_ignore_on_load_missing or []\n",
      "        unexpected_patterns = (self._keys_to_ignore_on_load_unexpected or []) + additional_unexpected_patterns\n",
      "        ignore_missing_regex, ignore_unexpected_regex = (None, None)\n",
      "        if len(missing_patterns) > 0:...\n",
      "        if len(unexpected_patterns) > 0:\n",
      "            ignore_unexpected_regex = re.compile('|'.join((f'({pattern})' for pattern in unexpected_patterns)))\n",
      "        if ignore_missing_regex is not None:...\n",
      "        if ignore_unexpected_regex is not None:\n",
      "            unexpected_keys = [key for key in unexpected_keys if ignore_unexpected_regex.search(key) is None]\n",
      "        if loading_task_model_from_base_state_dict:...\n",
      "        return (missing_keys, unexpected_keys)\n",
      "\n",
      "    def get_parameter_or_buffer(self, target: str):...\n",
      "        try:\n",
      "            return self.get_parameter(target)\n",
      "        except AttributeError:\n",
      "            pass...\n",
      "\n",
      "    def train(self, mode: bool=True):\n",
      "        out = super().train(mode)\n",
      "        if self.use_kernels:...\n",
      "        return out\n",
      "\n",
      "    def eval(self):\n",
      "        return self.train(False)\n",
      "PreTrainedModel.push_to_hub = copy_func(PreTrainedModel.push_to_hub)\n",
      "if PreTrainedModel.push_to_hub.__doc__ is not None:\n",
      "    PreTrainedModel.push_to_hub.__doc__ = PreTrainedModel.push_to_hub.__doc__.format(object='model', object_class='AutoModel', object_files='model file')\n",
      "\n",
      "def unwrap_model(model: nn.Module, recursive: bool=False) -> nn.Module:...\n",
      "\n",
      "def expand_device_map(device_map, param_names):...\n",
      "\n",
      "def is_accelerator_device(device: Union[str, int, torch.device]) -> bool:...\n",
      "\n",
      "def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict, hf_quantizer: Optional[HfQuantizer]):...\n",
      "\n",
      "def get_disk_only_shard_files(device_map, weight_map):...\n",
      "\n",
      "class AttentionInterface(GeneralInterface):\n",
      "    _global_mapping = {'flash_attention_3': flash_attention_forward, 'flash_attention_2': flash_attention_forward, 'flex_attention': flex_attention_forward, 'paged_attention': paged_attention_forward, 'sdpa': sdpa_attention_forward, 'sdpa_paged': sdpa_attention_paged_forward, 'eager_paged': eager_paged_attention_forward}\n",
      "ALL_ATTENTION_FUNCTIONS: AttentionInterface = AttentionInterface()\n",
      "\n",
      "class PreTrainedAudioTokenizerBase(PreTrainedModel):\n",
      "\n",
      "    @abstractmethod\n",
      "    def encode(self, input_values: torch.Tensor, *args, **kwargs):...\n",
      "\n",
      "    @abstractmethod\n",
      "    def decode(self, audio_codes: torch.Tensor, *args, **kwargs):...\n"
     ]
    }
   ],
   "source": [
    "print(sketches['/home/jw2858/miniconda3/envs/NKI/lib/python3.12/site-packages/transformers/modeling_utils.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b72f6346-638d-4abe-bc68-34636b1cedeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import itertools\n",
      "from typing import Callable, Optional, Union\n",
      "import torch\n",
      "import torch.nn.functional as F\n",
      "from .cache_utils import Cache\n",
      "from .configuration_utils import PretrainedConfig\n",
      "from .utils import is_torch_xpu_available, logging\n",
      "from .utils.generic import GeneralInterface\n",
      "from .utils.import_utils import is_torch_flex_attn_available, is_torch_greater_or_equal, is_torchdynamo_compiling\n",
      "if is_torch_flex_attn_available():\n",
      "    from torch.nn.attention.flex_attention import _DEFAULT_SPARSE_BLOCK_SIZE as flex_default_block_size\n",
      "    from torch.nn.attention.flex_attention import BlockMask, create_block_mask\n",
      "else:...\n",
      "_is_torch_greater_or_equal_than_2_5 = is_torch_greater_or_equal('2.5', accept_dev=True)\n",
      "_is_torch_greater_or_equal_than_2_6 = is_torch_greater_or_equal('2.6', accept_dev=True)\n",
      "_is_torch_xpu_available = is_torch_xpu_available()\n",
      "if _is_torch_greater_or_equal_than_2_6:\n",
      "    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex\n",
      "logger = logging.get_logger(__name__)\n",
      "\n",
      "def and_masks(*mask_functions: Callable) -> Callable:...\n",
      "    if not all((callable(arg) for arg in mask_functions)):...\n",
      "\n",
      "    def and_mask(batch_idx, head_idx, q_idx, kv_idx):\n",
      "        result = q_idx.new_ones((), dtype=torch.bool)\n",
      "        for mask in mask_functions:\n",
      "            result = result & mask(batch_idx, head_idx, q_idx, kv_idx).to(result.device)\n",
      "        return result\n",
      "    return and_mask\n",
      "\n",
      "def or_masks(*mask_functions: Callable) -> Callable:...\n",
      "\n",
      "def causal_mask_function(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:...\n",
      "    return kv_idx <= q_idx\n",
      "\n",
      "def sliding_window_overlay(sliding_window: int) -> Callable:...\n",
      "\n",
      "    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n",
      "        return kv_idx > q_idx - sliding_window\n",
      "    return inner_mask\n",
      "\n",
      "def chunked_overlay(chunk_size: int, left_padding: torch.Tensor) -> Callable:...\n",
      "\n",
      "def _legacy_chunked_overlay(chunk_size: int) -> Callable:...\n",
      "\n",
      "def sliding_window_causal_mask_function(sliding_window: int) -> Callable:...\n",
      "    return and_masks(sliding_window_overlay(sliding_window), causal_mask_function)\n",
      "\n",
      "def chunked_causal_mask_function(chunk_size: int, left_padding: torch.Tensor) -> Callable:...\n",
      "\n",
      "def padding_mask_function(padding_mask: torch.Tensor) -> Callable:...\n",
      "\n",
      "    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n",
      "        return padding_mask[batch_idx, kv_idx]\n",
      "    return inner_mask\n",
      "\n",
      "def packed_sequence_mask_function(packed_sequence_mask: torch.Tensor) -> Callable:...\n",
      "\n",
      "def add_offsets_to_mask_function(mask_function: Callable, q_offset: int, kv_offset: int) -> Callable:...\n",
      "\n",
      "def _vmap_for_bhqkv(mask_function: Callable, bh_indices: bool=True) -> Callable:...\n",
      "    dimensions = [(None, None, None, 0), (None, None, 0, None)]\n",
      "    if bh_indices:\n",
      "        dimensions.extend([(None, 0, None, None), (0, None, None, None)])\n",
      "    for dims in dimensions:\n",
      "        mask_function = torch.vmap(mask_function, in_dims=dims, out_dims=0)\n",
      "    return mask_function\n",
      "\n",
      "def prepare_padding_mask(attention_mask: Optional[torch.Tensor], kv_length: int, kv_offset: int, _slice: bool=True) -> Optional[torch.Tensor]:...\n",
      "    local_padding_mask = attention_mask\n",
      "    if attention_mask is not None:\n",
      "        if (padding_length := (kv_length + kv_offset - attention_mask.shape[-1])) > 0:...\n",
      "        if _slice:...\n",
      "    return local_padding_mask\n",
      "\n",
      "def _ignore_causal_mask_sdpa(padding_mask: Optional[torch.Tensor], query_length: int, kv_length: int, kv_offset: int, local_attention_size: Optional[int]=None) -> bool:...\n",
      "\n",
      "def sdpa_mask_recent_torch(batch_size: int, cache_position: torch.Tensor, kv_length: int, kv_offset: int=0, mask_function: Callable=causal_mask_function, attention_mask: Optional[torch.Tensor]=None, local_size: Optional[int]=None, allow_is_causal_skip: bool=True, **kwargs) -> Optional[torch.Tensor]:...\n",
      "    q_length = cache_position.shape[0]\n",
      "    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n",
      "    if allow_is_causal_skip and _ignore_causal_mask_sdpa(padding_mask, q_length, kv_length, kv_offset, local_size):...\n",
      "    kv_arange = torch.arange(kv_length, device=cache_position.device)\n",
      "    kv_arange += kv_offset\n",
      "    if padding_mask is not None:\n",
      "        mask_function = and_masks(mask_function, padding_mask_function(padding_mask))\n",
      "    batch_arange = torch.arange(batch_size, device=cache_position.device)\n",
      "    head_arange = torch.arange(1, device=cache_position.device)\n",
      "    with TransformGetItemToIndex():\n",
      "        causal_mask = _vmap_for_bhqkv(mask_function)(batch_arange, head_arange, cache_position, kv_arange)\n",
      "    return causal_mask\n",
      "\n",
      "def sdpa_mask_older_torch(batch_size: int, cache_position: torch.Tensor, kv_length: int, kv_offset: int=0, mask_function: Callable=causal_mask_function, attention_mask: Optional[torch.Tensor]=None, local_size: Optional[int]=None, allow_is_causal_skip: bool=True, allow_torch_fix: bool=True, **kwargs) -> Optional[torch.Tensor]:...\n",
      "sdpa_mask = sdpa_mask_recent_torch if _is_torch_greater_or_equal_than_2_6 else sdpa_mask_older_torch\n",
      "\n",
      "def eager_mask(batch_size: int, cache_position: torch.Tensor, kv_length: int, kv_offset: int=0, mask_function: Callable=causal_mask_function, attention_mask: Optional[torch.Tensor]=None, dtype: torch.dtype=torch.float32, **kwargs) -> torch.Tensor:...\n",
      "    _ = kwargs.pop('allow_is_causal_skip', None)\n",
      "    mask = sdpa_mask(batch_size=batch_size, cache_position=cache_position, kv_length=kv_length, kv_offset=kv_offset, mask_function=mask_function, attention_mask=attention_mask, allow_is_causal_skip=False, allow_torch_fix=False, **kwargs)\n",
      "    min_dtype = torch.finfo(dtype).min\n",
      "    mask = torch.where(mask, torch.tensor(0.0, device=mask.device, dtype=dtype), min_dtype)\n",
      "    return mask\n",
      "\n",
      "def flash_attention_mask(batch_size: int, cache_position: torch.Tensor, kv_length: int, kv_offset: int=0, mask_function: Callable=causal_mask_function, attention_mask: Optional[torch.Tensor]=None, **kwargs):...\n",
      "\n",
      "def flex_attention_mask(batch_size: int, cache_position: torch.Tensor, kv_length: int, kv_offset: int=0, mask_function: Callable=causal_mask_function, attention_mask: Optional[torch.Tensor]=None, **kwargs) -> BlockMask:...\n",
      "\n",
      "class AttentionMaskInterface(GeneralInterface):\n",
      "    _global_mapping = {'sdpa': sdpa_mask, 'eager': eager_mask, 'flash_attention_2': flash_attention_mask, 'flash_attention_3': flash_attention_mask, 'flex_attention': flex_attention_mask}\n",
      "ALL_MASK_ATTENTION_FUNCTIONS: AttentionMaskInterface = AttentionMaskInterface()\n",
      "\n",
      "def find_packed_sequence_indices(position_ids: torch.Tensor) -> torch.Tensor:...\n",
      "\n",
      "def _preprocess_mask_arguments(config: PretrainedConfig, input_embeds: torch.Tensor, attention_mask: Optional[Union[torch.Tensor, BlockMask]], cache_position: torch.Tensor, past_key_values: Optional[Cache], position_ids: Optional[torch.Tensor], layer_idx: Optional[int]) -> tuple[bool, Optional[Union[torch.Tensor, BlockMask]], int, int]:...\n",
      "    if isinstance(attention_mask, (torch.Tensor, BlockMask)) and len(attention_mask.shape) == 4:...\n",
      "    if config._attn_implementation not in ALL_MASK_ATTENTION_FUNCTIONS._global_mapping:...\n",
      "    if attention_mask is not None and attention_mask.ndim == 2:\n",
      "        attention_mask = attention_mask.to(device=cache_position.device, dtype=torch.bool)\n",
      "    if past_key_values is not None:\n",
      "        kv_length, kv_offset = past_key_values.get_mask_sizes(cache_position, layer_idx)\n",
      "    else:...\n",
      "    packed_sequence_mask = None\n",
      "    if position_ids is not None and attention_mask is None and (past_key_values is None):...\n",
      "    return (False, attention_mask, packed_sequence_mask, kv_length, kv_offset)\n",
      "\n",
      "def create_causal_mask(config: PretrainedConfig, input_embeds: torch.Tensor, attention_mask: Optional[torch.Tensor], cache_position: torch.Tensor, past_key_values: Optional[Cache], position_ids: Optional[torch.Tensor]=None, or_mask_function: Optional[Callable]=None, and_mask_function: Optional[Callable]=None) -> Optional[Union[torch.Tensor, BlockMask]]:...\n",
      "    if hasattr(past_key_values, 'is_sliding') and False in past_key_values.is_sliding:\n",
      "        layer_idx = past_key_values.is_sliding.index(False)\n",
      "    else:...\n",
      "    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx)\n",
      "    if early_exit:...\n",
      "    batch_size, dtype = (input_embeds.shape[0], input_embeds.dtype)\n",
      "    mask_factory_function = causal_mask_function\n",
      "    mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n",
      "    if _is_torch_xpu_available:...\n",
      "    else:\n",
      "        allow_is_causal_skip = not getattr(past_key_values, 'is_compileable', False)\n",
      "    if or_mask_function is not None:...\n",
      "    if and_mask_function is not None:...\n",
      "    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:...\n",
      "    causal_mask = mask_interface(batch_size=batch_size, cache_position=cache_position, kv_length=kv_length, kv_offset=kv_offset, mask_function=mask_factory_function, attention_mask=attention_mask, allow_is_causal_skip=allow_is_causal_skip, dtype=dtype, config=config)\n",
      "    return causal_mask\n",
      "\n",
      "def create_sliding_window_causal_mask(config: PretrainedConfig, input_embeds: torch.Tensor, attention_mask: Optional[torch.Tensor], cache_position: torch.Tensor, past_key_values: Optional[Cache], position_ids: Optional[torch.Tensor]=None, or_mask_function: Optional[Callable]=None, and_mask_function: Optional[Callable]=None) -> Optional[Union[torch.Tensor, BlockMask]]:...\n",
      "    if hasattr(past_key_values, 'is_sliding') and True in past_key_values.is_sliding:\n",
      "        layer_idx = past_key_values.is_sliding.index(True)\n",
      "    else:...\n",
      "    early_exit, attention_mask, packed_sequence_mask, kv_length, kv_offset = _preprocess_mask_arguments(config, input_embeds, attention_mask, cache_position, past_key_values, position_ids, layer_idx)\n",
      "    if early_exit:...\n",
      "    sliding_window = getattr(config, 'sliding_window', None)\n",
      "    if sliding_window is None:...\n",
      "    batch_size, dtype = (input_embeds.shape[0], input_embeds.dtype)\n",
      "    mask_factory_function = sliding_window_causal_mask_function(sliding_window)\n",
      "    mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n",
      "    allow_is_causal_skip = not getattr(past_key_values, 'is_compileable', False)\n",
      "    if or_mask_function is not None:...\n",
      "    if and_mask_function is not None:...\n",
      "    if packed_sequence_mask is not None and _is_torch_greater_or_equal_than_2_6:...\n",
      "    causal_mask = mask_interface(batch_size=batch_size, cache_position=cache_position, kv_length=kv_length, kv_offset=kv_offset, mask_function=mask_factory_function, attention_mask=attention_mask, allow_is_causal_skip=allow_is_causal_skip, local_size=sliding_window, dtype=dtype, config=config)\n",
      "    return causal_mask\n",
      "\n",
      "def create_chunked_causal_mask(config: PretrainedConfig, input_embeds: torch.Tensor, attention_mask: Optional[torch.Tensor], cache_position: torch.Tensor, past_key_values: Optional[Cache], position_ids: Optional[torch.Tensor]=None, or_mask_function: Optional[Callable]=None, and_mask_function: Optional[Callable]=None) -> Optional[Union[torch.Tensor, BlockMask]]:...\n",
      "LAYER_PATTERN_TO_MASK_FUNCTION_MAPPING = {'full_attention': create_causal_mask, 'sliding_attention': create_sliding_window_causal_mask, 'chunked_attention': create_chunked_causal_mask}\n",
      "\n",
      "def create_masks_for_generate(config: PretrainedConfig, input_embeds: torch.Tensor, attention_mask: Optional[torch.Tensor], cache_position: torch.Tensor, past_key_values: Optional[Cache], position_ids: Optional[torch.Tensor]=None, or_mask_function: Optional[Callable]=None, and_mask_function: Optional[Callable]=None, **kwargs):...\n",
      "GREEN = '\\x1b[92m'\n",
      "YELLOW = '\\x1b[93m'\n",
      "RESET = '\\x1b[0m'\n",
      "BLACK_SQUARE = ''\n",
      "WHITE_SQUARE = ''\n",
      "GREY_SQUARE = ''\n",
      "LOW_TRIANGLE = ''\n",
      "UPPER_TRIANGLE = ''\n",
      "\n",
      "def get_style(style):...\n",
      "YELLOW_SQUARE = f'{YELLOW}{BLACK_SQUARE}{RESET}'\n",
      "GREEN_SQUARE = f'{GREEN}{BLACK_SQUARE}{RESET}'\n",
      "\n",
      "def tensor_to_mask_visual(original_tensor: torch.Tensor, grid_size=(20, 40), style='majong') -> str:...\n",
      "\n",
      "class AttentionMask(torch.Tensor):\n",
      "\n",
      "    def __new__(cls, data, style=None):...\n",
      "\n",
      "    def __init__(self, data):...\n",
      "\n",
      "    def to_string(self, grid_size=(20, 40), limit=4):...\n",
      "\n",
      "    def __repr__(self):...\n",
      "\n",
      "    def __str__(self):...\n",
      "\n",
      "    @classmethod\n",
      "    def from_tensor(cls, tensor: torch.Tensor, style: Optional[str]=None) -> 'AttentionMask':...\n"
     ]
    }
   ],
   "source": [
    "print(sketches['/home/jw2858/miniconda3/envs/NKI/lib/python3.12/site-packages/transformers/masking_utils.py'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92af67bd-870d-471e-95ba-1d87ef932cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import copy\n",
      "import inspect\n",
      "import os\n",
      "import warnings\n",
      "from dataclasses import dataclass\n",
      "from typing import TYPE_CHECKING, Any, Callable, Optional, Union\n",
      "import torch\n",
      "import torch.distributed as dist\n",
      "from packaging import version\n",
      "from torch import nn\n",
      "from ..cache_utils import Cache, DynamicCache, EncoderDecoderCache, QuantizedCache, StaticCache\n",
      "from ..dynamic_module_utils import check_python_requirements, get_cached_module_file, get_class_in_module, resolve_trust_remote_code\n",
      "from ..integrations.deepspeed import is_deepspeed_zero3_enabled\n",
      "from ..integrations.fsdp import is_fsdp_managed_module\n",
      "from ..masking_utils import create_masks_for_generate\n",
      "from ..pytorch_utils import isin_mps_friendly\n",
      "from ..tokenization_utils import ExtensionsTrie\n",
      "from ..utils import ModelOutput, TransformersKwargs, is_accelerate_available, is_hqq_available, is_optimum_quanto_available, is_torchdynamo_exporting, logging\n",
      "from .candidate_generator import AssistantVocabTranslatorCache, AssistedCandidateGenerator, AssistedCandidateGeneratorDifferentTokenizers, CandidateGenerator, EarlyExitCandidateGenerator, PromptLookupCandidateGenerator, UniversalSpeculativeDecodingGenerator, _prepare_attention_mask, _prepare_token_type_ids\n",
      "from .configuration_utils import ALL_STATIC_CACHE_IMPLEMENTATIONS, DEPRECATED_STATIC_CACHE_IMPLEMENTATIONS, STATIC_CACHE_IMPLEMENTATIONS, GenerationConfig, GenerationMode\n",
      "from .continuous_batching import ContinuousMixin\n",
      "from .logits_process import EncoderNoRepeatNGramLogitsProcessor, EncoderRepetitionPenaltyLogitsProcessor, EpsilonLogitsWarper, EtaLogitsWarper, ExponentialDecayLengthPenalty, ForcedBOSTokenLogitsProcessor, ForcedEOSTokenLogitsProcessor, InfNanRemoveLogitsProcessor, LogitNormalization, LogitsProcessorList, MinLengthLogitsProcessor, MinNewTokensLengthLogitsProcessor, MinPLogitsWarper, NoBadWordsLogitsProcessor, NoRepeatNGramLogitsProcessor, PrefixConstrainedLogitsProcessor, RepetitionPenaltyLogitsProcessor, SequenceBiasLogitsProcessor, SuppressTokensAtBeginLogitsProcessor, SuppressTokensLogitsProcessor, TemperatureLogitsWarper, TopKLogitsWarper, TopPLogitsWarper, TypicalLogitsWarper, UnbatchedClassifierFreeGuidanceLogitsProcessor\n",
      "from .stopping_criteria import ConfidenceCriteria, EosTokenCriteria, MaxLengthCriteria, MaxTimeCriteria, StoppingCriteria, StoppingCriteriaList, StopStringCriteria\n",
      "if TYPE_CHECKING:...\n",
      "logger = logging.get_logger(__name__)\n",
      "if is_accelerate_available():...\n",
      "ALL_CACHE_NAMES = [...]\n",
      "GENERATION_MODES_MAPPING = {GenerationMode.SAMPLE: '_sample', GenerationMode.GREEDY_SEARCH: '_sample', GenerationMode.BEAM_SEARCH: '_beam_search', GenerationMode.BEAM_SAMPLE: '_beam_search', GenerationMode.ASSISTED_GENERATION: '_assisted_decoding', GenerationMode.DOLA_GENERATION: 'transformers-community/dola', GenerationMode.CONTRASTIVE_SEARCH: 'transformers-community/contrastive-search', GenerationMode.GROUP_BEAM_SEARCH: 'transformers-community/group-beam-search', GenerationMode.CONSTRAINED_BEAM_SEARCH: 'transformers-community/constrained-beam-search'}\n",
      "\n",
      "@dataclass\n",
      "class GenerateDecoderOnlyOutput(ModelOutput):\n",
      "    sequences: torch.LongTensor\n",
      "    scores: Optional[tuple[torch.FloatTensor]] = None\n",
      "    logits: Optional[tuple[torch.FloatTensor]] = None\n",
      "    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
      "    hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
      "    past_key_values: Optional[Cache] = None\n",
      "\n",
      "@dataclass\n",
      "class GenerateEncoderDecoderOutput(ModelOutput):\n",
      "    sequences: torch.LongTensor\n",
      "    scores: Optional[tuple[torch.FloatTensor]] = None\n",
      "    logits: Optional[tuple[torch.FloatTensor]] = None\n",
      "    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n",
      "    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n",
      "    decoder_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
      "    cross_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
      "    decoder_hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
      "    past_key_values: Optional[Cache] = None\n",
      "\n",
      "@dataclass\n",
      "class GenerateBeamDecoderOnlyOutput(ModelOutput):\n",
      "    sequences: torch.LongTensor\n",
      "    sequences_scores: Optional[torch.FloatTensor] = None\n",
      "    scores: Optional[tuple[torch.FloatTensor]] = None\n",
      "    logits: Optional[tuple[torch.FloatTensor]] = None\n",
      "    beam_indices: Optional[torch.LongTensor] = None\n",
      "    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
      "    hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
      "    past_key_values: Optional[Cache] = None\n",
      "\n",
      "@dataclass\n",
      "class GenerateBeamEncoderDecoderOutput(ModelOutput):\n",
      "    sequences: torch.LongTensor\n",
      "    sequences_scores: Optional[torch.FloatTensor] = None\n",
      "    scores: Optional[tuple[torch.FloatTensor]] = None\n",
      "    logits: Optional[tuple[torch.FloatTensor]] = None\n",
      "    beam_indices: Optional[torch.LongTensor] = None\n",
      "    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n",
      "    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n",
      "    decoder_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
      "    cross_attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
      "    decoder_hidden_states: Optional[tuple[tuple[torch.FloatTensor]]] = None\n",
      "    past_key_values: Optional[Cache] = None\n",
      "GreedySearchDecoderOnlyOutput = GenerateDecoderOnlyOutput\n",
      "ContrastiveSearchDecoderOnlyOutput = GenerateDecoderOnlyOutput\n",
      "SampleDecoderOnlyOutput = GenerateDecoderOnlyOutput\n",
      "ContrastiveSearchEncoderDecoderOutput = GenerateEncoderDecoderOutput\n",
      "GreedySearchEncoderDecoderOutput = GenerateEncoderDecoderOutput\n",
      "SampleEncoderDecoderOutput = GenerateEncoderDecoderOutput\n",
      "BeamSearchDecoderOnlyOutput = GenerateBeamDecoderOnlyOutput\n",
      "BeamSampleDecoderOnlyOutput = GenerateBeamDecoderOnlyOutput\n",
      "BeamSearchEncoderDecoderOutput = GenerateBeamEncoderDecoderOutput\n",
      "BeamSampleEncoderDecoderOutput = GenerateBeamEncoderDecoderOutput\n",
      "GreedySearchOutput = Union[GreedySearchEncoderDecoderOutput, GreedySearchDecoderOnlyOutput]\n",
      "SampleOutput = Union[SampleEncoderDecoderOutput, SampleDecoderOnlyOutput]\n",
      "BeamSearchOutput = Union[BeamSearchEncoderDecoderOutput, BeamSearchDecoderOnlyOutput]\n",
      "BeamSampleOutput = Union[BeamSampleEncoderDecoderOutput, BeamSampleDecoderOnlyOutput]\n",
      "ContrastiveSearchOutput = Union[ContrastiveSearchEncoderDecoderOutput, ContrastiveSearchDecoderOnlyOutput]\n",
      "GenerateNonBeamOutput = Union[GenerateDecoderOnlyOutput, GenerateEncoderDecoderOutput]\n",
      "GenerateBeamOutput = Union[GenerateBeamDecoderOnlyOutput, GenerateBeamEncoderDecoderOutput]\n",
      "GenerateOutput = Union[GenerateNonBeamOutput, GenerateBeamOutput]\n",
      "\n",
      "class GenerationMixin(ContinuousMixin):\n",
      "\n",
      "    def load_custom_generate(self, pretrained_model_name_or_path: Optional[Union[str, os.PathLike]]=None, trust_remote_code: Optional[bool]=None, **kwargs) -> Callable:...\n",
      "        try:\n",
      "            module = get_cached_module_file(pretrained_model_name_or_path, module_file='custom_generate/generate.py', **kwargs)\n",
      "        except OSError:\n",
      "            raise OSError(f\"`{pretrained_model_name_or_path}` does not contain a `custom_generate` subdirectory with a `generate.py` file, can't load the custom generate function.\")...\n",
      "\n",
      "    def _cache_dependant_input_preparation(self, input_ids: torch.LongTensor, inputs_embeds: Optional[torch.FloatTensor], cache_position: Optional[torch.LongTensor]) -> tuple[torch.FloatTensor, torch.LongTensor]:...\n",
      "        if is_torchdynamo_exporting():...\n",
      "        if inputs_embeds is not None and input_ids.shape[1] == 0:...\n",
      "        elif inputs_embeds is not None or cache_position[-1] >= input_ids.shape[1]:...\n",
      "        elif input_ids.shape[1] != cache_position.shape[0]:\n",
      "            input_ids = input_ids[:, cache_position]\n",
      "        return (inputs_embeds, input_ids)\n",
      "\n",
      "    def _cache_dependant_input_preparation_exporting(self, input_ids: torch.LongTensor, inputs_embeds: Optional[torch.FloatTensor], cache_position: Optional[torch.LongTensor]) -> tuple[torch.FloatTensor, torch.LongTensor]:...\n",
      "\n",
      "    def prepare_inputs_for_generation(self, input_ids: torch.LongTensor, past_key_values: Optional[Cache]=None, attention_mask: Optional[torch.LongTensor]=None, inputs_embeds: Optional[torch.FloatTensor]=None, cache_position: Optional[torch.LongTensor]=None, **kwargs):...\n",
      "        model_inputs = {}\n",
      "        model_inputs['cache_position'] = cache_position\n",
      "        if past_key_values is not None:\n",
      "            model_inputs['past_key_values'] = past_key_values\n",
      "            inputs_embeds, input_ids = self._cache_dependant_input_preparation(input_ids, inputs_embeds, cache_position)\n",
      "        input_ids_key = 'decoder_input_ids' if self.config.is_encoder_decoder else 'input_ids'\n",
      "        if not self.config.is_encoder_decoder:\n",
      "            if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:...\n",
      "            else:\n",
      "                model_inputs[input_ids_key] = input_ids.clone(memory_format=torch.contiguous_format)\n",
      "                model_inputs['inputs_embeds'] = None\n",
      "        else:...\n",
      "        encoder_attention_mask = attention_mask if self.config.is_encoder_decoder else None\n",
      "        attention_mask = kwargs.pop('decoder_attention_mask', None) if self.config.is_encoder_decoder else attention_mask\n",
      "        attention_mask_key = 'decoder_attention_mask' if self.config.is_encoder_decoder else 'attention_mask'\n",
      "        position_ids_key = 'decoder_position_ids' if self.config.is_encoder_decoder else 'position_ids'\n",
      "        if attention_mask is not None and kwargs.get(position_ids_key) is None and (position_ids_key in set(inspect.signature(self.forward).parameters.keys())):\n",
      "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
      "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
      "            kwargs[position_ids_key] = position_ids\n",
      "        for model_input_name in ['position_ids', 'token_type_ids', 'decoder_position_ids']:\n",
      "            model_input = kwargs.get(model_input_name)\n",
      "            if model_input is not None:\n",
      "                if past_key_values is not None:\n",
      "                    current_input_length = ... if model_inputs.get('inputs_embeds') is not None else model_inputs[input_ids_key].shape[1]\n",
      "                    model_input = model_input[:, -current_input_length:]\n",
      "                    model_input = model_input.clone(memory_format=torch.contiguous_format)\n",
      "                model_inputs[model_input_name] = model_input\n",
      "        if isinstance(past_key_values, Cache) and past_key_values.is_compileable and ...:...\n",
      "        if attention_mask is not None:\n",
      "            model_inputs[attention_mask_key] = attention_mask\n",
      "        if encoder_attention_mask is not None:...\n",
      "        for key, value in kwargs.items():\n",
      "            if key not in model_inputs:\n",
      "                model_inputs[key] = value\n",
      "        model_inputs.pop('labels', None)\n",
      "        return model_inputs\n",
      "\n",
      "    def _prepare_model_inputs(self, inputs: Optional[torch.Tensor]=None, bos_token_id: Optional[torch.Tensor]=None, model_kwargs: Optional[dict[str, torch.Tensor]]=None) -> tuple[torch.Tensor, Optional[str], dict[str, torch.Tensor]]:...\n",
      "        if self.config.is_encoder_decoder and ...:...\n",
      "        else:\n",
      "            input_name = self.main_input_name\n",
      "        model_kwargs = {k: v for k, v in model_kwargs.items() if v is not None or k != input_name}\n",
      "        inputs_kwarg = model_kwargs.pop(input_name, None)\n",
      "        if inputs_kwarg is not None and inputs is not None:...\n",
      "        elif inputs_kwarg is not None:\n",
      "            inputs = inputs_kwarg\n",
      "        if input_name == 'input_ids' and 'inputs_embeds' in model_kwargs:...\n",
      "        inputs = self._maybe_initialize_input_ids_for_generation(inputs, bos_token_id, model_kwargs)\n",
      "        return (inputs, input_name, model_kwargs)\n",
      "\n",
      "    def _maybe_initialize_input_ids_for_generation(self, inputs: Optional[torch.Tensor]=None, bos_token_id: Optional[torch.Tensor]=None, model_kwargs: Optional[dict[str, torch.Tensor]]=None) -> torch.LongTensor:...\n",
      "        if inputs is not None:\n",
      "            return inputs...\n",
      "\n",
      "    def _prepare_attention_mask_for_generation(self, inputs_tensor: torch.Tensor, generation_config: GenerationConfig, model_kwargs: dict[str, Any]) -> torch.LongTensor:...\n",
      "\n",
      "    def _prepare_encoder_decoder_kwargs_for_generation(self, inputs_tensor: torch.Tensor, model_kwargs, model_input_name: Optional[str], generation_config: GenerationConfig) -> dict[str, Any]:...\n",
      "\n",
      "    def _prepare_decoder_input_ids_for_generation(self, batch_size: int, model_input_name: str, model_kwargs: dict[str, torch.Tensor], decoder_start_token_id: torch.Tensor, device: Optional[torch.device]=None) -> tuple[torch.LongTensor, dict[str, torch.Tensor]]:...\n",
      "\n",
      "    @staticmethod\n",
      "    def _expand_inputs_for_generation(expand_size: int=1, is_encoder_decoder: bool=False, input_ids: Optional[torch.LongTensor]=None, **model_kwargs) -> tuple[torch.LongTensor, dict[str, Any]]:...\n",
      "        if expand_size == 1:\n",
      "            return (input_ids, model_kwargs)...\n",
      "\n",
      "    def _update_model_kwargs_for_generation(self, outputs: ModelOutput, model_kwargs: dict[str, Any], is_encoder_decoder: bool=False, num_new_tokens: int=1) -> dict[str, Any]:\n",
      "        for possible_cache_name in ALL_CACHE_NAMES:\n",
      "            if possible_cache_name in outputs:\n",
      "                if possible_cache_name in ('past_buckets_states', 'mems'):...\n",
      "                else:\n",
      "                    cache_name = possible_cache_name\n",
      "                model_kwargs[cache_name] = getattr(outputs, possible_cache_name)\n",
      "                break\n",
      "        if 'token_type_ids' in model_kwargs:...\n",
      "        if not is_encoder_decoder:\n",
      "            if 'attention_mask' in model_kwargs:\n",
      "                attention_mask = model_kwargs['attention_mask']\n",
      "                model_kwargs['attention_mask'] = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)\n",
      "        else:...\n",
      "        if model_kwargs.get('use_cache', True):\n",
      "            model_kwargs['cache_position'] = model_kwargs['cache_position'][-1:] + num_new_tokens\n",
      "        else:...\n",
      "        return model_kwargs\n",
      "\n",
      "    def _get_candidate_generator(self, generation_config: GenerationConfig, input_ids: torch.LongTensor, inputs_tensor: torch.Tensor, logits_processor: LogitsProcessorList, model_kwargs: dict[str, Any], assistant_model: Optional['PreTrainedModel']=None, target_tokenizer: Optional['PreTrainedTokenizerBase']=None, assistant_tokenizer: Optional['PreTrainedTokenizerBase']=None) -> CandidateGenerator:...\n",
      "\n",
      "    def _get_logits_processor(self, generation_config: GenerationConfig, input_ids_seq_length: Optional[int]=None, encoder_input_ids: Optional[torch.LongTensor]=None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]]=None, logits_processor: Optional[LogitsProcessorList]=None, device: Optional[str]=None, model_kwargs: Optional[dict[str, Any]]=None, negative_prompt_ids: Optional[torch.Tensor]=None, negative_prompt_attention_mask: Optional[torch.Tensor]=None) -> LogitsProcessorList:...\n",
      "        processors = LogitsProcessorList()\n",
      "        if logits_processor is None:...\n",
      "        if generation_config.guidance_scale is not None and generation_config.guidance_scale != 1:...\n",
      "        if generation_config.sequence_bias is not None:...\n",
      "        if generation_config.encoder_repetition_penalty is not None and generation_config.encoder_repetition_penalty != 1.0:...\n",
      "        if generation_config.repetition_penalty is not None and generation_config.repetition_penalty != 1.0:...\n",
      "        if generation_config.no_repeat_ngram_size is not None and generation_config.no_repeat_ngram_size > 0:...\n",
      "        if generation_config.encoder_no_repeat_ngram_size is not None and generation_config.encoder_no_repeat_ngram_size > 0:...\n",
      "        if generation_config.bad_words_ids is not None:...\n",
      "        if generation_config.min_length is not None and getattr(generation_config, '_eos_token_tensor', None) is not None and (generation_config.min_length > 0):...\n",
      "        if generation_config.min_new_tokens is not None and ...:...\n",
      "        if prefix_allowed_tokens_fn is not None:...\n",
      "        if generation_config.forced_bos_token_id is not None:...\n",
      "        if generation_config.forced_eos_token_id is not None:...\n",
      "        if generation_config.remove_invalid_values is True:...\n",
      "        if generation_config.exponential_decay_length_penalty is not None:...\n",
      "        if generation_config.suppress_tokens is not None:...\n",
      "        if generation_config.begin_suppress_tokens is not None:...\n",
      "        processors = self._merge_criteria_processor_list(processors, logits_processor)\n",
      "        if generation_config.do_sample:\n",
      "            if generation_config.num_beams > 1:...\n",
      "            else:\n",
      "                min_tokens_to_keep = 1\n",
      "            if generation_config.temperature is not None and generation_config.temperature != 1.0:...\n",
      "            if generation_config.top_k is not None and generation_config.top_k != 0:\n",
      "                processors.append(TopKLogitsWarper(top_k=generation_config.top_k, min_tokens_to_keep=min_tokens_to_keep))\n",
      "            if generation_config.top_p is not None and generation_config.top_p < 1.0:...\n",
      "            if generation_config.min_p is not None:...\n",
      "            if generation_config.typical_p is not None and generation_config.typical_p < 1.0:...\n",
      "            if generation_config.epsilon_cutoff is not None and 0.0 < generation_config.epsilon_cutoff < 1.0:...\n",
      "            if generation_config.eta_cutoff is not None and 0.0 < generation_config.eta_cutoff < 1.0:...\n",
      "        if generation_config.watermarking_config is not None:...\n",
      "        if generation_config.renormalize_logits is True:...\n",
      "        return processors\n",
      "\n",
      "    def _get_stopping_criteria(self, generation_config: GenerationConfig, stopping_criteria: Optional[StoppingCriteriaList], tokenizer: Optional['PreTrainedTokenizerBase']=None) -> StoppingCriteriaList:\n",
      "        criteria = StoppingCriteriaList()\n",
      "        if generation_config.max_length is not None:\n",
      "            max_position_embeddings = getattr(self.config, 'max_position_embeddings', None)\n",
      "            criteria.append(MaxLengthCriteria(max_length=generation_config.max_length, max_position_embeddings=max_position_embeddings))\n",
      "        if generation_config.max_time is not None:...\n",
      "        if generation_config.stop_strings is not None:...\n",
      "        if generation_config._eos_token_tensor is not None:\n",
      "            criteria.append(EosTokenCriteria(eos_token_id=generation_config._eos_token_tensor))\n",
      "        if generation_config.is_assistant and ...:...\n",
      "        criteria = self._merge_criteria_processor_list(criteria, stopping_criteria)\n",
      "        return criteria\n",
      "\n",
      "    def _merge_criteria_processor_list(self, default_list: Union[LogitsProcessorList, StoppingCriteriaList], custom_list: Union[LogitsProcessorList, StoppingCriteriaList]) -> Union[LogitsProcessorList, StoppingCriteriaList]:...\n",
      "        if len(custom_list) == 0:\n",
      "            return default_list...\n",
      "\n",
      "    def compute_transition_scores(self, sequences: torch.Tensor, scores: tuple[torch.Tensor], beam_indices: Optional[torch.Tensor]=None, normalize_logits: bool=False) -> torch.Tensor:...\n",
      "\n",
      "    def _validate_generation_mode(self, generation_mode, generation_config, generation_mode_kwargs):\n",
      "        if generation_mode == GenerationMode.BEAM_SEARCH and 'streamer' in generation_mode_kwargs:...\n",
      "        if generation_mode == GenerationMode.ASSISTED_GENERATION:...\n",
      "        if (assistant_model := generation_mode_kwargs.get('assistant_model')) is not None:...\n",
      "\n",
      "    def _validate_model_kwargs(self, model_kwargs: dict[str, Any]):...\n",
      "        if self.config.is_encoder_decoder:...\n",
      "        unused_model_args = []\n",
      "        model_args = set(inspect.signature(self.prepare_inputs_for_generation).parameters)\n",
      "        if 'kwargs' in model_args or 'model_kwargs' in model_args:\n",
      "            model_args |= set(inspect.signature(self.forward).parameters)\n",
      "        if self.config.is_encoder_decoder:...\n",
      "        for key, value in model_kwargs.items():\n",
      "            if value is not None and key not in model_args and (key not in TransformersKwargs.__optional_keys__):...\n",
      "        if unused_model_args:...\n",
      "\n",
      "    def _validate_generated_length(self, generation_config, input_ids_length, has_default_max_length):...\n",
      "        if has_default_max_length and generation_config.max_new_tokens is None and (generation_config.max_length == 20):...\n",
      "        if input_ids_length >= generation_config.max_length:...\n",
      "        min_length_error_suffix = ' Generation will stop at the defined maximum length. You should decrease the minimum length and/or increase the maximum length.'\n",
      "        if has_default_max_length:\n",
      "            min_length_error_suffix += f' Note that `max_length` is set to {generation_config.max_length}, its default value.'\n",
      "        if generation_config.min_length is not None and generation_config.min_length > generation_config.max_length:...\n",
      "        if generation_config.min_new_tokens is not None:...\n",
      "\n",
      "    def _prepare_generated_length(self, generation_config, has_default_max_length, has_default_min_length, model_input_name, input_ids_length, inputs_tensor):...\n",
      "        if generation_config.max_new_tokens is not None:...\n",
      "        elif model_input_name == 'inputs_embeds' and ...:...\n",
      "        elif has_default_max_length:\n",
      "            if generation_config.max_length == GenerationConfig().max_length:\n",
      "                generation_config.max_length = generation_config.max_length + input_ids_length\n",
      "                max_position_embeddings = getattr(self.config, 'max_position_embeddings', None)\n",
      "                if max_position_embeddings is not None:\n",
      "                    generation_config.max_length = min(generation_config.max_length, max_position_embeddings)\n",
      "        if generation_config.min_new_tokens is not None:...\n",
      "        elif model_input_name == 'inputs_embeds' and ...:...\n",
      "        return generation_config\n",
      "\n",
      "    def _prepare_generation_config(self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool]=None, **kwargs: Any) -> tuple[GenerationConfig, dict]:...\n",
      "        using_model_generation_config = False\n",
      "        if generation_config is None:...\n",
      "        generation_config = copy.deepcopy(generation_config)\n",
      "        if not using_model_generation_config:\n",
      "            model_base_version = version.parse(version.parse(self.generation_config.transformers_version).base_version)\n",
      "            if use_model_defaults is True or (use_model_defaults is None and model_base_version >= version.parse('4.50.0')):\n",
      "                modified_values = {}\n",
      "                global_default_generation_config = GenerationConfig()\n",
      "                model_generation_config = self.generation_config\n",
      "                for key, model_gen_config_value in model_generation_config.__dict__.items():\n",
      "                    if key.startswith('_') or key == 'transformers_version':\n",
      "                        continue\n",
      "                    if key == 'cache_implementation' and model_generation_config.cache_implementation == 'hybrid':...\n",
      "                    global_default_value = getattr(global_default_generation_config, key, None)\n",
      "                    custom_gen_config_value = getattr(generation_config, key, None)\n",
      "                    if custom_gen_config_value == global_default_value and model_gen_config_value != global_default_value:...\n",
      "                if generation_config.temperature == 0.0:...\n",
      "                if use_model_defaults is None and len(modified_values) > 0:...\n",
      "            else:...\n",
      "        model_kwargs = generation_config.update(**kwargs)\n",
      "        output_attentions = generation_config.output_attentions\n",
      "        output_hidden_states = generation_config.output_hidden_states\n",
      "        model_kwargs.update({'output_attentions': output_attentions} if output_attentions else {})\n",
      "        model_kwargs.update({'output_hidden_states': output_hidden_states} if output_hidden_states else {})\n",
      "        return (generation_config, model_kwargs)\n",
      "\n",
      "    def _get_initial_cache_position(self, seq_length, device, model_kwargs):...\n",
      "        if 'cache_position' in model_kwargs and model_kwargs['cache_position'] is not None:...\n",
      "        if 'inputs_embeds' in model_kwargs and (not self.config.is_encoder_decoder):...\n",
      "        elif 'decoder_inputs_embeds' in model_kwargs and self.config.is_encoder_decoder:...\n",
      "        else:\n",
      "            cache_position = torch.ones(seq_length, dtype=torch.int64, device=device).cumsum(0) - 1\n",
      "        past_length = 0\n",
      "        if model_kwargs.get('past_key_values') is not None:\n",
      "            cache = model_kwargs['past_key_values']\n",
      "            past_length = 0\n",
      "            if isinstance(cache, tuple):...\n",
      "            elif hasattr(cache, 'get_seq_length'):\n",
      "                past_length = cache.get_seq_length()\n",
      "            cache_position = cache_position[past_length:]\n",
      "        model_kwargs['cache_position'] = cache_position\n",
      "        return model_kwargs\n",
      "\n",
      "    def _get_cache(self, cache_implementation: str, batch_size: int, max_cache_len: int, model_kwargs) -> Cache:...\n",
      "\n",
      "    @classmethod\n",
      "    def _supports_default_dynamic_cache(cls) -> bool:...\n",
      "        return not cls._is_stateful and all((special_model_name not in cls.__name__.lower() for special_model_name in [...]))\n",
      "\n",
      "    def _prepare_cache_for_generation(self, generation_config: GenerationConfig, model_kwargs: dict, generation_mode: GenerationMode, batch_size: int, max_cache_length: int) -> bool:...\n",
      "        is_hybrid_cache = any((class_name in self.__class__.__name__.lower() for class_name in ['mamba', 'falconh1']))\n",
      "        cache_name = 'past_key_values' if not is_hybrid_cache else 'cache_params'\n",
      "        requires_cross_attention_cache = self.config.is_encoder_decoder or model_kwargs.get('encoder_outputs') is not None\n",
      "        user_defined_cache = model_kwargs.get(cache_name)\n",
      "        if user_defined_cache is not None:...\n",
      "        if generation_config.use_cache is False:...\n",
      "        if not self._supports_default_dynamic_cache():...\n",
      "        if generation_mode == GenerationMode.ASSISTED_GENERATION and ...:...\n",
      "        if generation_mode in (GenerationMode.ASSISTED_GENERATION, GenerationMode.CONTRASTIVE_SEARCH) or generation_config.cache_implementation == 'dynamic_full':...\n",
      "        else:\n",
      "            dynamic_cache_kwargs = {'config': self.config.get_text_config(decoder=True)}\n",
      "        if generation_config.cache_implementation is not None:...\n",
      "        else:\n",
      "            model_kwargs[cache_name] = DynamicCache(**dynamic_cache_kwargs)\n",
      "        if requires_cross_attention_cache and (not isinstance(model_kwargs[cache_name], EncoderDecoderCache)):...\n",
      "\n",
      "    def _supports_logits_to_keep(self) -> bool:...\n",
      "        return 'logits_to_keep' in set(inspect.signature(self.forward).parameters.keys())\n",
      "\n",
      "    def _prepare_special_tokens(self, generation_config: GenerationConfig, kwargs_has_attention_mask: Optional[bool]=None, device: Optional[Union[torch.device, str]]=None):...\n",
      "\n",
      "        def _tensor_or_none(token, device=None):\n",
      "            if token is None:\n",
      "                return token\n",
      "            device = device if device is not None else self.device\n",
      "            if isinstance(token, torch.Tensor):...\n",
      "            return torch.tensor(token, device=device, dtype=torch.long)\n",
      "        bos_token_tensor = _tensor_or_none(generation_config.bos_token_id, device=device)\n",
      "        eos_token_tensor = _tensor_or_none(generation_config.eos_token_id, device=device)\n",
      "        pad_token_tensor = _tensor_or_none(generation_config.pad_token_id, device=device)\n",
      "        decoder_start_token_tensor = _tensor_or_none(generation_config.decoder_start_token_id, device=device)\n",
      "        if self.config.is_encoder_decoder:...\n",
      "        if eos_token_tensor is not None and eos_token_tensor.ndim == 0:...\n",
      "        if pad_token_tensor is None and eos_token_tensor is not None:...\n",
      "        if self.config.is_encoder_decoder and decoder_start_token_tensor is None:...\n",
      "        if eos_token_tensor is not None and isin_mps_friendly(elements=eos_token_tensor, test_elements=pad_token_tensor).any():\n",
      "            if kwargs_has_attention_mask is not None and (not kwargs_has_attention_mask):...\n",
      "        if eos_token_tensor is not None and (torch.is_floating_point(eos_token_tensor) or (eos_token_tensor < 0).any()):...\n",
      "        generation_config._bos_token_tensor = bos_token_tensor\n",
      "        generation_config._eos_token_tensor = eos_token_tensor\n",
      "        generation_config._pad_token_tensor = pad_token_tensor\n",
      "        generation_config._decoder_start_token_tensor = decoder_start_token_tensor\n",
      "\n",
      "    def _valid_auto_compile_criteria(self, model_kwargs: dict[str, Any], generation_config: GenerationConfig) -> bool:...\n",
      "        if generation_config.disable_compile:...\n",
      "        valid_hardware = self.device.type == 'cuda' or bool(generation_config.compile_config is not None and generation_config.compile_config._compile_all_devices)\n",
      "        using_compilable_cache = isinstance(model_kwargs.get('past_key_values'), Cache) and model_kwargs['past_key_values'].is_compileable\n",
      "        can_compile = valid_hardware and using_compilable_cache\n",
      "        if getattr(self, 'hf_quantizer', None) is not None:...\n",
      "        if hasattr(self, 'hf_device_map'):...\n",
      "        if generation_config.compile_config is not None and (not can_compile):...\n",
      "        return can_compile\n",
      "\n",
      "    def _get_deprecated_gen_repo(self, generation_mode: GenerationMode, trust_remote_code: bool, custom_generate: Optional[str]=None) -> Optional[str]:...\n",
      "        if custom_generate is not None or '/' not in (repo := GENERATION_MODES_MAPPING[generation_mode]):\n",
      "            return None...\n",
      "\n",
      "    def _extract_generation_mode_kwargs(self, custom_generate, kwargs, synced_gpus, assistant_model, streamer) -> dict[str, Any]:...\n",
      "        generation_mode_kwargs = {'tokenizer': kwargs.pop('tokenizer', None), 'assistant_tokenizer': kwargs.pop('assistant_tokenizer', None), 'assistant_model': assistant_model, 'streamer': streamer}\n",
      "        generation_mode_kwargs['synced_gpus'] = (is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)) and dist.get_world_size() > 1 if synced_gpus is None else ...\n",
      "        generation_mode_kwargs = {k: v for k, v in generation_mode_kwargs.items() if v is not None}\n",
      "        if isinstance(custom_generate, Callable):...\n",
      "        return generation_mode_kwargs\n",
      "\n",
      "    @torch.no_grad()\n",
      "    def generate(self, inputs: Optional[torch.Tensor]=None, generation_config: Optional[GenerationConfig]=None, logits_processor: Optional[LogitsProcessorList]=None, stopping_criteria: Optional[StoppingCriteriaList]=None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]]=None, synced_gpus: Optional[bool]=None, assistant_model: Optional['PreTrainedModel']=None, streamer: Optional['BaseStreamer']=None, negative_prompt_ids: Optional[torch.Tensor]=None, negative_prompt_attention_mask: Optional[torch.Tensor]=None, use_model_defaults: Optional[bool]=None, custom_generate: Optional[Union[str, Callable]]=None, **kwargs) -> Union[GenerateOutput, torch.LongTensor]:...\n",
      "        trust_remote_code = kwargs.pop('trust_remote_code', None)\n",
      "        if custom_generate is not None and isinstance(custom_generate, str):...\n",
      "        generation_mode_kwargs = self._extract_generation_mode_kwargs(custom_generate, kwargs, synced_gpus, assistant_model, streamer)\n",
      "        generation_config, model_kwargs = self._prepare_generation_config(generation_config, use_model_defaults, **kwargs)\n",
      "        generation_mode = generation_config.get_generation_mode(assistant_model)\n",
      "        if isinstance(custom_generate, Callable):...\n",
      "        else:\n",
      "            decoding_method = getattr(type(self), GENERATION_MODES_MAPPING[generation_mode])\n",
      "        self._validate_model_kwargs(model_kwargs.copy())\n",
      "        self._validate_generation_mode(generation_mode, generation_config, generation_mode_kwargs)\n",
      "        if (deprecated_mode_repo := self._get_deprecated_gen_repo(generation_mode, trust_remote_code, custom_generate)):...\n",
      "        logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n",
      "        stopping_criteria = stopping_criteria if stopping_criteria is not None else StoppingCriteriaList()\n",
      "        accepts_attention_mask = 'attention_mask' in set(inspect.signature(self.forward).parameters.keys())\n",
      "        requires_attention_mask = 'encoder_outputs' not in model_kwargs\n",
      "        kwargs_has_attention_mask = model_kwargs.get('attention_mask', None) is not None\n",
      "        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(inputs, generation_config.bos_token_id, model_kwargs)\n",
      "        if 'inputs_tensor' in inspect.signature(decoding_method).parameters.keys():...\n",
      "        batch_size = inputs_tensor.shape[0]\n",
      "        device = inputs_tensor.device\n",
      "        self._prepare_special_tokens(generation_config, kwargs_has_attention_mask, device=device)\n",
      "        if not self.config.is_encoder_decoder:\n",
      "            if generation_config._pad_token_tensor is not None and batch_size > 1 and ...:...\n",
      "        if not self.config.is_encoder_decoder and model_input_name == 'inputs_embeds':...\n",
      "        if not kwargs_has_attention_mask and requires_attention_mask and accepts_attention_mask:...\n",
      "        elif kwargs_has_attention_mask:\n",
      "            if model_input_name == 'input_ids' and len(model_kwargs['attention_mask'].shape) > 2:...\n",
      "        if self.config.is_encoder_decoder and 'encoder_outputs' not in model_kwargs:...\n",
      "        if self.config.is_encoder_decoder:...\n",
      "        else:\n",
      "            input_ids = inputs_tensor if model_input_name == 'input_ids' else model_kwargs.pop('input_ids')\n",
      "        input_ids, model_kwargs = self._expand_inputs_for_generation(input_ids=input_ids, expand_size=max(generation_config.num_beams, generation_config.num_return_sequences), is_encoder_decoder=self.config.is_encoder_decoder, **model_kwargs)\n",
      "        if generation_config.token_healing:...\n",
      "        if streamer is not None:...\n",
      "        input_ids_length = input_ids.shape[1]\n",
      "        has_default_max_length = kwargs.get('max_length') is None and generation_config.max_length is not None\n",
      "        has_default_min_length = kwargs.get('min_length') is None and generation_config.min_length is not None\n",
      "        generation_config = self._prepare_generated_length(generation_config=generation_config, has_default_max_length=has_default_max_length, has_default_min_length=has_default_min_length, model_input_name=model_input_name, inputs_tensor=inputs_tensor, input_ids_length=input_ids_length)\n",
      "        if self._supports_logits_to_keep() and 'logits_to_keep' not in model_kwargs:\n",
      "            model_kwargs['logits_to_keep'] = 1\n",
      "        self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n",
      "        max_cache_length = generation_config.max_length - 1\n",
      "        if inputs_tensor.shape[1] != input_ids_length and ...:...\n",
      "        self._prepare_cache_for_generation(generation_config, model_kwargs, generation_mode, batch_size, max_cache_length)\n",
      "        if self.device.type != input_ids.device.type:...\n",
      "        prepared_logits_processor = self._get_logits_processor(generation_config=generation_config, input_ids_seq_length=input_ids_length, encoder_input_ids=inputs_tensor, prefix_allowed_tokens_fn=prefix_allowed_tokens_fn, logits_processor=logits_processor, device=inputs_tensor.device, model_kwargs=model_kwargs, negative_prompt_ids=negative_prompt_ids, negative_prompt_attention_mask=negative_prompt_attention_mask)\n",
      "        prepared_stopping_criteria = self._get_stopping_criteria(generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=generation_mode_kwargs.get('tokenizer'))\n",
      "        model_kwargs['use_cache'] = generation_config.use_cache\n",
      "        result = decoding_method(self, input_ids, logits_processor=prepared_logits_processor, stopping_criteria=prepared_stopping_criteria, generation_config=generation_config, **generation_mode_kwargs, **model_kwargs)\n",
      "        if generation_config.return_legacy_cache is True and ...:...\n",
      "        return result\n",
      "\n",
      "    def _has_unfinished_sequences(self, this_peer_finished: bool, synced_gpus: bool, device: torch.device) -> bool:...\n",
      "        if synced_gpus:...\n",
      "        elif this_peer_finished:\n",
      "            return False\n",
      "        return True\n",
      "\n",
      "    def heal_tokens(self, input_ids: torch.LongTensor, tokenizer: Optional['PreTrainedTokenizerBase']=None) -> torch.LongTensor:...\n",
      "\n",
      "    def _sample(self, input_ids: torch.LongTensor, logits_processor: LogitsProcessorList, stopping_criteria: StoppingCriteriaList, generation_config: GenerationConfig, synced_gpus: bool=False, streamer: Optional['BaseStreamer']=None, **model_kwargs) -> Union[GenerateNonBeamOutput, torch.LongTensor]:...\n",
      "        pad_token_id = generation_config._pad_token_tensor\n",
      "        output_attentions = generation_config.output_attentions\n",
      "        output_hidden_states = generation_config.output_hidden_states\n",
      "        output_scores = generation_config.output_scores\n",
      "        output_logits = generation_config.output_logits\n",
      "        return_dict_in_generate = generation_config.return_dict_in_generate\n",
      "        has_eos_stopping_criteria = any((hasattr(criteria, 'eos_token_id') for criteria in stopping_criteria))\n",
      "        do_sample = generation_config.do_sample\n",
      "        scores = () if return_dict_in_generate and output_scores else None\n",
      "        raw_logits = () if return_dict_in_generate and output_logits else None\n",
      "        decoder_attentions = () if return_dict_in_generate and output_attentions else None\n",
      "        cross_attentions = () if return_dict_in_generate and output_attentions else None\n",
      "        decoder_hidden_states = () if return_dict_in_generate and output_hidden_states else None\n",
      "        if return_dict_in_generate and self.config.is_encoder_decoder:...\n",
      "        batch_size, cur_len = input_ids.shape[:2]\n",
      "        this_peer_finished = False\n",
      "        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n",
      "        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n",
      "        model_forward = self.__call__\n",
      "        compile_forward = self._valid_auto_compile_criteria(model_kwargs, generation_config)\n",
      "        if compile_forward:...\n",
      "        if generation_config.prefill_chunk_size is not None:...\n",
      "        else:\n",
      "            is_prefill = True\n",
      "        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n",
      "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
      "            if is_prefill:\n",
      "                outputs = self(**model_inputs, return_dict=True)\n",
      "                is_prefill = False\n",
      "            else:\n",
      "                outputs = model_forward(**model_inputs, return_dict=True)\n",
      "            model_kwargs = self._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder)\n",
      "            if synced_gpus and this_peer_finished:...\n",
      "            next_token_logits = outputs.logits[:, -1, :].to(copy=True, dtype=torch.float32, device=input_ids.device)\n",
      "            next_token_scores = logits_processor(input_ids, next_token_logits)\n",
      "            if return_dict_in_generate:...\n",
      "            if do_sample:\n",
      "                probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
      "                next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
      "            else:...\n",
      "            if has_eos_stopping_criteria:\n",
      "                next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
      "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
      "            if streamer is not None:...\n",
      "            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n",
      "            this_peer_finished = unfinished_sequences.max() == 0\n",
      "            cur_len += 1\n",
      "            del outputs\n",
      "        if streamer is not None:...\n",
      "        if return_dict_in_generate:...\n",
      "        else:\n",
      "            return input_ids\n",
      "\n",
      "    @staticmethod\n",
      "    def _flatten_beam_dim(tensor: torch.Tensor) -> torch.Tensor:...\n",
      "\n",
      "    @staticmethod\n",
      "    def _unflatten_beam_dim(tensor: torch.Tensor, batch_size: int, num_beams: int) -> torch.Tensor:...\n",
      "\n",
      "    @staticmethod\n",
      "    def _gather_beams(tensor: torch.Tensor, beam_indices: torch.Tensor) -> torch.Tensor:...\n",
      "\n",
      "    @staticmethod\n",
      "    def _check_early_stop_heuristic(is_early_stop_heuristic_unsatisfied: torch.Tensor, running_beam_scores: torch.Tensor, beam_scores: torch.Tensor, is_sent_finished: torch.Tensor, cur_len: int, max_length: int, decoder_prompt_len: int, early_stopping: Union[bool, str], length_penalty: float):...\n",
      "\n",
      "    @staticmethod\n",
      "    def _beam_search_has_unfinished_sequences(is_early_stop_heuristic_unsatisfied: torch.Tensor, is_sent_finished: torch.Tensor, next_token_hits_stopping_criteria: torch.Tensor, early_stopping: Union[bool, str]):...\n",
      "\n",
      "    def _get_top_k_continuations(self, accumulated_log_probs: torch.Tensor, running_sequences: torch.Tensor, running_beam_indices: torch.Tensor, cur_len: int, decoder_prompt_len: int, do_sample: bool, beams_to_keep: int, num_beams: int, vocab_size: int, batch_size: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:...\n",
      "\n",
      "    def _get_running_beams_for_next_iteration(self, topk_log_probs: torch.Tensor, topk_running_sequences: torch.Tensor, topk_running_beam_indices: torch.Tensor, next_token_hits_stopping_criteria: torch.Tensor, num_beams: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:...\n",
      "\n",
      "    def _update_finished_beams(self, sequences: torch.Tensor, topk_running_sequences: torch.Tensor, beam_scores: torch.Tensor, topk_log_probs: torch.Tensor, beam_indices: torch.Tensor, topk_running_beam_indices: torch.Tensor, is_early_stop_heuristic_unsatisfied: torch.Tensor, is_sent_finished: torch.Tensor, next_token_hits_stopping_criteria: torch.Tensor, top_num_beam_mask: torch.Tensor, num_beams: int, cur_len: int, decoder_prompt_len: int, length_penalty: float, early_stopping: Union[bool, str]) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:...\n",
      "\n",
      "    def _beam_search(self, input_ids: torch.LongTensor, logits_processor: LogitsProcessorList, stopping_criteria: StoppingCriteriaList, generation_config: GenerationConfig, synced_gpus: bool=False, **model_kwargs) -> Union[GenerateBeamOutput, torch.LongTensor]:...\n",
      "\n",
      "    def _assisted_decoding(self, input_ids: torch.LongTensor, logits_processor: LogitsProcessorList, stopping_criteria: StoppingCriteriaList, generation_config: GenerationConfig, synced_gpus: bool=False, streamer: Optional['BaseStreamer']=None, inputs_tensor: Optional[torch.FloatTensor]=None, assistant_model: Optional['PreTrainedModel']=None, assistant_tokenizer: Optional['PreTrainedTokenizerBase']=None, tokenizer: Optional['PreTrainedTokenizerBase']=None, **model_kwargs) -> Union[GenerateNonBeamOutput, torch.LongTensor]:...\n",
      "\n",
      "    def _prefill_chunking(self, input_ids: torch.LongTensor, generation_config: GenerationConfig, **model_kwargs):...\n",
      "\n",
      "def _speculative_sampling(candidate_input_ids, candidate_logits, candidate_length, new_logits, is_done_candidate):...\n",
      "\n",
      "def _split_model_outputs(outputs, new_outputs, cur_len, added_len, is_decoder_attention=False):...\n"
     ]
    }
   ],
   "source": [
    "print(sketches['/home/jw2858/miniconda3/envs/NKI/lib/python3.12/site-packages/transformers/generation/utils.py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28806829-315a-4e41-9145-912fc3dcbb1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
