{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb0905f-0f1b-45c3-947d-00b17e3daf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class GptOssTopKRouter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(32, 2880))\n",
    "        self.bias = nn.Parameter(torch.empty(32))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = hidden_states.reshape(-1, 2880)\n",
    "        router_logits = F.linear(hidden_states, self.weight, self.bias)  # (seq_len, num_experts)\n",
    "        router_top_value, router_indices = torch.topk(router_logits, 4, dim=-1)  # (seq_len, top_k)\n",
    "        router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n",
    "        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n",
    "        return router_scores, router_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74709f98-1c0a-492b-9232-7b9084f0f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "router = GptOssTopKRouter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3d0ff5d-8902-4e9c-82db-d08e8596b94a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router.load_state_dict(torch.load('model.layers.23.mlp.router.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ab569c-1011-4924-86e3-95a909c26a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.layers.23.mlp.router.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42a56ac5-2b37-4ef1-b7e9-8a21433df242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hidden_states', 'return'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3fdd2d9-14c3-4651-a039-501523377cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = torch.Tensor(parameters['hidden_states']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a03a2de-2c23-4ece-8f1d-e8a228eaa190",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_scores, router_indices = torch.Tensor(parameters['return']['items'][0]['data']), torch.LongTensor(parameters['return']['items'][1]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abb5b64b-89de-4031-b0ab-267d5ba86b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_router_scores, actual_router_indices = router(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd5c86f3-0b52-4c9b-88e6-dea125381e77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_scores.allclose(actual_router_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65f0184a-d73b-4026-a8ef-260d90051a41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router_indices.allclose(actual_router_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40fac0-cea4-4fad-8d53-4c38b7d52be1",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "169f474a-6629-4449-8f1f-ca73ca05537f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssRMSNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(2880))\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        input_dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(torch.float32)\n",
    "        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "        hidden_states = hidden_states * torch.rsqrt(variance + 1e-05)\n",
    "        return (self.weight * hidden_states).to(input_dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98c7bd05-834f-44a9-bd7f-6c071aaa8874",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_attention_layernorm = GptOssRMSNorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f2c0a35-b3e7-4fe5-adc8-5766bad5d539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_attention_layernorm.load_state_dict(torch.load('model.layers.7.post_attention_layernorm.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccdaa5be-1f5c-4e16-ace2-dd21f693f315",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.layers.7.post_attention_layernorm.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "128c49ef-b139-4365-9da5-afbea68fc94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = torch.Tensor(parameters['hidden_states']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "45564f68-3666-481f-80f6-f9ad79408956",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = torch.Tensor(parameters['return']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbf8f2ef-69f3-4026-a161-5ef8ce5c86bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ret = post_attention_layernorm(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe1756d0-5181-40de-97f8-265f39e459e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.allclose(actual_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3efb67-3784-496b-8fef-02c3818f751e",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a11a7a93-9201-421b-940d-17bd98edd450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_yarn_parameters():\n",
    "    base = 150000\n",
    "    partial_rotary_factor = 1.0\n",
    "    head_dim = 64\n",
    "    dim = int(head_dim * partial_rotary_factor)\n",
    "    factor = 32.0\n",
    "    attention_factor = None\n",
    "    mscale = None\n",
    "    mscale_all_dim = None\n",
    "    original_max_position_embeddings = 4096\n",
    "\n",
    "    def get_mscale(scale, mscale=1):\n",
    "        return 0.1 * mscale * math.log(scale) + 1.0\n",
    "\n",
    "    if attention_factor is None:\n",
    "        attention_factor = get_mscale(factor)\n",
    "            \n",
    "    beta_fast = 32.0\n",
    "    beta_slow = 1.0\n",
    "\n",
    "    def find_correction_dim(num_rotations, dim, base, max_position_embeddings):\n",
    "        return dim * math.log(max_position_embeddings / (num_rotations * 2 * math.pi)) / (2 * math.log(base))\n",
    "\n",
    "    def find_correction_range(low_rot, high_rot, dim, base, max_position_embeddings, truncate):\n",
    "        low = find_correction_dim(low_rot, dim, base, max_position_embeddings)\n",
    "        high = find_correction_dim(high_rot, dim, base, max_position_embeddings)\n",
    "        return (max(low, 0), min(high, dim - 1))\n",
    "\n",
    "    def linear_ramp_factor(min, max, dim):\n",
    "        linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n",
    "        ramp_func = torch.clamp(linear_func, 0, 1)\n",
    "        return ramp_func\n",
    "    \n",
    "    pos_freqs = base ** (torch.arange(0, dim, 2).to(dtype=torch.float) / dim)\n",
    "    inv_freq_extrapolation = 1.0 / pos_freqs\n",
    "    inv_freq_interpolation = 1.0 / (factor * pos_freqs)\n",
    "    truncate = False\n",
    "    low, high = find_correction_range(beta_fast, beta_slow, dim, base, original_max_position_embeddings, truncate)\n",
    "    inv_freq_extrapolation_factor = 1 - linear_ramp_factor(low, high, dim // 2).to(dtype=torch.float)\n",
    "    inv_freq = inv_freq_interpolation * (1 - inv_freq_extrapolation_factor) + inv_freq_extrapolation * inv_freq_extrapolation_factor\n",
    "    return (inv_freq, attention_factor)\n",
    "\n",
    "\n",
    "class GptOssRotaryEmbedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        inv_freq, self.attention_scaling = _compute_yarn_parameters()\n",
    "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
    "        self.original_inv_freq = self.inv_freq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def forward(self, x, position_ids):\n",
    "        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n",
    "        position_ids_expanded = position_ids[:, None, :].float()\n",
    "        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != 'mps' else 'cpu'\n",
    "        with torch.autocast(device_type=device_type, enabled=False):\n",
    "            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "            emb = freqs\n",
    "            cos = emb.cos() * self.attention_scaling\n",
    "            sin = emb.sin() * self.attention_scaling\n",
    "        return (cos.to(x.dtype), sin.to(x.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c1ad588-20a0-42ef-8e6b-adf3ba0d1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rotary_embedding = GptOssRotaryEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fef52fe-9a0a-4cb2-9e5b-9ea22219a0be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rotary_embedding.load_state_dict(torch.load('model.rotary_emb.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "229ca646-de69-4257-ac92-e5250c56f679",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.rotary_emb.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0fd6c4e-ccd7-4abe-8923-af2be0be9787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x', 'position_ids', 'return'])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a6f917ad-cda9-4ce8-ac68-fefc38504ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(parameters['x']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2bcdcdd9-60c8-431b-b868-c9ca4870200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_ids = torch.Tensor(parameters['position_ids']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f00c0895-d26f-4e59-86fc-2481a444e192",
   "metadata": {},
   "outputs": [],
   "source": [
    "return_0, return_1 = torch.Tensor(parameters['return']['items'][0]['data']), torch.Tensor(parameters['return']['items'][1]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2919e1bf-7c5b-4a66-a103-920b00d5acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_return_0, actual_return_1 = rotary_embedding(x, position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67d9e218-299d-463e-b2b3-006fd718eaba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_0.allclose(actual_return_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d2bfe61-2375-4be5-b1bc-e0ae24b78588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_1.allclose(actual_return_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80318b3c-7751-48c4-9ed3-2c67f80a604a",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc843071-a595-4264-9d9b-d23614963ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.layers.8.mlp.experts.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75a1d1d8-856a-468f-97eb-e8e747cc65d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hidden_states', 'router_indices', 'routing_weights', 'return'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34bfa9a3-3831-40a6-8c83-9fed481d68cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_states = torch.Tensor(parameters['hidden_states']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8b3f2f95-f068-4137-9f9c-89350d04b9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "router_indices = torch.LongTensor(parameters['router_indices']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "485912b8-6c57-4177-872d-1458f118e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "routing_weights = torch.Tensor(parameters['routing_weights']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b109fea-5d14-4c67-b982-faa66b468e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = torch.Tensor(parameters['return']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7478f090-3e13-4104-9a80-5fc21deed4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssExperts(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gate_up_proj = nn.Parameter(torch.empty(32, 2880, 2 * 2880))\n",
    "        self.gate_up_proj_bias = nn.Parameter(torch.empty(32, 2 * 2880))\n",
    "        self.down_proj = nn.Parameter(torch.empty((32, 2880, 2880)))\n",
    "        self.down_proj_bias = nn.Parameter(torch.empty(32, 2880))\n",
    "        self.alpha = 1.702\n",
    "        self.limit = 7.0\n",
    "\n",
    "    def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weights=None) -> torch.Tensor:\n",
    "        batch_size = hidden_states.shape[0]\n",
    "        hidden_states = hidden_states.reshape(-1, 2880)\n",
    "        num_experts = routing_weights.shape[1]\n",
    "\n",
    "        next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n",
    "        with torch.no_grad():\n",
    "            expert_mask = torch.nn.functional.one_hot(router_indices, num_classes=num_experts + 1)\n",
    "            expert_mask = expert_mask.permute(2, 1, 0)\n",
    "            expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n",
    "        for expert_idx in expert_hit[:]:\n",
    "            expert_idx = expert_idx[0]\n",
    "            with torch.no_grad():\n",
    "                _, token_idx = torch.where(expert_mask[expert_idx])\n",
    "            current_state = hidden_states[token_idx]\n",
    "            gate_up = current_state @ self.gate_up_proj[expert_idx] + self.gate_up_proj_bias[expert_idx]\n",
    "            gate, up = (gate_up[..., ::2], gate_up[..., 1::2])\n",
    "            gate = gate.clamp(min=None, max=self.limit)\n",
    "            up = up.clamp(min=-self.limit, max=self.limit)\n",
    "            glu = gate * torch.sigmoid(gate * self.alpha)\n",
    "            gated_output = (up + 1) * glu\n",
    "            out = gated_output @ self.down_proj[expert_idx] + self.down_proj_bias[expert_idx]\n",
    "            weighted_output = out * routing_weights[token_idx, expert_idx, None]\n",
    "            next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n",
    "        next_states = next_states.view(batch_size, -1, 2880)\n",
    "\n",
    "        return next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25fd5713-817a-4b16-960e-691774efed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "experts = GptOssExperts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e522b4f-19d7-42aa-8e06-c991bee36a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experts.load_state_dict(torch.load('model.layers.8.mlp.experts.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "07e55a77-5e65-4a38-a405-86faae9a6031",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ret = experts(hidden_states, router_indices, routing_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "44cd14e5-7c7e-42ad-b074-0d0d53033dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.allclose(actual_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a65764-81ff-4800-b16e-84d9e5fed614",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56039e4d-3e8e-49a6-b77e-dcc1d577cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.layers.3.self_attn.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35174625-b337-444c-b989-5ba464669f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hidden_states', 'attention_mask', 'position_ids', 'use_cache', 'cache_position', 'position_embeddings', 'output_router_logits', 'return'])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0cac0471-84b7-4b27-8fc4-245786f4ab57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3566, -0.1026,  1.1694,  ..., -0.4005,  0.1886, -0.1290],\n",
       "         [ 0.4086,  0.2832,  0.8145,  ..., -0.1869,  0.1457,  0.2259],\n",
       "         [ 0.6011, -0.5394,  0.0729,  ...,  0.2194, -0.0664, -0.1242],\n",
       "         ...,\n",
       "         [-0.1970,  0.3161, -0.1640,  ...,  0.0289, -0.1230, -0.3016],\n",
       "         [ 0.1955, -0.8916,  0.2378,  ...,  0.0600,  0.0521, -0.4781],\n",
       "         [-0.2455,  0.3101,  0.1855,  ...,  0.1516, -0.2417, -0.1635]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = torch.Tensor(parameters['hidden_states']['data'])\n",
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9dd8f180-edce-4fbf-a34e-3742b374f67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.Tensor(parameters['attention_mask']['data'])\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0c65c0f-c217-4130-8b86-1904b43ff7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4, 5, 6]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_ids = torch.LongTensor(parameters['position_ids']['data'])\n",
    "position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5af3032f-5579-4066-be8a-b0987f518338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['use_cache']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eda38b02-941e-4033-9f91-1a0a679112f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Tensor', 'data': [0, 1, 2, 3, 4, 5, 6]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['cache_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7dfb7a7f-1afc-4901-8648-fb8fb61a2c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embeddings_0 = torch.Tensor(parameters['position_embeddings']['items'][0]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "43896d66-a5da-486f-9e07-45b32d9dbacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "position_embeddings_1 = torch.Tensor(parameters['position_embeddings']['items'][1]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a4161d88-9655-41f7-975f-58e862cd6ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['output_router_logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f49ca6c3-54f0-4135-8793-adf5b112f5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_0 = torch.Tensor(parameters['return']['items'][0]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5ea0a406-f6ed-4cbc-b75d-2fd6ccb4cdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_1 = torch.Tensor(parameters['return']['items'][1]['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a290721-8a04-421d-aca2-f6b86a631812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from cowlist import COWList\n",
    "\n",
    "CONFIG_LAYER_TYPES = COWList([\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention',\n",
    "    'sliding_attention',\n",
    "    'full_attention'\n",
    "])\n",
    "\n",
    "\n",
    "def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n",
    "    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n",
    "    \"\"\"\n",
    "    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n",
    "    if n_rep == 1:\n",
    "        return hidden_states\n",
    "    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n",
    "    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n",
    "\n",
    "\n",
    "def eager_attention_forward(\n",
    "    module: nn.Module,\n",
    "    query: torch.Tensor,\n",
    "    key: torch.Tensor,\n",
    "    value: torch.Tensor,\n",
    "    attention_mask: Optional[torch.Tensor],\n",
    "    scaling: float,\n",
    "    dropout: float=0.0,\n",
    "    # **kwargs\n",
    "):\n",
    "    key_states = repeat_kv(key, 8)\n",
    "    value_states = repeat_kv(value, 8)\n",
    "    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n",
    "    if attention_mask is not None:\n",
    "        causal_mask = attention_mask[:, :, :, :key_states.shape[-2]]\n",
    "        attn_weights = attn_weights + causal_mask\n",
    "    sinks = module.sinks.reshape(1, -1, 1, 1).expand(query.shape[0], -1, query.shape[-2], -1)\n",
    "    combined_logits = torch.cat([attn_weights, sinks], dim=-1)\n",
    "    combined_logits = combined_logits - combined_logits.max(dim=-1, keepdim=True).values\n",
    "    probs = F.softmax(combined_logits, dim=-1, dtype=combined_logits.dtype)\n",
    "    scores = probs[..., :-1]\n",
    "    attn_weights = nn.functional.dropout(scores, p=dropout, training=False)\n",
    "    attn_output = torch.matmul(attn_weights, value_states)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "    return (attn_output, attn_weights)\n",
    "\n",
    "\n",
    "def _apply_rotary_emb(\n",
    "    x: torch.Tensor,\n",
    "    cos: torch.Tensor,\n",
    "    sin: torch.Tensor,\n",
    ") -> torch.Tensor:\n",
    "    first_half, second_half = torch.chunk(x, 2, dim=-1)\n",
    "    first_ = first_half * cos - second_half * sin\n",
    "    second_ = second_half * cos + first_half * sin\n",
    "    return torch.cat((first_, second_), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = _apply_rotary_emb(q, cos, sin)\n",
    "    k_embed = _apply_rotary_emb(k, cos, sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "\n",
    "class GptOssAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(2880, 64 * 64, bias=True)\n",
    "        self.k_proj = nn.Linear(2880, 8 * 64, bias=True)\n",
    "        self.v_proj = nn.Linear(2880, 8 * 64, bias=True)\n",
    "        self.o_proj = nn.Linear(64 * 64, 2880, bias=True)\n",
    "        self.sinks = nn.Parameter(torch.empty(64))\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        # past_key_values: Optional[Cache]=None,\n",
    "        # cache_position: Optional[torch.LongTensor]=None,\n",
    "        # **kwargs\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, 64)\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "        attn_output, attn_weights = eager_attention_forward(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0,\n",
    "            scaling=0.125\n",
    "        )\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return (attn_output, attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cd635915-058b-4ca7-9b7f-cc404a3c2716",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_attn = GptOssAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6c97a51e-e6f8-4b3e-9c1b-1d8b97576a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_attn.load_state_dict(torch.load('model.layers.3.self_attn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d045d49d-c332-4077-84eb-718729f53910",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ret_0, actual_ret_1 = self_attn(hidden_states, (position_embeddings_0, position_embeddings_1), attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "51fa3e31-6c3c-428a-b212-e7c1a009e096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_0.allclose(actual_ret_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "460d1aff-11aa-4f42-8408-6282c126543c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_1.allclose(actual_ret_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6672a3da-3a43-4659-bced-5827d5c3724d",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0a464971-c5f8-420a-9d45-8130617d508c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.router = GptOssTopKRouter()\n",
    "        self.experts = GptOssExperts()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        router_scores, router_indices = self.router(hidden_states)\n",
    "        routed_out = self.experts(hidden_states, router_indices=router_indices, routing_weights=router_scores)\n",
    "        return (routed_out, router_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18e323cf-d058-496f-9c4d-b2e1ac925918",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = GptOssMLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "263f91bd-99cf-4f12-9b28-85bf9241fe15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.load_state_dict(torch.load('model.layers.0.mlp.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "06a9c265-574f-4053-b98c-debbdb90580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.layers.0.mlp.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "68671537-085f-46ad-b338-90b9d91cc805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hidden_states', 'return'])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5679bdba-d2da-4ff5-b7cc-90f03ff32140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2309,  0.2867,  1.0308,  ..., -0.0260, -0.0321,  0.0591],\n",
       "         [ 0.3464,  0.4684,  0.6387,  ..., -0.4321, -0.1286,  0.0166],\n",
       "         [ 0.3301,  0.2904,  0.1818,  ..., -0.1855, -0.3324, -0.1021],\n",
       "         ...,\n",
       "         [-0.2346,  0.1293,  0.2423,  ..., -0.5792,  0.0849, -0.1463],\n",
       "         [ 0.0930, -0.3895,  0.5061,  ...,  0.0616, -0.2823, -0.1552],\n",
       "         [ 0.2867,  0.2045,  0.6961,  ..., -0.3314, -0.2441, -0.1707]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = torch.Tensor(parameters['hidden_states']['data'])\n",
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6dd93ee6-8ba1-49a0-b4f7-fbc059b2d4be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0024, -0.3997, -0.3772,  ...,  0.0044,  0.0997,  0.0103],\n",
       "         [ 0.1005, -0.2501,  0.1974,  ...,  0.0395,  0.0471,  0.3597],\n",
       "         [-0.0042,  0.0123,  0.2023,  ...,  0.1977,  0.0931,  0.0578],\n",
       "         ...,\n",
       "         [ 0.0902,  0.0755, -0.2559,  ...,  0.0017, -0.0158,  0.0860],\n",
       "         [ 0.1149,  0.1651,  0.4096,  ...,  0.0608,  0.0294, -0.2438],\n",
       "         [-0.2755,  0.0243, -0.3894,  ..., -0.1046,  0.0164,  0.1729]]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_0 = torch.Tensor(parameters['return']['items'][0]['data'])\n",
    "return_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d22bce64-0fdd-4465-99e1-4ad71cb7279b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3030, 0.0000, 0.0000, 0.0000,\n",
       "         0.0823, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3616,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.2531],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2018, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.1148, 0.0000, 0.0000, 0.0000, 0.0000, 0.4577, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2258,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0798, 0.0000, 0.7746, 0.0000, 0.0000, 0.0000, 0.0756,\n",
       "         0.0000, 0.0000, 0.0000, 0.0700, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1269, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4060, 0.0000,\n",
       "         0.0000, 0.1124, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.3547, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4421,\n",
       "         0.0000, 0.0000, 0.0519, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0371,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4690, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.1058, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.2626, 0.0000, 0.1793, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.4522, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.1294, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0769,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4282, 0.3655,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_1 = torch.Tensor(parameters['return']['items'][1]['data'])\n",
    "return_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e2c523ef-d910-4c4f-9cf3-bc2449906fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ret_0, actual_ret_1 = mlp(hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8635d92e-2299-407a-8814-df2ad6ff56d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_0.allclose(actual_ret_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "dac08724-9df6-4689-92a3-ec16d71fb0b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "return_1.allclose(actual_ret_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23112683-c811-46c9-bd70-9e45f925cd65",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71636c0d-c0b9-4cee-9017-c03d3a1701ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GptOssDecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.self_attn = GptOssAttention()\n",
    "        self.mlp = GptOssMLP()\n",
    "        self.input_layernorm = GptOssRMSNorm()\n",
    "        self.post_attention_layernorm = GptOssRMSNorm()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor]=None,\n",
    "        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]]=None,\n",
    "    ) -> torch.Tensor:\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "        hidden_states, _ = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_embeddings=position_embeddings,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states, _ = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dc1546c9-084b-4fa9-968f-114a571b9448",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_20 = GptOssDecoderLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "be934386-576c-48aa-bc18-6ae8f63952e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_20.load_state_dict(torch.load('model.layers.20.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "35d2a5aa-02e1-4338-90f1-8991b948cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('model.layers.20.json', 'r') as f:\n",
    "    parameters = json.load(f)['parameters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "60709e9a-40da-404e-8a14-1fd8e7ef16e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['hidden_states', 'attention_mask', 'position_ids', 'use_cache', 'cache_position', 'position_embeddings', 'output_router_logits', 'return'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "315b8a8a-a564-41b6-aca7-aa31a7dd3ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  -5.8659,   33.0872,   91.4604,  ...,   20.2924,   73.2631,\n",
       "          -235.1105],\n",
       "         [-121.1287,  -56.9332,   17.2398,  ...,  -56.2602,    2.1464,\n",
       "          -236.1957],\n",
       "         [ 148.5167,  106.2464,  100.2911,  ...,    6.6027,   85.7875,\n",
       "          -127.2576],\n",
       "         ...,\n",
       "         [ 241.3062,  -62.1225,  -36.7799,  ...,  -26.3095,  -28.3285,\n",
       "             5.2132],\n",
       "         [ -86.6025, -114.4028,  118.7643,  ...,  -17.0406,   84.3951,\n",
       "          -219.4708],\n",
       "         [-318.7719,   50.1896,   68.3206,  ...,   56.5228,   97.6040,\n",
       "            39.3755]]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_states = torch.Tensor(parameters['hidden_states']['data'])\n",
    "hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aa2153ff-288e-4465-bcd1-82e209ed1a29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           -3.4028e+38, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00, -3.4028e+38],\n",
       "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "            0.0000e+00,  0.0000e+00]]]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_mask = torch.Tensor(parameters['attention_mask']['data'])\n",
    "attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b6cd480d-d45c-4c19-946d-745545f0999a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 2., 3., 4., 5., 6.]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(parameters['position_ids']['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "63fb6ec8-c54b-471d-8cb5-2bcbbe9603de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['use_cache']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "195aca30-3995-402b-b105-c823ac89f1ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'Tensor', 'data': [0, 1, 2, 3, 4, 5, 6]}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters['cache_position']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a2c3bc5d-5d90-450a-bcd8-8da8ee097a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466],\n",
       "         [ 0.7276,  1.0394,  1.1976,  1.2752,  1.3125,  1.3304,  1.3389,\n",
       "           1.3429,  1.3448,  1.3459,  1.3463,  1.3465,  1.3465,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466],\n",
       "         [-0.5604,  0.2579,  0.7838,  1.0685,  1.2120,  1.2821,  1.3158,\n",
       "           1.3320,  1.3396,  1.3439,  1.3456,  1.3462,  1.3464,  1.3465,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466],\n",
       "         [-1.3331, -0.6412,  0.1965,  0.7485,  1.0502,  1.2030,  1.2778,\n",
       "           1.3138,  1.3310,  1.3405,  1.3443,  1.3458,  1.3463,  1.3465,\n",
       "           1.3465,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466],\n",
       "         [-0.8802, -1.2478, -0.4342,  0.3491,  0.8353,  1.0949,  1.2251,\n",
       "           1.2884,  1.3189,  1.3358,  1.3425,  1.3451,  1.3461,  1.3464,\n",
       "           1.3465,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466],\n",
       "         [ 0.3820, -1.2850, -0.9689, -0.0874,  0.5781,  0.9605,  1.1583,\n",
       "           1.2561,  1.3033,  1.3297,  1.3403,  1.3443,  1.3458,  1.3463,\n",
       "           1.3465,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466],\n",
       "         [ 1.2929, -0.7358, -1.2892, -0.5145,  0.2916,  0.8029,  1.0784,\n",
       "           1.2169,  1.2845,  1.3223,  1.3375,  1.3433,  1.3455,  1.3462,\n",
       "           1.3465,  1.3465,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,  1.3466,\n",
       "           1.3466,  1.3466,  1.3466,  1.3466]]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings_0 = torch.Tensor(parameters['position_embeddings']['items'][0]['data'])\n",
    "position_embeddings_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "de284898-b181-4a54-8cc7-d7733345f258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 1.1331e+00,  8.5615e-01,  6.1558e-01,  4.3271e-01,  3.0098e-01,\n",
       "           2.0831e-01,  1.4384e-01,  9.9212e-02,  6.8394e-02,  4.2687e-02,\n",
       "           2.6034e-02,  1.5609e-02,  9.1498e-03,  5.1982e-03,  2.8194e-03,\n",
       "           1.4174e-03,  6.1469e-04,  1.7414e-04,  5.1586e-05,  3.5545e-05,\n",
       "           2.4492e-05,  1.6876e-05,  1.1628e-05,  8.0124e-06,  5.5209e-06,\n",
       "           3.8042e-06,  2.6212e-06,  1.8061e-06,  1.2445e-06,  8.5753e-07,\n",
       "           5.9087e-07,  4.0714e-07],\n",
       "         [ 1.2244e+00,  1.3216e+00,  1.0950e+00,  8.1952e-01,  5.8673e-01,\n",
       "           4.1161e-01,  2.8604e-01,  1.9789e-01,  1.3661e-01,  8.5331e-02,\n",
       "           5.2059e-02,  3.1216e-02,  1.8299e-02,  1.0396e-02,  5.6389e-03,\n",
       "           2.8348e-03,  1.2294e-03,  3.4827e-04,  1.0317e-04,  7.1090e-05,\n",
       "           4.8984e-05,  3.3752e-05,  2.3257e-05,  1.6025e-05,  1.1042e-05,\n",
       "           7.6083e-06,  5.2425e-06,  3.6123e-06,  2.4890e-06,  1.7151e-06,\n",
       "           1.1817e-06,  8.1428e-07],\n",
       "         [ 1.9003e-01,  1.1841e+00,  1.3322e+00,  1.1194e+00,  8.4279e-01,\n",
       "           6.0500e-01,  4.2496e-01,  2.9548e-01,  2.0448e-01,  1.2789e-01,\n",
       "           7.8064e-02,  4.6819e-02,  2.7448e-02,  1.5594e-02,  8.4583e-03,\n",
       "           4.2522e-03,  1.8441e-03,  5.2241e-04,  1.5476e-04,  1.0663e-04,\n",
       "           7.3476e-05,  5.0628e-05,  3.4885e-05,  2.4037e-05,  1.6563e-05,\n",
       "           1.1412e-05,  7.8637e-06,  5.4184e-06,  3.7335e-06,  2.5726e-06,\n",
       "           1.7726e-06,  1.2214e-06],\n",
       "         [-1.0191e+00,  5.0624e-01,  1.2746e+00,  1.3005e+00,  1.0562e+00,\n",
       "           7.8382e-01,  5.5902e-01,  3.9147e-01,  2.7181e-01,  1.7032e-01,\n",
       "           1.0404e-01,  6.2416e-02,  3.6595e-02,  2.0792e-02,  1.1278e-02,\n",
       "           5.6696e-03,  2.4588e-03,  6.9655e-04,  2.0634e-04,  1.4218e-04,\n",
       "           9.7968e-05,  6.7504e-05,  4.6513e-05,  3.2050e-05,  2.2084e-05,\n",
       "           1.5217e-05,  1.0485e-05,  7.2246e-06,  4.9781e-06,  3.4301e-06,\n",
       "           2.3635e-06,  1.6286e-06],\n",
       "         [-1.2913e+00, -4.0261e-01,  9.3515e-01,  1.3437e+00,  1.2162e+00,\n",
       "           9.4377e-01,  6.8668e-01,  4.8534e-01,  3.3845e-01,  2.1258e-01,\n",
       "           1.2998e-01,  7.8004e-02,  4.5741e-02,  2.5990e-02,  1.4097e-02,\n",
       "           7.0870e-03,  3.0734e-03,  8.7069e-04,  2.5793e-04,  1.7772e-04,\n",
       "           1.2246e-04,  8.4380e-05,  5.8142e-05,  4.0062e-05,  2.7605e-05,\n",
       "           1.9021e-05,  1.3106e-05,  9.0307e-06,  6.2226e-06,  4.2876e-06,\n",
       "           2.9544e-06,  2.0357e-06],\n",
       "         [-3.7625e-01, -1.1277e+00,  3.8880e-01,  1.2444e+00,  1.3146e+00,\n",
       "           1.0810e+00,  8.0648e-01,  5.7656e-01,  4.0421e-01,  2.5462e-01,\n",
       "           1.5587e-01,  9.3582e-02,  5.4884e-02,  3.1187e-02,  1.6916e-02,\n",
       "           8.5044e-03,  3.6881e-03,  1.0448e-03,  3.0951e-04,  2.1327e-04,\n",
       "           1.4695e-04,  1.0126e-04,  6.9770e-05,  4.8075e-05,  3.3126e-05,\n",
       "           2.2825e-05,  1.5727e-05,  1.0837e-05,  7.4671e-06,  5.1452e-06,\n",
       "           3.5452e-06,  2.4428e-06]]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_embeddings_1 = torch.Tensor(parameters['position_embeddings']['items'][1]['data'])\n",
    "position_embeddings_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "028e0a0b-71be-4ed4-bba8-1797306488ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -51.7529,  -79.2223,  166.9466,  ...,   45.3354,   29.9371,\n",
       "          -294.3643],\n",
       "         [-106.0870,  -51.1902,  113.9774,  ..., -122.4729,  -74.4957,\n",
       "          -298.7038],\n",
       "         [ 162.0087,   43.3856,  182.8648,  ..., -100.3009,   22.0254,\n",
       "          -199.8858],\n",
       "         ...,\n",
       "         [ 225.2012,  -14.6964,  -12.7762,  ...,  -66.5061, -120.1939,\n",
       "            30.1922],\n",
       "         [-202.3708,  -67.3173,  108.8600,  ...,  -21.3696,  119.0166,\n",
       "          -300.0491],\n",
       "         [-507.0756,  107.2169,   57.5793,  ...,   -6.3350,   65.9705,\n",
       "           126.3017]]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = torch.Tensor(parameters['return']['data'])\n",
    "ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "462f4cfd-6736-4d91-83de-5319b0271daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_ret = layers_20(hidden_states, attention_mask, (position_embeddings_0, position_embeddings_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3f0c8690-5bda-4fc7-8686-5c8f5b9b1d32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.allclose(actual_ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6f6b1-bdcc-4b32-8a3c-ffc8e63ae71d",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc6dfd7-e949-4222-ac74-b47747d56e0b",
   "metadata": {},
   "source": [
    "# Work in Progress\n",
    "\n",
    "The whole model:\n",
    "\n",
    "```python\n",
    "class GptOssModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed_tokens = nn.Embedding(201088, 2880, 199999)\n",
    "        self.layers = nn.ModuleList([GptOssDecoderLayer() for _ in range(24)])\n",
    "        self.norm = GptOssRMSNorm(2880, eps=1e-05)\n",
    "        self.rotary_emb = GptOssRotaryEmbedding()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_ids: torch.LongTensor,\n",
    "        cache_position: torch.LongTensor,\n",
    "    ):\n",
    "        inputs_embeds = self.embed_tokens(input_ids)\n",
    "        mask_kwargs = {'config': self.config, 'input_embeds': inputs_embeds, 'attention_mask': attention_mask, 'cache_position': cache_position, 'past_key_values': past_key_values}\n",
    "        causal_mask_mapping = {'full_attention': create_causal_mask(**mask_kwargs), 'sliding_attention': create_sliding_window_causal_mask(**mask_kwargs)}\n",
    "        hidden_states = inputs_embeds\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "        for decoder_layer in self.layers:\n",
    "            hidden_states = decoder_layer(\n",
    "                hidden_states,\n",
    "                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n",
    "                position_embeddings=position_embeddings,\n",
    "            )\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class GptOssForCausalLM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = GptOssModel()\n",
    "        self.lm_head = nn.Linear(2880, 201088, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: torch.Tensor,\n",
    "        position_ids: torch.LongTensor,\n",
    "        cache_position: torch.LongTensor,\n",
    "    ):\n",
    "        hidden_states = self.model(input_ids=input_ids, attention_mask=attention_mask, position_ids=position_ids, cache_position=cache_position)\n",
    "        logits = self.lm_head(hidden_states[:, -1:, :])\n",
    "        return logits\n",
    "```\n",
    "\n",
    "Mask creation logic:\n",
    "\n",
    "```python\n",
    "def create_causal_mask(\n",
    "    attention_mask,\n",
    "    kv_length,\n",
    "    batch_size,\n",
    "    dtype,\n",
    "    cache_position\n",
    "):\n",
    "    causal_mask = eager_mask(\n",
    "        batch_size=batch_size,\n",
    "        cache_position=cache_position,\n",
    "        kv_length=kv_length,\n",
    "        kv_offset=0,\n",
    "        mask_function=causal_mask_function,\n",
    "        attention_mask=attention_mask,\n",
    "        allow_is_causal_skip=True,\n",
    "        dtype=dtype,\n",
    "    )\n",
    "    return causal_mask\n",
    "\n",
    "\n",
    "def create_sliding_window_causal_mask(\n",
    "    attention_mask,\n",
    "    kv_length,\n",
    "    batch_size,\n",
    "    dtype,\n",
    "    cache_position\n",
    "):\n",
    "    mask_factory_function = and_masks(sliding_window_overlay, causal_mask_function)\n",
    "    causal_mask = eager_mask(\n",
    "        batch_size=batch_size,\n",
    "        cache_position=cache_position,\n",
    "        kv_length=kv_length,\n",
    "        kv_offset=0,\n",
    "        mask_function=mask_factory_function,\n",
    "        attention_mask=attention_mask,\n",
    "        allow_is_causal_skip=True,\n",
    "        dtype=dtype,\n",
    "        local_size=128,\n",
    "    )\n",
    "    return causal_mask\n",
    "\n",
    "\n",
    "def eager_mask(\n",
    "    batch_size: int,\n",
    "    cache_position: torch.Tensor,\n",
    "    kv_length: int,\n",
    "    kv_offset: int=0,\n",
    "    mask_function: Callable=causal_mask_function,\n",
    "    attention_mask: Optional[torch.Tensor]=None,\n",
    "    dtype: torch.dtype=torch.float32,\n",
    "    **kwargs\n",
    "):\n",
    "    mask = sdpa_mask_recent_torch(\n",
    "        batch_size=batch_size,\n",
    "        cache_position=cache_position,\n",
    "        kv_length=kv_length,\n",
    "        kv_offset=kv_offset,\n",
    "        mask_function=mask_function,\n",
    "        attention_mask=attention_mask,\n",
    "        allow_is_causal_skip=False,\n",
    "        allow_torch_fix=False,\n",
    "        **kwargs\n",
    "    )\n",
    "    min_dtype = torch.finfo(dtype).min\n",
    "    mask = torch.where(mask, torch.tensor(0.0, device=mask.device, dtype=dtype), min_dtype)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def sdpa_mask_recent_torch(\n",
    "    batch_size: int,\n",
    "    cache_position: torch.Tensor,\n",
    "    kv_length: int,\n",
    "    kv_offset: int=0,\n",
    "    mask_function: Callable=causal_mask_function,\n",
    "    attention_mask: Optional[torch.Tensor]=None,\n",
    "    local_size: Optional[int]=None,\n",
    "    allow_is_causal_skip: bool=True\n",
    ") -> Optional[torch.Tensor]:\n",
    "    q_length = cache_position.shape[0]\n",
    "    kv_arange = torch.arange(kv_length, device=cache_position.device)\n",
    "    mask_function = and_masks(mask_function, padding_mask_function(attention_mask))\n",
    "    batch_arange = torch.arange(batch_size, device=cache_position.device)\n",
    "    head_arange = torch.arange(1, device=cache_position.device)\n",
    "    with TransformGetItemToIndex():\n",
    "        causal_mask = _vmap_for_bhqkv(mask_function)(batch_arange, head_arange, cache_position, kv_arange)\n",
    "    return causal_mask\n",
    "\n",
    "\n",
    "def causal_mask_function(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int):\n",
    "    return kv_idx <= q_idx\n",
    "\n",
    "def sliding_window_overlay(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int):\n",
    "    return kv_idx > q_idx - 128\n",
    "\n",
    "\n",
    "def padding_mask_function(padding_mask: torch.Tensor) -> Callable:\n",
    "    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n",
    "        return padding_mask[batch_idx, kv_idx]\n",
    "    return inner_mask\n",
    "\n",
    "\n",
    "def _vmap_for_bhqkv(mask_function: Callable, bh_indices: bool=True) -> Callable:\n",
    "    dimensions = [(None, None, None, 0), (None, None, 0, None)]\n",
    "    if bh_indices:\n",
    "        dimensions.extend([(None, 0, None, None), (0, None, None, None)])\n",
    "    for dims in dimensions:\n",
    "        mask_function = torch.vmap(mask_function, in_dims=dims, out_dims=0)\n",
    "    return mask_function\n",
    "\n",
    "\n",
    "def and_masks(*mask_functions: Callable) -> Callable:\n",
    "    def and_mask(batch_idx, head_idx, q_idx, kv_idx):\n",
    "        result = q_idx.new_ones((), dtype=torch.bool)\n",
    "        for mask in mask_functions:\n",
    "            result = result & mask(batch_idx, head_idx, q_idx, kv_idx).to(result.device)\n",
    "        return result\n",
    "    return and_mask\n",
    "```\n",
    "\n",
    "Values used to create masks:\n",
    "\n",
    "```\n",
    "tensor([[True, True, True, True, True, True, True]]) 7 1 torch.float32 tensor([0, 1, 2, 3, 4, 5, 6])\n",
    "tensor([[True, True, True, True, True, True, True]]) 7 1 torch.float32 tensor([0, 1, 2, 3, 4, 5, 6])\n",
    "\n",
    "tensor([[True, True, True, True, True, True, True, True]]) 8 1 torch.float32 tensor([7])\n",
    "tensor([[True, True, True, True, True, True, True, True]]) 8 1 torch.float32 tensor([7])\n",
    "\n",
    "tensor([[True, True, True, True, True, True, True, True, True]]) 9 1 torch.float32 tensor([8])\n",
    "tensor([[True, True, True, True, True, True, True, True, True]]) 9 1 torch.float32 tensor([8])\n",
    "\n",
    "...\n",
    "\n",
    "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True]]) 26 1 torch.float32 tensor([25])\n",
    "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
    "         True, True]]) 26 1 torch.float32 tensor([25])\n",
    "```\n",
    "\n",
    "Created masks:\n",
    "\n",
    "```\n",
    "{'full_attention': tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
    "           -3.4028e+38, -3.4028e+38],\n",
    "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
    "           -3.4028e+38, -3.4028e+38],\n",
    "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
    "           -3.4028e+38, -3.4028e+38],\n",
    "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
    "           -3.4028e+38, -3.4028e+38],\n",
    "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "           -3.4028e+38, -3.4028e+38],\n",
    "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "            0.0000e+00, -3.4028e+38],\n",
    "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "            0.0000e+00,  0.0000e+00]]]]), 'sliding_attention': tensor([[[[ 0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
    "           -3.4028e+38, -3.4028e+38],\n",
    "          [ 0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38, -3.4028e+38,\n",
    "           -3.4028e+38, -3.4028e+38],\n",
    "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38, -3.4028e+38,\n",
    "           -3.4028e+38, -3.4028e+38],\n",
    "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00, -3.4028e+38,\n",
    "           -3.4028e+38, -3.4028e+38],\n",
    "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "           -3.4028e+38, -3.4028e+38],\n",
    "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "            0.0000e+00, -3.4028e+38],\n",
    "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
    "            0.0000e+00,  0.0000e+00]]]])}\n",
    "\n",
    "{'full_attention': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.]]]]), 'sliding_attention': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0.]]]])}\n",
    "\n",
    "{'full_attention': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0.]]]]), 'sliding_attention': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0.]]]])}\n",
    "\n",
    "...\n",
    "\n",
    "{'full_attention': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "           0., 0., 0.]]]]), 'sliding_attention': tensor([[[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "           0., 0., 0.]]]])}\n",
    "```\n",
    "\n",
    "Generation logic:\n",
    "\n",
    "```python\n",
    "class GenerationMixin(ContinuousMixin):\n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        # inputs: Optional[torch.Tensor]=None,\n",
    "        # generation_config: Optional[GenerationConfig]=None,\n",
    "        # logits_processor: Optional[LogitsProcessorList]=None,\n",
    "        # stopping_criteria: Optional[StoppingCriteriaList]=None,\n",
    "        # prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]]=None,\n",
    "        # synced_gpus: Optional[bool]=None,\n",
    "        # assistant_model: Optional['PreTrainedModel']=None,\n",
    "        # streamer: Optional['BaseStreamer']=None,\n",
    "        # negative_prompt_ids: Optional[torch.Tensor]=None,\n",
    "        # negative_prompt_attention_mask: Optional[torch.Tensor]=None,\n",
    "        # use_model_defaults: Optional[bool]=None,\n",
    "        # custom_generate: Optional[Union[str, Callable]]=None,\n",
    "        **kwargs\n",
    "    ) -> Union[GenerateOutput, torch.LongTensor]:...\n",
    "        # {'synced_gpus': False}\n",
    "        generation_mode_kwargs = self._extract_generation_mode_kwargs(custom_generate, kwargs, synced_gpus, assistant_model, streamer)\n",
    "        # GenerationConfig {\n",
    "        #   \"bos_token_id\": 199998,\n",
    "        #   \"do_sample\": true,\n",
    "        #   \"eos_token_id\": [\n",
    "        #     200002,\n",
    "        #     199999\n",
    "        #   ],\n",
    "        #   \"pad_token_id\": 199999\n",
    "        # }\n",
    "        # {'input_ids': tensor([[   40,  6423,   290, 10915,   328,  2615,   382]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
    "        generation_config, model_kwargs = self._prepare_generation_config(generation_config, use_model_defaults, **kwargs)\n",
    "        # <GenerationMode.SAMPLE: 'sample'>\n",
    "        generation_mode = generation_config.get_generation_mode(assistant_model)\n",
    "\n",
    "        logits_processor = LogitsProcessorList()\n",
    "        stopping_criteria = StoppingCriteriaList()\n",
    "        # tensor([[   40,  6423,   290, 10915,   328,  2615,   382]])\n",
    "        # 'input_ids'\n",
    "        # {'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
    "        inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(inputs, generation_config.bos_token_id, model_kwargs)\n",
    "        # 1\n",
    "        batch_size = inputs_tensor.shape[0]\n",
    "        device = inputs_tensor.device\n",
    "        self._prepare_special_tokens(generation_config, True, device=device)\n",
    "        # tensor([[   40,  6423,   290, 10915,   328,  2615,   382]])\n",
    "        input_ids = inputs_tensor if model_input_name == 'input_ids' else model_kwargs.pop('input_ids')\n",
    "        # 7\n",
    "        input_ids_length = input_ids.shape[1]\n",
    "        # True\n",
    "        # 20\n",
    "        has_default_max_length = kwargs.get('max_length') is None and generation_config.max_length is not None\n",
    "        # True\n",
    "        # 0\n",
    "        has_default_min_length = kwargs.get('min_length') is None and generation_config.min_length is not None\n",
    "        generation_config = self._prepare_generated_length(generation_config=generation_config, has_default_max_length=has_default_max_length, has_default_min_length=has_default_min_length, model_input_name=model_input_name, inputs_tensor=inputs_tensor, input_ids_length=input_ids_length)\n",
    "        model_kwargs['logits_to_keep'] = 1\n",
    "        # 27\n",
    "        # 26\n",
    "        max_cache_length = generation_config.max_length - 1\n",
    "\n",
    "        dynamic_cache_kwargs = {'config': self.config.get_text_config(decoder=True)}\n",
    "        model_kwargs['past_key_values'] = DynamicCache(**dynamic_cache_kwargs)\n",
    "        self._prepare_cache_for_generation(generation_config, model_kwargs, generation_mode, batch_size, max_cache_length)\n",
    "\n",
    "        prepared_logits_processor = self._get_logits_processor(generation_config=generation_config, input_ids_seq_length=input_ids_length, encoder_input_ids=inputs_tensor, prefix_allowed_tokens_fn=prefix_allowed_tokens_fn, logits_processor=logits_processor, device=inputs_tensor.device, model_kwargs=model_kwargs, negative_prompt_ids=negative_prompt_ids, negative_prompt_attention_mask=negative_prompt_attention_mask)\n",
    "\n",
    "        prepared_stopping_criteria = self._get_stopping_criteria(generation_config=generation_config, stopping_criteria=stopping_criteria, tokenizer=generation_mode_kwargs.get('tokenizer'))\n",
    "\n",
    "        # True\n",
    "        model_kwargs['use_cache'] = generation_config.use_cache\n",
    "        \n",
    "        # <function GenerationMixin._sample at 0x7fef8e6998a0>\n",
    "        result = self._sample(self, input_ids, logits_processor=prepared_logits_processor, stopping_criteria=prepared_stopping_criteria, generation_config=generation_config, **generation_mode_kwargs, **model_kwargs)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _sample(self, input_ids: torch.LongTensor, logits_processor: LogitsProcessorList, stopping_criteria: StoppingCriteriaList, generation_config: GenerationConfig, synced_gpus: bool=False, streamer: Optional['BaseStreamer']=None, **model_kwargs) -> Union[GenerateNonBeamOutput, torch.LongTensor]:...\n",
    "        # tensor(199999)\n",
    "        pad_token_id = generation_config._pad_token_tensor\n",
    "        scores = None\n",
    "        # (1, 7)\n",
    "        batch_size, cur_len = input_ids.shape[:2]\n",
    "        this_peer_finished = False\n",
    "        unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n",
    "        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n",
    "        model_forward = self.__call__\n",
    "        is_prefill = True\n",
    "        while not this_peer_finished:\n",
    "            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n",
    "            if is_prefill:\n",
    "                # <function GptOssForCausalLM.forward at 0x7fef8d8cbba0>\n",
    "                outputs = self(**model_inputs, return_dict=True)\n",
    "                is_prefill = False\n",
    "            else:\n",
    "                # <function GptOssForCausalLM.forward at 0x7fef8d8cbba0>\n",
    "                outputs = model_forward(**model_inputs, return_dict=True)\n",
    "            model_kwargs = self._update_model_kwargs_for_generation(outputs, model_kwargs, is_encoder_decoder=self.config.is_encoder_decoder)\n",
    "            next_token_logits = outputs.logits[:, -1, :].to(copy=True, dtype=torch.float32, device=input_ids.device)\n",
    "            next_token_scores = logits_processor(input_ids, next_token_logits)\n",
    "            probs = nn.functional.softmax(next_token_scores, dim=-1)\n",
    "            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
    "            next_tokens = next_tokens * unfinished_sequences + pad_token_id * (1 - unfinished_sequences)\n",
    "            input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n",
    "            unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n",
    "            this_peer_finished = unfinished_sequences.max() == 0\n",
    "            cur_len += 1\n",
    "            del outputs\n",
    "        return input_ids\n",
    "\n",
    "    def _cache_dependant_input_preparation(self, input_ids: torch.LongTensor, inputs_embeds: Optional[torch.FloatTensor], cache_position: Optional[torch.LongTensor]) -> tuple[torch.FloatTensor, torch.LongTensor]:...\n",
    "        if input_ids.shape[1] != cache_position.shape[0]:\n",
    "            input_ids = input_ids[:, cache_position]\n",
    "        return (inputs_embeds, input_ids)\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids: torch.LongTensor, past_key_values: Optional[Cache]=None, attention_mask: Optional[torch.LongTensor]=None, inputs_embeds: Optional[torch.FloatTensor]=None, cache_position: Optional[torch.LongTensor]=None, **kwargs):...\n",
    "        model_inputs = {}\n",
    "        model_inputs['cache_position'] = cache_position\n",
    "        if past_key_values is not None:\n",
    "            model_inputs['past_key_values'] = past_key_values\n",
    "            inputs_embeds, input_ids = self._cache_dependant_input_preparation(input_ids, inputs_embeds, cache_position)\n",
    "        model_inputs['input_ids'] = input_ids.clone(memory_format=torch.contiguous_format)\n",
    "        model_inputs['inputs_embeds'] = None\n",
    "        if attention_mask is not None and kwargs.get('position_ids') is None and ('position_ids' in set(inspect.signature(self.forward).parameters.keys())):\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            kwargs['position_ids'] = position_ids\n",
    "        for model_input_name in ['position_ids', 'token_type_ids', 'decoder_position_ids']:\n",
    "            model_input = kwargs.get(model_input_name)\n",
    "            if model_input is not None:\n",
    "                if past_key_values is not None:\n",
    "                    current_input_length = ... if model_inputs.get('inputs_embeds') is not None else model_inputs['input_ids'].shape[1]\n",
    "                    model_input = model_input[:, -current_input_length:]\n",
    "                    model_input = model_input.clone(memory_format=torch.contiguous_format)\n",
    "                model_inputs[model_input_name] = model_input\n",
    "        if attention_mask is not None:\n",
    "            model_inputs['attention_mask'] = attention_mask\n",
    "        for key, value in kwargs.items():\n",
    "            if key not in model_inputs:\n",
    "                model_inputs[key] = value\n",
    "        model_inputs.pop('labels', None)\n",
    "        return model_inputs\n",
    "\n",
    "    def _prepare_model_inputs(self, inputs: Optional[torch.Tensor]=None, bos_token_id: Optional[torch.Tensor]=None, model_kwargs: Optional[dict[str, torch.Tensor]]=None) -> tuple[torch.Tensor, Optional[str], dict[str, torch.Tensor]]:...\n",
    "        # input_ids\n",
    "        input_name = self.main_input_name\n",
    "        model_kwargs = {k: v for k, v in model_kwargs.items() if v is not None or k != input_name}\n",
    "        inputs_kwarg = model_kwargs.pop(input_name, None)\n",
    "        inputs = inputs_kwarg\n",
    "        return (inputs, input_name, model_kwargs)\n",
    "\n",
    "    def _update_model_kwargs_for_generation(self, outputs: ModelOutput, model_kwargs: dict[str, Any], is_encoder_decoder: bool=False, num_new_tokens: int=1) -> dict[str, Any]:\n",
    "        model_kwargs['past_key_values'] = outputs.possible_cache_name\n",
    "        # tensor([[1, 1, 1, 1, 1, 1, 1]])\n",
    "        attention_mask = model_kwargs['attention_mask']\n",
    "        # tensor([[1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "        model_kwargs['attention_mask'] = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)\n",
    "        model_kwargs['cache_position'] = model_kwargs['cache_position'][-1:] + num_new_tokens\n",
    "        return model_kwargs\n",
    "\n",
    "    def _get_logits_processor(self, generation_config: GenerationConfig, input_ids_seq_length: Optional[int]=None, encoder_input_ids: Optional[torch.LongTensor]=None, prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]]=None, logits_processor: Optional[LogitsProcessorList]=None, device: Optional[str]=None, model_kwargs: Optional[dict[str, Any]]=None, negative_prompt_ids: Optional[torch.Tensor]=None, negative_prompt_attention_mask: Optional[torch.Tensor]=None) -> LogitsProcessorList:...\n",
    "        processors = LogitsProcessorList()\n",
    "        min_tokens_to_keep = 1\n",
    "        # 50\n",
    "        # 1\n",
    "        processors.append(TopKLogitsWarper(top_k=generation_config.top_k, min_tokens_to_keep=min_tokens_to_keep))\n",
    "        return processors\n",
    "\n",
    "    def _get_stopping_criteria(self, generation_config: GenerationConfig, stopping_criteria: Optional[StoppingCriteriaList], tokenizer: Optional['PreTrainedTokenizerBase']=None) -> StoppingCriteriaList:\n",
    "        criteria = StoppingCriteriaList()\n",
    "        # 131072\n",
    "        max_position_embeddings = getattr(self.config, 'max_position_embeddings', None)\n",
    "        criteria.append(MaxLengthCriteria(max_length=generation_config.max_length, max_position_embeddings=max_position_embeddings))\n",
    "        criteria.append(EosTokenCriteria(eos_token_id=generation_config._eos_token_tensor))\n",
    "        return criteria\n",
    "\n",
    "    def _prepare_generated_length(self, generation_config, has_default_max_length, has_default_min_length, model_input_name, input_ids_length, inputs_tensor):...\n",
    "        generation_config.max_length = generation_config.max_length + input_ids_length\n",
    "        # 131072\n",
    "        max_position_embeddings = getattr(self.config, 'max_position_embeddings', None)\n",
    "        generation_config.max_length = min(generation_config.max_length, max_position_embeddings)\n",
    "        return generation_config\n",
    "\n",
    "    def _prepare_generation_config(self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool]=None, **kwargs: Any) -> tuple[GenerationConfig, dict]:...\n",
    "        using_model_generation_config = False\n",
    "        if generation_config is None:...\n",
    "        generation_config = copy.deepcopy(generation_config)\n",
    "        if not using_model_generation_config:\n",
    "            model_base_version = version.parse(version.parse(self.generation_config.transformers_version).base_version)\n",
    "            if use_model_defaults is True or (use_model_defaults is None and model_base_version >= version.parse('4.50.0')):\n",
    "                modified_values = {}\n",
    "                global_default_generation_config = GenerationConfig()\n",
    "                model_generation_config = self.generation_config\n",
    "                for key, model_gen_config_value in model_generation_config.__dict__.items():\n",
    "                    if key.startswith('_') or key == 'transformers_version':\n",
    "                        continue\n",
    "                    if key == 'cache_implementation' and model_generation_config.cache_implementation == 'hybrid':...\n",
    "                    global_default_value = getattr(global_default_generation_config, key, None)\n",
    "                    custom_gen_config_value = getattr(generation_config, key, None)\n",
    "                    if custom_gen_config_value == global_default_value and model_gen_config_value != global_default_value:...\n",
    "                if generation_config.temperature == 0.0:...\n",
    "                if use_model_defaults is None and len(modified_values) > 0:...\n",
    "            else:...\n",
    "        model_kwargs = generation_config.update(**kwargs)\n",
    "        output_attentions = generation_config.output_attentions\n",
    "        output_hidden_states = generation_config.output_hidden_states\n",
    "        model_kwargs.update({'output_attentions': output_attentions} if output_attentions else {})\n",
    "        model_kwargs.update({'output_hidden_states': output_hidden_states} if output_hidden_states else {})\n",
    "        return (generation_config, model_kwargs)\n",
    "\n",
    "    def _get_initial_cache_position(self, seq_length, device, model_kwargs):...\n",
    "        # tensor([0, 1, 2, 3, 4, 5, 6])\n",
    "        cache_position = torch.ones(seq_length, dtype=torch.int64, device=device).cumsum(0) - 1\n",
    "        past_length = 0\n",
    "        if model_kwargs.get('past_key_values') is not None:\n",
    "            # DynamicCache(layers=[DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer, DynamicSlidingWindowLayer, DynamicLayer])\n",
    "            cache = model_kwargs['past_key_values']\n",
    "            past_length = 0\n",
    "            past_length = cache.get_seq_length()\n",
    "            cache_position = cache_position[past_length:]\n",
    "        model_kwargs['cache_position'] = cache_position\n",
    "        return model_kwargs\n",
    "\n",
    "    def _prepare_special_tokens(self, generation_config: GenerationConfig, kwargs_has_attention_mask: Optional[bool]=None, device: Optional[Union[torch.device, str]]=None):...\n",
    "\n",
    "        def _tensor_or_none(token, device=None):\n",
    "            if token is None:\n",
    "                return token\n",
    "            device = device if device is not None else self.device\n",
    "            if isinstance(token, torch.Tensor):...\n",
    "            return torch.tensor(token, device=device, dtype=torch.long)\n",
    "        # 199998\n",
    "        # tensor(199998)\n",
    "        bos_token_tensor = _tensor_or_none(generation_config.bos_token_id, device=device)\n",
    "        # [200002, 199999]\n",
    "        # tensor([200002, 199999])\n",
    "        eos_token_tensor = _tensor_or_none(generation_config.eos_token_id, device=device)\n",
    "        # 199999\n",
    "        # tensor(199999)\n",
    "        pad_token_tensor = _tensor_or_none(generation_config.pad_token_id, device=device)\n",
    "        # None\n",
    "        # None\n",
    "        decoder_start_token_tensor = _tensor_or_none(generation_config.decoder_start_token_id, device=device)\n",
    "        generation_config._bos_token_tensor = bos_token_tensor\n",
    "        generation_config._eos_token_tensor = eos_token_tensor\n",
    "        generation_config._pad_token_tensor = pad_token_tensor\n",
    "        generation_config._decoder_start_token_tensor = decoder_start_token_tensor\n",
    "\n",
    "    def _extract_generation_mode_kwargs(self, custom_generate, kwargs, synced_gpus, assistant_model, streamer) -> dict[str, Any]:...\n",
    "        generation_mode_kwargs = {'tokenizer': kwargs.pop('tokenizer', None), 'assistant_tokenizer': kwargs.pop('assistant_tokenizer', None), 'assistant_model': assistant_model, 'streamer': streamer}\n",
    "        generation_mode_kwargs['synced_gpus'] = (is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)) and dist.get_world_size() > 1 if synced_gpus is None else ...\n",
    "        generation_mode_kwargs = {k: v for k, v in generation_mode_kwargs.items() if v is not None}\n",
    "        if isinstance(custom_generate, Callable):...\n",
    "        return generation_mode_kwargs\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae747dd-b9e2-455a-b74d-29e694431ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
